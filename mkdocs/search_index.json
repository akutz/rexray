{
    "docs": [
        {
            "location": "/", 
            "text": "REX-Ray\n\n\nOpenly serious about storage\n\n\n\n\nREX-Ray delivers persistent storage access for container runtimes, such as\nDocker and Mesos, and provides an easy interface for enabling advanced storage\nfunctionality across common storage, virtualization and cloud platforms. For\nexample, here's how to list the volumes for a VirtualBox VM running Linux with\nREX-Ray:\n\n\n$ rexray volume --service virtualbox\n- attachments:\n  - instanceID:\n      id: e71578b0-1bfb-4fa5-bcd5-4ae982fd4a9b\n      driver: virtualbox\n    status: /Users/akutz/VirtualBox/libStorage/libStorage.vmdk\n    volumeID: 1b819454-a280-4cff-aff5-141f4e8fd154\n  name: libStorage.vmdk\n  size: 64\n  status: /Users/akutz/VirtualBox/libStorage/libStorage.vmdk\n  id: 1b819454-a280-4cff-aff5-141f4e8fd154\n  type: \n\n\n\n\n\nOverview\n\n\nREX-Ray is a storage orchestration tool that provides a set of common commands\nfor managing multiple storage platforms. Built on top of the\n\nlibStorage\n framework, REX-Ray\nconnects enables persistent storage for container runtimes such as Docker and\nMesos.\n\n\n\n\nnote\n\n\nThe initial REX-Ray 0.4.x release omits support for several,\npreviously verified storage platforms. These providers will be\nreintroduced incrementally, beginning with 0.4.1. If an absent driver\nprevents the use of REX-Ray, please continue to use 0.3.3 until such time\nthe storage platform is introduced in REX-Ray 0.4.x. Instructions on how\nto \ninstall\n and\n\nconfigure\n REX-Ray 0.3.3 are both\navailable.\n\n\n\n\nStorage Provider Support\n\n\nThe following storage providers and platforms are supported by REX-Ray.\n\n\n\n\n\n\n\n\nProvider\n\n\nStorage Platform(s)\n\n\n\n\n\n\n\n\n\n\nEMC\n\n\nScaleIO\n, \nIsilon\n\n\n\n\n\n\nOracle VirtualBox\n\n\nVirtual Media\n\n\n\n\n\n\n\n\nSupport for the following storage providers will be reintroduced in upcoming\nreleases:\n\n\n\n\n\n\n\n\nProvider\n\n\nStorage Platform(s)\n\n\n\n\n\n\n\n\n\n\nAmazon EC2\n\n\nEBS\n\n\n\n\n\n\nGoogle Compute Engine\n\n\nDisk\n\n\n\n\n\n\nOpen Stack\n\n\nCinder\n\n\n\n\n\n\nRackspace\n\n\nCinder\n\n\n\n\n\n\nEMC\n\n\nXtremIO\n, \nVMAX\n\n\n\n\n\n\n\n\nOperating System Support\n\n\nThe following operating systems (OS) are supported by REX-Ray:\n\n\n\n\n\n\n\n\nOS\n\n\nCommand Line\n\n\nService\n\n\n\n\n\n\n\n\n\n\nUbuntu 12+\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nDebian 6+\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nRedHat\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nCentOS 6+\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nCoreOS\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nTinyLinux (boot2docker)\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nOS X Yosemite+\n\n\nYes\n\n\nNo\n\n\n\n\n\n\nWindows\n\n\nNo\n\n\nNo\n\n\n\n\n\n\n\n\nContainer Platform Support\n\n\nREX-Ray currently supports the following container platforms:\n\n\n\n\n\n\n\n\nPlatform\n\n\nUse\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\nVolume Driver Plugin\n\n\n\n\n\n\nMesos\n\n\nVolume Driver Isolator module\n\n\n\n\n\n\nMesos + Docker\n\n\nVolume Driver Plugin\n\n\n\n\n\n\n\n\nGetting Started\n\n\nThis section will help in getting REX-Ray up and running quickly.\n\n\nInstalling REX-Ray\n\n\nThe following command will download the most recent, stable build of REX-Ray\nand install it to \n/usr/bin/rexray\n on Linux systems. REX-Ray will be\nregistered as either a SystemD or SystemV service depending upon the OS.\n\n\n$ curl -sSL https://dl.bintray.com/emccode/rexray/install | sh\n\n\n\n\nRefer to the User Guide's\n\ninstallation topic\n for instructions\non building REX-Ray from source, installing specific versions, and more.\n\n\nConfiguring REX-Ray\n\n\nREX-Ray requires a configuration file for storing details used to communicate\nwith storage providers. This can include authentication credentials and\ndriver-specific configuration options. Refer to the libStorage Storage Providers\n\ndocumentation\n\nfor sample configurations of all supported storage platforms. Additionally, look\nat \ncore properties\n \n\n\nlogging\n for advanced\nconfigurations.\n\n\nCreate a configuration file on the host at \n/etc/rexray/config.yml\n. Here is a\nsimple example for using Oracle VirtualBox:\n\n\nlibstorage:\n  service: virtualbox\n\n\n\n\nRefer to the\n\nVirtualBox documentation\n\nfor additional VirtualBox configuration options.\n\n\nFrom here, REX-Ray can now be used as a command line tool. View the commands\navailable:\n\n\n$ rexray --help\n\n\n\n\nTo verify the configuration file, use REX-Ray to list the volumes:\n\n\n$ rexray volume\n- attachments:\n  - instanceID:\n      id: e71578b0-1bfb-4fa5-bcd5-4ae982fd4a9b\n      driver: virtualbox\n    status: /Users/akutz/VirtualBox/libStorage/libStorage.vmdk\n    volumeID: 1b819454-a280-4cff-aff5-141f4e8fd154\n  name: libStorage.vmdk\n  size: 64\n  status: /Users/akutz/VirtualBox/libStorage/libStorage.vmdk\n  id: 1b819454-a280-4cff-aff5-141f4e8fd154\n  type: \n\n\n\n\n\nIf there is an error, use the \n-l debug\n flag and consult\n\ndebugging instructions\n.\n\n\nStart REX-Ray as a Service\n\n\nContainer platforms rely on REX-Ray to be running as a service to function\nproperly. For instance, Docker communicates to the REX-Ray Volume Driver via\na UNIX socket file.\n\n\n$ rexray service start\n\n\n\n\nHello REX-Ray\n\n\nIn the grand tradition of technical documentation, the first true end-to-end\nexample of REX-Ray is called \nHello REX-Ray\n. It showcases a two-node\ndeployment with the first node configured as a REX-Ray/libStorage server and\nthe second node as merely a client. Both nodes have Docker (1.11+) installed\nand configured to leverage REX-Ray for persistent storage.\n\n\nThe below example does have a few requirements:\n\n\n\n\nVirtualBox 5.0+\n\n\nVagrant 1.8+\n\n\nRuby 2.0+\n\n\n\n\nStart REX-Ray Vagrant Environment\n\n\nBefore bringing the Vagrant environment online, please ensure it is\naccomplished in a clean directory:\n\n\n$ cd $(mktemp -d)\n\n\n\n\nInside the newly created, temporary directory, download the REX-Ray\n\nVagrantfile\n:\n\n\n$ curl -fsSLO https://raw.githubusercontent.com/emccode/rexray/master/Vagrantfile\n\n\n\n\nNow it is time to bring the REX-Ray environment online:\n\n\n\n\nnote\n\n\nThe next step could potentially open up the system on which the command\nis executed to security vulnerabilities. The Vagrantfile brings the\nVirtualBox web service online if it is not already running. However,\nin the name of simplicity the Vagrantfile also disables the web server's\nauthentication module. Please do not disable authentication for the\nVirtualBox web server if this example is being executed on an open network\nor without some type of firewall in place.\n\n\n\n\n$ vagrant up\n\n\n\n\nThe above command should result in output similar to\n\nthis Gist\n.\n\n\nOnce the command has been completed successfully there will be two VMs online\nnamed \nnode0\n and \nnode1\n. Both nodes are running Docker and REX-Ray with\n\nnode0\n configured to act as a libStorage server.\n\n\nNow that the environment is online it is time to showcase Docker leveraging\nREX-Ray to create persistent storage as well as illustrating REX-Ray'a\ndistributed deployment capabilities.\n\n\nNode 0\n\n\nFirst, SSH into \nnode0\n\n\n$ vagrant ssh node0\n\n\n\n\nFrom \nnode0\n use Docker with REX-Ray to create a new volume named\n\nhellopersistence\n:\n\n\nvagrant@node0:~$ docker volume create --driver rexray --opt size=1 \\\n                 --name hellopersistence\n\n\n\n\nAfter the volume is created, mount it to the host and container using the\n\n--volume-driver\n and \n-v\n flag in the \ndocker run\n command:\n\n\nvagrant@node0:~$ docker run -tid --volume-driver=rexray \\\n                 -v hellopersistence:/mystore \\\n                 --name temp01 busybox\n\n\n\n\nCreate a new file named \nmyfile\n on the file system backed by the persistent\nvolume using \ndocker exec\n:\n\n\nvagrant@node0:~$ docker exec temp01 touch /mystore/myfile\n\n\n\n\nVerify the file was successfully created by listing the contents of the\npersistent volume:\n\n\nvagrant@node0:~$ docker exec temp01 ls /mystore\n\n\n\n\nRemove the container that was used to write the data to the persistent volume:\n\n\nvagrant@node0:~$ docker rm -f temp01\n\n\n\n\nFinally, exit the SSH session to \nnode0\n:\n\n\nvagrant@node0:~$ exit\n\n\n\n\nNode 1\n\n\nIt's time to connect to \nnode1\n and use the volume \nhellopersistence\n that was\ncreated in the previous section from \nnode0\n.\n\n\n\n\nnote\n\n\nWhile \nnode1\n runs both the Docker and REX-Ray services like \nnode1\n, the\nREX-Ray service on \nnode0\n in no way understands or is configured for the\nVirtualBox storage driver. All interactions with the VirtualBox web service\noccurs via \nnode0\n's libStorage server with which \nnode1\n communicates.\n\n\n\n\nUse the vagrant command to SSH into \nnode1\n:\n\n\n$ vagrant ssh node1\n\n\n\n\nNext, create a new container that mounts the existing volume,\n\nhellopersistence\n:\n\n\nvagrant@node1:~$ docker run -tid --volume-driver=rexray \\\n                 -v hellopersistence:/mystore \\\n                 --name temp01 busybox\n\n\n\n\nThe next command validates the file \nmyfile\n created from \nnode0\n in the\nprevious section has persisted inside the volume across machines:\n\n\nvagrant@node1:~$ docker exec temp01 ls /mystore\n\n\n\n\nFinally, exit the SSH session to \nnode1\n:\n\n\nvagrant@node1:~$ exit\n\n\n\n\nCleaning Up\n\n\nBe sure to kill the VirtualBox web server with a quick \nkillall vboxwebsrv\n and\nto tear down the Vagrant environment with \nvagrant destroy\n. Omitting these\ncommands will leave the web service and REX-Ray Vagrant nodes online and\nconsume additional system resources.\n\n\nCongratulations\n\n\nREX-Ray has been used to provide persistence for stateless containers! Examples\nusing MongoDB, Postgres, and more with persistent storage can be found at\n\nApplication Examples\n.\n\n\nGetting Help\n\n\nHaving issues? No worries, let's figure it out together.\n\n\nDebug\n\n\nThe \n-l debug\n flag can be appended to any command in order to get verbose\noutput. The following command will list all of the volumes visible to REX-Ray\nwith debug logging enabled:\n\n\n$ rexray volume -l debug\n\n\n\n\nFor an example of the full output from the above command, please refer to this\n\nGist\n.\n\n\nGitHub and Slack\n\n\nIf a little extra help is needed, please don't hesitate to use\n\nGitHub issues\n or join the active\nconversation on the\n\nEMC {code} Community Slack Team\n in\nthe #project-rexray channel", 
            "title": "Home"
        }, 
        {
            "location": "/#rex-ray", 
            "text": "Openly serious about storage   REX-Ray delivers persistent storage access for container runtimes, such as\nDocker and Mesos, and provides an easy interface for enabling advanced storage\nfunctionality across common storage, virtualization and cloud platforms. For\nexample, here's how to list the volumes for a VirtualBox VM running Linux with\nREX-Ray:  $ rexray volume --service virtualbox\n- attachments:\n  - instanceID:\n      id: e71578b0-1bfb-4fa5-bcd5-4ae982fd4a9b\n      driver: virtualbox\n    status: /Users/akutz/VirtualBox/libStorage/libStorage.vmdk\n    volumeID: 1b819454-a280-4cff-aff5-141f4e8fd154\n  name: libStorage.vmdk\n  size: 64\n  status: /Users/akutz/VirtualBox/libStorage/libStorage.vmdk\n  id: 1b819454-a280-4cff-aff5-141f4e8fd154\n  type:", 
            "title": "REX-Ray"
        }, 
        {
            "location": "/#overview", 
            "text": "REX-Ray is a storage orchestration tool that provides a set of common commands\nfor managing multiple storage platforms. Built on top of the libStorage  framework, REX-Ray\nconnects enables persistent storage for container runtimes such as Docker and\nMesos.   note  The initial REX-Ray 0.4.x release omits support for several,\npreviously verified storage platforms. These providers will be\nreintroduced incrementally, beginning with 0.4.1. If an absent driver\nprevents the use of REX-Ray, please continue to use 0.3.3 until such time\nthe storage platform is introduced in REX-Ray 0.4.x. Instructions on how\nto  install  and configure  REX-Ray 0.3.3 are both\navailable.", 
            "title": "Overview"
        }, 
        {
            "location": "/#storage-provider-support", 
            "text": "The following storage providers and platforms are supported by REX-Ray.     Provider  Storage Platform(s)      EMC  ScaleIO ,  Isilon    Oracle VirtualBox  Virtual Media     Support for the following storage providers will be reintroduced in upcoming\nreleases:     Provider  Storage Platform(s)      Amazon EC2  EBS    Google Compute Engine  Disk    Open Stack  Cinder    Rackspace  Cinder    EMC  XtremIO ,  VMAX", 
            "title": "Storage Provider Support"
        }, 
        {
            "location": "/#operating-system-support", 
            "text": "The following operating systems (OS) are supported by REX-Ray:     OS  Command Line  Service      Ubuntu 12+  Yes  Yes    Debian 6+  Yes  Yes    RedHat  Yes  Yes    CentOS 6+  Yes  Yes    CoreOS  Yes  Yes    TinyLinux (boot2docker)  Yes  Yes    OS X Yosemite+  Yes  No    Windows  No  No", 
            "title": "Operating System Support"
        }, 
        {
            "location": "/#container-platform-support", 
            "text": "REX-Ray currently supports the following container platforms:     Platform  Use      Docker  Volume Driver Plugin    Mesos  Volume Driver Isolator module    Mesos + Docker  Volume Driver Plugin", 
            "title": "Container Platform Support"
        }, 
        {
            "location": "/#getting-started", 
            "text": "This section will help in getting REX-Ray up and running quickly.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#installing-rex-ray", 
            "text": "The following command will download the most recent, stable build of REX-Ray\nand install it to  /usr/bin/rexray  on Linux systems. REX-Ray will be\nregistered as either a SystemD or SystemV service depending upon the OS.  $ curl -sSL https://dl.bintray.com/emccode/rexray/install | sh  Refer to the User Guide's installation topic  for instructions\non building REX-Ray from source, installing specific versions, and more.", 
            "title": "Installing REX-Ray"
        }, 
        {
            "location": "/#configuring-rex-ray", 
            "text": "REX-Ray requires a configuration file for storing details used to communicate\nwith storage providers. This can include authentication credentials and\ndriver-specific configuration options. Refer to the libStorage Storage Providers documentation \nfor sample configurations of all supported storage platforms. Additionally, look\nat  core properties    logging  for advanced\nconfigurations.  Create a configuration file on the host at  /etc/rexray/config.yml . Here is a\nsimple example for using Oracle VirtualBox:  libstorage:\n  service: virtualbox  Refer to the VirtualBox documentation \nfor additional VirtualBox configuration options.  From here, REX-Ray can now be used as a command line tool. View the commands\navailable:  $ rexray --help  To verify the configuration file, use REX-Ray to list the volumes:  $ rexray volume\n- attachments:\n  - instanceID:\n      id: e71578b0-1bfb-4fa5-bcd5-4ae982fd4a9b\n      driver: virtualbox\n    status: /Users/akutz/VirtualBox/libStorage/libStorage.vmdk\n    volumeID: 1b819454-a280-4cff-aff5-141f4e8fd154\n  name: libStorage.vmdk\n  size: 64\n  status: /Users/akutz/VirtualBox/libStorage/libStorage.vmdk\n  id: 1b819454-a280-4cff-aff5-141f4e8fd154\n  type:    If there is an error, use the  -l debug  flag and consult debugging instructions .", 
            "title": "Configuring REX-Ray"
        }, 
        {
            "location": "/#start-rex-ray-as-a-service", 
            "text": "Container platforms rely on REX-Ray to be running as a service to function\nproperly. For instance, Docker communicates to the REX-Ray Volume Driver via\na UNIX socket file.  $ rexray service start", 
            "title": "Start REX-Ray as a Service"
        }, 
        {
            "location": "/#hello-rex-ray", 
            "text": "In the grand tradition of technical documentation, the first true end-to-end\nexample of REX-Ray is called  Hello REX-Ray . It showcases a two-node\ndeployment with the first node configured as a REX-Ray/libStorage server and\nthe second node as merely a client. Both nodes have Docker (1.11+) installed\nand configured to leverage REX-Ray for persistent storage.  The below example does have a few requirements:   VirtualBox 5.0+  Vagrant 1.8+  Ruby 2.0+", 
            "title": "Hello REX-Ray"
        }, 
        {
            "location": "/#start-rex-ray-vagrant-environment", 
            "text": "Before bringing the Vagrant environment online, please ensure it is\naccomplished in a clean directory:  $ cd $(mktemp -d)  Inside the newly created, temporary directory, download the REX-Ray Vagrantfile :  $ curl -fsSLO https://raw.githubusercontent.com/emccode/rexray/master/Vagrantfile  Now it is time to bring the REX-Ray environment online:   note  The next step could potentially open up the system on which the command\nis executed to security vulnerabilities. The Vagrantfile brings the\nVirtualBox web service online if it is not already running. However,\nin the name of simplicity the Vagrantfile also disables the web server's\nauthentication module. Please do not disable authentication for the\nVirtualBox web server if this example is being executed on an open network\nor without some type of firewall in place.   $ vagrant up  The above command should result in output similar to this Gist .  Once the command has been completed successfully there will be two VMs online\nnamed  node0  and  node1 . Both nodes are running Docker and REX-Ray with node0  configured to act as a libStorage server.  Now that the environment is online it is time to showcase Docker leveraging\nREX-Ray to create persistent storage as well as illustrating REX-Ray'a\ndistributed deployment capabilities.", 
            "title": "Start REX-Ray Vagrant Environment"
        }, 
        {
            "location": "/#node-0", 
            "text": "First, SSH into  node0  $ vagrant ssh node0  From  node0  use Docker with REX-Ray to create a new volume named hellopersistence :  vagrant@node0:~$ docker volume create --driver rexray --opt size=1 \\\n                 --name hellopersistence  After the volume is created, mount it to the host and container using the --volume-driver  and  -v  flag in the  docker run  command:  vagrant@node0:~$ docker run -tid --volume-driver=rexray \\\n                 -v hellopersistence:/mystore \\\n                 --name temp01 busybox  Create a new file named  myfile  on the file system backed by the persistent\nvolume using  docker exec :  vagrant@node0:~$ docker exec temp01 touch /mystore/myfile  Verify the file was successfully created by listing the contents of the\npersistent volume:  vagrant@node0:~$ docker exec temp01 ls /mystore  Remove the container that was used to write the data to the persistent volume:  vagrant@node0:~$ docker rm -f temp01  Finally, exit the SSH session to  node0 :  vagrant@node0:~$ exit", 
            "title": "Node 0"
        }, 
        {
            "location": "/#node-1", 
            "text": "It's time to connect to  node1  and use the volume  hellopersistence  that was\ncreated in the previous section from  node0 .   note  While  node1  runs both the Docker and REX-Ray services like  node1 , the\nREX-Ray service on  node0  in no way understands or is configured for the\nVirtualBox storage driver. All interactions with the VirtualBox web service\noccurs via  node0 's libStorage server with which  node1  communicates.   Use the vagrant command to SSH into  node1 :  $ vagrant ssh node1  Next, create a new container that mounts the existing volume, hellopersistence :  vagrant@node1:~$ docker run -tid --volume-driver=rexray \\\n                 -v hellopersistence:/mystore \\\n                 --name temp01 busybox  The next command validates the file  myfile  created from  node0  in the\nprevious section has persisted inside the volume across machines:  vagrant@node1:~$ docker exec temp01 ls /mystore  Finally, exit the SSH session to  node1 :  vagrant@node1:~$ exit", 
            "title": "Node 1"
        }, 
        {
            "location": "/#cleaning-up", 
            "text": "Be sure to kill the VirtualBox web server with a quick  killall vboxwebsrv  and\nto tear down the Vagrant environment with  vagrant destroy . Omitting these\ncommands will leave the web service and REX-Ray Vagrant nodes online and\nconsume additional system resources.", 
            "title": "Cleaning Up"
        }, 
        {
            "location": "/#congratulations", 
            "text": "REX-Ray has been used to provide persistence for stateless containers! Examples\nusing MongoDB, Postgres, and more with persistent storage can be found at Application Examples .", 
            "title": "Congratulations"
        }, 
        {
            "location": "/#getting-help", 
            "text": "Having issues? No worries, let's figure it out together.", 
            "title": "Getting Help"
        }, 
        {
            "location": "/#debug", 
            "text": "The  -l debug  flag can be appended to any command in order to get verbose\noutput. The following command will list all of the volumes visible to REX-Ray\nwith debug logging enabled:  $ rexray volume -l debug  For an example of the full output from the above command, please refer to this Gist .", 
            "title": "Debug"
        }, 
        {
            "location": "/#github-and-slack", 
            "text": "If a little extra help is needed, please don't hesitate to use GitHub issues  or join the active\nconversation on the EMC {code} Community Slack Team  in\nthe #project-rexray channel", 
            "title": "GitHub and Slack"
        }, 
        {
            "location": "/user-guide/installation/", 
            "text": "Installation\n\n\nGetting the bits, bit by bit\n\n\n\n\nOverview\n\n\nThere are several different methods available for installing REX-Ray. It\nis written in Go, so there are typically no dependencies that must be installed\nalongside its single binary file. The manual methods can be extremely simple\nthrough tools like \ncurl\n. You also have the opportunity to perform install\nsteps individually. Following the manual installs, \nconfiguration\n\nmust take place.\n\n\nGreat examples of automation tools, such as \nAnsible\n and \nPuppet\n, are also\nprovided. These approaches automate the entire configuration process.\n\n\nManual Installs\n\n\nManual installations are in contrast to batch, automated installations.\n\n\nMake sure that before installing REX-Ray that you have uninstalled any previous\nversions. A \nrexray uninstall\n can assist with this where appropriate.\n\n\nFollowing an installation and configuration, you can use REX-Ray interactively\nthrough commands like \nrexray volume\n. Noticeably different from this is having\nREX-Ray integrate with Container Engines such as Docker. This requires that\nyou run \nrexray start\n or relevant service start command like\n\nsystemctl start rexray\n.\n\n\nInstall via curl\n\n\nThe following command will download the most recent, stable build of REX-Ray\nand install it to \n/usr/bin/rexray\n or \n/opt/bin/rexray\n. On Linux systems\nREX-Ray will also be registered as either a SystemD or SystemV service.\n\n\nThere is an optional flag to choose which version to install. Notice how we\nspecify \nstable\n, see the additional version names below that are also valid.\n\n\ncurl -sSL https://dl.bintray.com/emccode/rexray/install | sh -s -- stable\n\n\n\n\nInstall a pre-built binary\n\n\nThere are a handful of necessary manual steps to properly install REX-Ray\nfrom pre-built binaries.\n\n\nREX-Ray 0.3.3\n\n\n\n\nnote\n\n\nThe initial REX-Ray 0.4.x release omits support for several,\npreviously verified storage platforms. These providers will be\nreintroduced incrementally, beginning with 0.4.1. If an absent driver\nprevents the use of REX-Ray, please continue to use 0.3.3 until such time\nthe storage platform is introduced in REX-Ray 0.4.x. The following command\ncan be used to explicitly install REX-Ray 0.3.3:\n\n\ncurl -sSL https://dl.bintray.com/emccode/rexray/install | sh -s -- stable 0.3.3\n\n\n\n\n\n\n\n\nDownload the proper binary. There are also pre-built binaries available for\nthe various release types.\n\n\n\n\n\n\n\n\nVersion\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nUnstable\n\n\nThe most up-to-date, bleeding-edge, and often unstable REX-Ray binaries.\n\n\n\n\n\n\nStaged\n\n\nThe most up-to-date, release candidate REX-Ray binaries.\n\n\n\n\n\n\nStable\n\n\nThe most up-to-date, stable REX-Ray binaries.\n\n\n\n\n\n\n\n\n\n\n\n\nUncompress and move the binary to the proper location. Preferably \n/usr/bin\n\nshould be where REX-Ray is moved, but this path is not required.\n\n\n\n\nInstall as a service with \nrexray install\n. This will register itself\nwith SystemD or SystemV for proper initialization.\n\n\n\n\nBuild and install from source\n\n\nIt is also easy to build REX-Ray from source.\n\n\n\n\nnote\n\n\nREX-Ray requires at least Go 1.6.0. The REX-Ray developers use Go 1.6.2,\nand that is what REX-Ray's Travis-CI build uses as well. The build\nreference for REX-Ray is available in the\n\nDeveloper's Guide\n.\n\n\n\n\n# go get the rexray repo using the -d flag to enable \ndownload only\n mode\ngo get -d github.com/emccode/rexray\n\n# change directories into the freshly-cloned repo\ncd $GOPATH/src/github.com/emccode/rexray\n\n# get and build REX-Ray's dependencies and then build and install REX-Ray\nmake deps \n make\n\n\n\n\nOnce REX-Ray is built and installed it will be available at\n\n$GOPATH/bin/rexray\n:\n\n\n$ $GOPATH/bin/rexray version\nREX-Ray\n-------\nBinary: /home/akutz/go/bin/rexray\nSemVer: 0.4.0-rc4+10+dirty\nOsArch: Linux-x86_64\nBranch: release/0.4.0-rc4\nCommit: 063a0794ac19af439c3ab5a01f2e6f5a4f4f85ae\nFormed: Tue, 14 Jun 2016 14:23:15 CDT\n\nlibStorage\n----------\nSemVer: 0.1.3\nOsArch: Linux-x86_64\nBranch: v0.1.3\nCommit: 182a626937677a081b89651598ee2eac839308e7\nFormed: Tue, 14 Jun 2016 14:21:25 CDT\n\n\n\n\nAutomated Installs\n\n\nBecause REX-Ray is simple to install using the \ncurl\n script, installation\nusing configuration management tools is relatively easy as well. However,\nthere are a few areas that may prove to be tricky, such as writing the\nconfiguration file.\n\n\nThis section provides examples of automated installations using common\nconfiguration management and orchestration tools.\n\n\nAnsible\n\n\nWith Ansible, installing the latest REX-Ray binaries can be accomplished by\nincluding the \ncodenrhoden.rexray\n role from Ansible Galaxy.  The role accepts\nall the necessary variables to properly fill out your \nconfig.yml\n file.\n\n\nInstall the role from Galaxy:\n\n\n$ ansible-galaxy install emccode.rexray\n\n\n\n\nExample playbook for installing REX-Ray on GCE Docker hosts:\n\n\n- hosts: gce_docker_hosts\n  roles:\n  - { role: emccode.rexray,\n      rexray_service: true,\n      rexray_storage_drivers: [gce],\n      rexray_gce_keyfile: \n/opt/gce_keyfile\n }\n\n\n\n\nRun the playbook:\n\n\n$ ansible-playbook -i \ninventory\n playbook.yml\n\n\n\n\nAWS CloudFormation\n\n\nWith CloudFormation, the installation of the latest Docker and REX-Ray binaries\ncan be passed to the orchestrator using the 'UserData' property in a\nCloudFormation template. While the payload could also be provided as raw user\ndata via the AWS GUI, it would not sustain scalable automation.\n\n\nProperties\n: {\n  \nUserData\n: {\n    \nFn::Base64\n: {\n      \nFn::Join\n: [\n, [\n        \n#!/bin/bash -xe\\n\n,\n        \napt-get update\\n\n,\n        \napt-get -y install python-setuptools\\n\n,\n        \neasy_install https://s3.amazonaws.com/cloudformation-examples/aws-cfn-bootstrap-latest.tar.gz\\n\n,\n        \nln -s /usr/local/lib/python2.7/dist-packages/aws_cfn_bootstrap-1.4-py2.7.egg/init/ubuntu/cfn-hup /etc/init.d/cfn-hup\\n\n,\n        \nchmod +x /etc/init.d/cfn-hup\\n\n,\n        \nupdate-rc.d cfn-hup defaults\\n \n,\n        \nservice cfn-hup start\\n\n,\n        \n/usr/local/bin/cfn-init --stack \n, {\n          \nRef\n: \nAWS::StackName\n\n        }, \n --resource RexrayInstance \n, \n --configsets InstallAndRun --region \n, {\n          \nRef\n: \nAWS::Region\n\n        }, \n\\n\n,\n\n        \n# Install the latest Docker..\\n\n,\n        \n/usr/bin/curl -o /tmp/install-docker.sh https://get.docker.com/\\n\n,\n        \nchmod +x /tmp/install-docker.sh\\n\n,\n        \n/tmp/install-docker.sh\\n\n,\n\n        \n# add the ubuntu user to the docker group..\\n\n,\n        \n/usr/sbin/usermod -G docker ubuntu\\n\n,\n\n        \n# Install the latest REX-ray\\n\n,\n        \n/usr/bin/curl -ssL -o /tmp/install-rexray.sh https://dl.bintray.com/emccode/rexray/install\\n\n,\n        \nchmod +x /tmp/install-rexray.sh\\n\n,\n        \n/tmp/install-rexray.sh\\n\n,\n        \nchgrp docker /etc/rexray/config.yml\\n\n,\n        \nreboot\\n\n\n      ]]\n    }\n  }\n}\n\n\n\n\nDocker Machine (VirtualBox)\n\n\nSSH can be used to remotely deploy REX-Ray to a Docker Machine. While the\nfollowing example used VirtualBox as the underlying storage platform, the\nprovided \nconfig.yml\n file \ncould\n be modified to use any of the supported\ndrivers.\n\n\n\n\n\n\nSSH into the Docker machine and install REX-Ray.\n\n\n$ docker-machine ssh testing1 \\\n\"curl -sSL https://dl.bintray.com/emccode/rexray/install | sh\"\n\n\n\n\n\n\n\nInstall the udev extras package. This step is only required for versions of\n   boot2docker older than 1.10.\n\n\n$ docker-machine ssh testing1 \\\n\"wget http://tinycorelinux.net/6.x/x86_64/tcz/udev-extra.tcz \\\n\n tce-load -i udev-extra.tcz \n sudo udevadm trigger\"\n\n\n\n\n\n\n\nCreate a basic REX-Ray configuration file inside the Docker machine.\n\n\nNote\n: It is recommended to replace the \nvolumePath\n parameter with the\nlocal path VirtualBox uses to store its virtual media disk files.\n\n\n$ docker-machine ssh testing1 \\\n    \"sudo tee -a /etc/rexray/config.yml \n EOF\n    libstorage:\n      integration:\n        volume:\n          operations:\n            mount:\n              preempt: false\n    virtualbox:\n      volumePath: $HOME/VirtualBox/Volumes\n    \"\n\n\n\n\n\n\n\nFinally, start the REX-Ray service inside the Docker machine.\n\n\n$ docker-machine ssh testing1 \"sudo rexray start\"\n\n\n\n\n\n\n\nOpenStack Heat\n\n\nUsing OpenStack Heat, in the HOT template format (yaml):\n\n\nresources:\n  my_server:\n    type: OS::Nova::Server\n    properties:\n      user_data_format: RAW\n      user_data:\n        str_replace:\n          template: |\n            #!/bin/bash -v\n            /usr/bin/curl -o /tmp/install-docker.sh https://get.docker.com\n            chmod +x /tmp/install-docker.sh\n            /tmp/install-docker.sh\n            /usr/sbin/usermod -G docker ubuntu\n            /usr/bin/curl -ssL -o /tmp/install-rexray.sh https://dl.bintray.com/emccode/rexray/install\n            chmod +x /tmp/install-rexray.sh\n            /tmp/install-rexray.sh\n            chgrp docker /etc/rexray/config.yml\n          params:\n            dummy: \n\n\n\n\n\nVagrant\n\n\nUsing Vagrant is a great option to deploy pre-configured REX-Ray nodes,\nincluding Docker, using the VirtualBox driver. All volume requests are handled\nusing VirtualBox's Virtual Media.\n\n\nA Vagrant environment and instructions using it are provided\n\nhere\n.", 
            "title": "Installation"
        }, 
        {
            "location": "/user-guide/installation/#installation", 
            "text": "Getting the bits, bit by bit", 
            "title": "Installation"
        }, 
        {
            "location": "/user-guide/installation/#overview", 
            "text": "There are several different methods available for installing REX-Ray. It\nis written in Go, so there are typically no dependencies that must be installed\nalongside its single binary file. The manual methods can be extremely simple\nthrough tools like  curl . You also have the opportunity to perform install\nsteps individually. Following the manual installs,  configuration \nmust take place.  Great examples of automation tools, such as  Ansible  and  Puppet , are also\nprovided. These approaches automate the entire configuration process.", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/installation/#manual-installs", 
            "text": "Manual installations are in contrast to batch, automated installations.  Make sure that before installing REX-Ray that you have uninstalled any previous\nversions. A  rexray uninstall  can assist with this where appropriate.  Following an installation and configuration, you can use REX-Ray interactively\nthrough commands like  rexray volume . Noticeably different from this is having\nREX-Ray integrate with Container Engines such as Docker. This requires that\nyou run  rexray start  or relevant service start command like systemctl start rexray .", 
            "title": "Manual Installs"
        }, 
        {
            "location": "/user-guide/installation/#install-via-curl", 
            "text": "The following command will download the most recent, stable build of REX-Ray\nand install it to  /usr/bin/rexray  or  /opt/bin/rexray . On Linux systems\nREX-Ray will also be registered as either a SystemD or SystemV service.  There is an optional flag to choose which version to install. Notice how we\nspecify  stable , see the additional version names below that are also valid.  curl -sSL https://dl.bintray.com/emccode/rexray/install | sh -s -- stable", 
            "title": "Install via curl"
        }, 
        {
            "location": "/user-guide/installation/#install-a-pre-built-binary", 
            "text": "There are a handful of necessary manual steps to properly install REX-Ray\nfrom pre-built binaries.", 
            "title": "Install a pre-built binary"
        }, 
        {
            "location": "/user-guide/installation/#rex-ray-033", 
            "text": "note  The initial REX-Ray 0.4.x release omits support for several,\npreviously verified storage platforms. These providers will be\nreintroduced incrementally, beginning with 0.4.1. If an absent driver\nprevents the use of REX-Ray, please continue to use 0.3.3 until such time\nthe storage platform is introduced in REX-Ray 0.4.x. The following command\ncan be used to explicitly install REX-Ray 0.3.3:  curl -sSL https://dl.bintray.com/emccode/rexray/install | sh -s -- stable 0.3.3     Download the proper binary. There are also pre-built binaries available for\nthe various release types.     Version  Description      Unstable  The most up-to-date, bleeding-edge, and often unstable REX-Ray binaries.    Staged  The most up-to-date, release candidate REX-Ray binaries.    Stable  The most up-to-date, stable REX-Ray binaries.       Uncompress and move the binary to the proper location. Preferably  /usr/bin \nshould be where REX-Ray is moved, but this path is not required.   Install as a service with  rexray install . This will register itself\nwith SystemD or SystemV for proper initialization.", 
            "title": "REX-Ray 0.3.3"
        }, 
        {
            "location": "/user-guide/installation/#build-and-install-from-source", 
            "text": "It is also easy to build REX-Ray from source.   note  REX-Ray requires at least Go 1.6.0. The REX-Ray developers use Go 1.6.2,\nand that is what REX-Ray's Travis-CI build uses as well. The build\nreference for REX-Ray is available in the Developer's Guide .   # go get the rexray repo using the -d flag to enable  download only  mode\ngo get -d github.com/emccode/rexray\n\n# change directories into the freshly-cloned repo\ncd $GOPATH/src/github.com/emccode/rexray\n\n# get and build REX-Ray's dependencies and then build and install REX-Ray\nmake deps   make  Once REX-Ray is built and installed it will be available at $GOPATH/bin/rexray :  $ $GOPATH/bin/rexray version\nREX-Ray\n-------\nBinary: /home/akutz/go/bin/rexray\nSemVer: 0.4.0-rc4+10+dirty\nOsArch: Linux-x86_64\nBranch: release/0.4.0-rc4\nCommit: 063a0794ac19af439c3ab5a01f2e6f5a4f4f85ae\nFormed: Tue, 14 Jun 2016 14:23:15 CDT\n\nlibStorage\n----------\nSemVer: 0.1.3\nOsArch: Linux-x86_64\nBranch: v0.1.3\nCommit: 182a626937677a081b89651598ee2eac839308e7\nFormed: Tue, 14 Jun 2016 14:21:25 CDT", 
            "title": "Build and install from source"
        }, 
        {
            "location": "/user-guide/installation/#automated-installs", 
            "text": "Because REX-Ray is simple to install using the  curl  script, installation\nusing configuration management tools is relatively easy as well. However,\nthere are a few areas that may prove to be tricky, such as writing the\nconfiguration file.  This section provides examples of automated installations using common\nconfiguration management and orchestration tools.", 
            "title": "Automated Installs"
        }, 
        {
            "location": "/user-guide/installation/#ansible", 
            "text": "With Ansible, installing the latest REX-Ray binaries can be accomplished by\nincluding the  codenrhoden.rexray  role from Ansible Galaxy.  The role accepts\nall the necessary variables to properly fill out your  config.yml  file.  Install the role from Galaxy:  $ ansible-galaxy install emccode.rexray  Example playbook for installing REX-Ray on GCE Docker hosts:  - hosts: gce_docker_hosts\n  roles:\n  - { role: emccode.rexray,\n      rexray_service: true,\n      rexray_storage_drivers: [gce],\n      rexray_gce_keyfile:  /opt/gce_keyfile  }  Run the playbook:  $ ansible-playbook -i  inventory  playbook.yml", 
            "title": "Ansible"
        }, 
        {
            "location": "/user-guide/installation/#aws-cloudformation", 
            "text": "With CloudFormation, the installation of the latest Docker and REX-Ray binaries\ncan be passed to the orchestrator using the 'UserData' property in a\nCloudFormation template. While the payload could also be provided as raw user\ndata via the AWS GUI, it would not sustain scalable automation.  Properties : {\n   UserData : {\n     Fn::Base64 : {\n       Fn::Join : [ , [\n         #!/bin/bash -xe\\n ,\n         apt-get update\\n ,\n         apt-get -y install python-setuptools\\n ,\n         easy_install https://s3.amazonaws.com/cloudformation-examples/aws-cfn-bootstrap-latest.tar.gz\\n ,\n         ln -s /usr/local/lib/python2.7/dist-packages/aws_cfn_bootstrap-1.4-py2.7.egg/init/ubuntu/cfn-hup /etc/init.d/cfn-hup\\n ,\n         chmod +x /etc/init.d/cfn-hup\\n ,\n         update-rc.d cfn-hup defaults\\n  ,\n         service cfn-hup start\\n ,\n         /usr/local/bin/cfn-init --stack  , {\n           Ref :  AWS::StackName \n        },   --resource RexrayInstance  ,   --configsets InstallAndRun --region  , {\n           Ref :  AWS::Region \n        },  \\n ,\n\n         # Install the latest Docker..\\n ,\n         /usr/bin/curl -o /tmp/install-docker.sh https://get.docker.com/\\n ,\n         chmod +x /tmp/install-docker.sh\\n ,\n         /tmp/install-docker.sh\\n ,\n\n         # add the ubuntu user to the docker group..\\n ,\n         /usr/sbin/usermod -G docker ubuntu\\n ,\n\n         # Install the latest REX-ray\\n ,\n         /usr/bin/curl -ssL -o /tmp/install-rexray.sh https://dl.bintray.com/emccode/rexray/install\\n ,\n         chmod +x /tmp/install-rexray.sh\\n ,\n         /tmp/install-rexray.sh\\n ,\n         chgrp docker /etc/rexray/config.yml\\n ,\n         reboot\\n \n      ]]\n    }\n  }\n}", 
            "title": "AWS CloudFormation"
        }, 
        {
            "location": "/user-guide/installation/#docker-machine-virtualbox", 
            "text": "SSH can be used to remotely deploy REX-Ray to a Docker Machine. While the\nfollowing example used VirtualBox as the underlying storage platform, the\nprovided  config.yml  file  could  be modified to use any of the supported\ndrivers.    SSH into the Docker machine and install REX-Ray.  $ docker-machine ssh testing1 \\\n\"curl -sSL https://dl.bintray.com/emccode/rexray/install | sh\"    Install the udev extras package. This step is only required for versions of\n   boot2docker older than 1.10.  $ docker-machine ssh testing1 \\\n\"wget http://tinycorelinux.net/6.x/x86_64/tcz/udev-extra.tcz \\  tce-load -i udev-extra.tcz   sudo udevadm trigger\"    Create a basic REX-Ray configuration file inside the Docker machine.  Note : It is recommended to replace the  volumePath  parameter with the\nlocal path VirtualBox uses to store its virtual media disk files.  $ docker-machine ssh testing1 \\\n    \"sudo tee -a /etc/rexray/config.yml   EOF\n    libstorage:\n      integration:\n        volume:\n          operations:\n            mount:\n              preempt: false\n    virtualbox:\n      volumePath: $HOME/VirtualBox/Volumes\n    \"    Finally, start the REX-Ray service inside the Docker machine.  $ docker-machine ssh testing1 \"sudo rexray start\"", 
            "title": "Docker Machine (VirtualBox)"
        }, 
        {
            "location": "/user-guide/installation/#openstack-heat", 
            "text": "Using OpenStack Heat, in the HOT template format (yaml):  resources:\n  my_server:\n    type: OS::Nova::Server\n    properties:\n      user_data_format: RAW\n      user_data:\n        str_replace:\n          template: |\n            #!/bin/bash -v\n            /usr/bin/curl -o /tmp/install-docker.sh https://get.docker.com\n            chmod +x /tmp/install-docker.sh\n            /tmp/install-docker.sh\n            /usr/sbin/usermod -G docker ubuntu\n            /usr/bin/curl -ssL -o /tmp/install-rexray.sh https://dl.bintray.com/emccode/rexray/install\n            chmod +x /tmp/install-rexray.sh\n            /tmp/install-rexray.sh\n            chgrp docker /etc/rexray/config.yml\n          params:\n            dummy:", 
            "title": "OpenStack Heat"
        }, 
        {
            "location": "/user-guide/installation/#vagrant", 
            "text": "Using Vagrant is a great option to deploy pre-configured REX-Ray nodes,\nincluding Docker, using the VirtualBox driver. All volume requests are handled\nusing VirtualBox's Virtual Media.  A Vagrant environment and instructions using it are provided here .", 
            "title": "Vagrant"
        }, 
        {
            "location": "/user-guide/config/", 
            "text": "Configuring REX-Ray\n\n\nTweak this, turn that, peek behind the curtain...\n\n\n\n\nOverview\n\n\nThis page reviews how to configure REX-Ray to suit any environment, beginning\nwith the the most common use cases, exploring recommended guidelines, and\nfinally, delving into the details of more advanced settings.\n\n\nBasic Configuration\n\n\nThis section outlines the two most common configuration scenarios encountered\nby REX-Ray's users:\n\n\n\n\nREX-Ray as a stand-alone CLI tool\n\n\nREX-Ray as a service\n\n\n\n\n\n\nnote\n\n\nPlease remember to replace the placeholders in the following examples\nwith values valid for the systems on which the examples are executed.\n\n\nThe example below specifies the \nvolumePath\n property as\n\n$HOME/VirtualBox/Volumes\n. While the text \n$HOME\n will be replaced with\nthe actual value for that environment variable at runtime, the path may\nstill be invalid. The \nvolumePath\n property should reflect a path on the\nsystem on which the VirtualBox server is running, and that is not always\nthe same system on which the \nlibStorage\n server is running.\n\n\nSo please, make sure to update the \nvolumePath\n property for the VirtualBox\ndriver to a path valid on the system on which the VirtualBox server is\nrunning.\n\n\nThe same goes for VirtualBox property \nendpoint\n as the VirtualBox\nweb service is not always available at \n10.0.2.2:18083\n.\n\n\n\n\nStand-alone CLI Mode\n\n\nIt is possible to use REX-Ray directly from the command line without any\nconfiguration files. The following example uses REX-Ray to list the storage\nvolumes available to a Linux VM hosted by VirtualBox:\n\n\n\n\nnote\n\n\nThe examples below assume that the VirtualBox web server is running on the\nhost OS with authentication disabled and accessible to the guest OS. For\nmore information please refer to the VirtualBox storage driver\n\ndocumentation\n.\n\n\n\n\n$ rexray volume --service virtualbox\n- attachments:\n  - instanceID:\n      id: e71578b0-1bfb-4fa5-bcd5-4ae982fd4a9b\n      driver: virtualbox\n    status: /Users/akutz/VirtualBox/libStorage/libStorage.vmdk\n    volumeID: 1b819454-a280-4cff-aff5-141f4e8fd154\n  name: libStorage.vmdk\n  size: 64\n  status: /Users/akutz/VirtualBox/libStorage/libStorage.vmdk\n  id: 1b819454-a280-4cff-aff5-141f4e8fd154\n  type: \n\n\n\n\n\nIn addition to listing volumes, the REX-Ray CLI can be used to create and\nremove them as well as manage volume snapshots. For an end-to-end example of\nvolume creation, see \nHello REX-Ray\n.\n\n\nEmbedded Server Mode\n\n\nWhen operating as a stand-alone CLI, REX-Ray actually loads an embedded\nlibStorage server for the duration of the CLI process and is accessible by\nonly the the process that hosts it. This is known as \nEmbedded Server Mode\n.\n\n\nWhile commonly used when executing one-off commands with REX-Ray as a\nstand-alone CLI tool, Embedded Server Mode can be utilized when configuring\nREX-Ray to advertise a static libStorage server as well. The following\nqualifications must be met for Embedded Server Mode to be activated:\n\n\n\n\n\n\nThe property \nlibstorage.host\n must not be defined via configuration file,\n   environment variable, or CLI flag\n\n\n\n\n\n\nIf the \nlibstorage.host\n property \nis\n defined then the property\n   \nlibstorage.embedded\n can be set to \ntrue\n to explicitly activate\n   Embedded Server Mode.\n\n\n\n\n\n\nIf the \nlibstorage.host\n property is set and \nlibtorage.embedded\n is\n   set to true, Embedded Server Mode will still only activate if the address\n   specified by \nlibstorage.host\n (whether a UNIX socket or TCP port) is\n   not currently in use.\n\n\n\n\n\n\nAuto Service Mode\n\n\nThe Stand-alone CLI Mode \nexample\n also uses the\n\n--service\n flag. This flag's argument sets the \nlibstorage.service\n property,\nwhich has a special meaning inside of REX-Ray -- it serves to enabled\n\nAuto Service Mode\n.\n\n\nServices represent unique libStorage endpoints that are available to libStorage\nclients. Each service is associated with a storage driver. Thus\nAuto Service Mode minimizes configuration for simple environments.\n\n\nThe value of the \nlibstorage.service\n property is used to create a default\nservice configured with a storage driver. This special mode is only activated\nif all of the following conditions are met:\n\n\n\n\nThe \nlibstorage.service\n property is set via:\n\n\nThe CLI flags \n-s|--service\n or \n--libstorageService\n\n\nThe environment variable \nLIBSTORAGE_SERVICE\n\n\nThe configuration file property \nlibstorage.service\n\n\n\n\n\n\nThe \nlibstorage.host\n property is \nnot\n set. This property can be set via:\n\n\nThe CLI flags \n-h|--host\n or \n--libstorageHost\n\n\nThe environment variable \nLIBSTORAGE_HOST\n\n\nThe configuration file property \nlibstorage.host\n\n\n\n\n\n\nThe configuration property \nlibstorage.server.services\n must \nnot\n be set.\n    This property is only configurable via a configuration file.\n\n\n\n\nBecause the above example met the auto service mode conditions, REX-Ray\ncreated a service named \nvirtualbox\n configured to use the \nvirtualbox\n driver.\nThis service runs on the libStorage server embedded inside of REX-Ray and is\naccessible only by the executing CLI process for the duration of said process.\nWhen used in this manner, the service name must also be a valid driver name.\n\n\nService Mode\n\n\nREX-Ray can also run as a persistent service that advertises both\n\nDocker Volume Plug-in\n\nand \nlibStorage\n endpoints.\n\n\nDocker Volume Plug-in\n\n\nThis section refers to the only operational mode that REX-Ray supported in\nversions 0.3.3 and prior. A UNIX socket is created by REX-Ray that serves as a\nDocker Volume Plugin compliant API endpoint. Docker is able to leverage this\nendpoint to deliver on-demand, persistent storage to containers.\n\n\nThe following is a simple example of a configuration file that should be\nlocated at \n/etc/rexray/config.yml\n. This file can be used to configure the\nsame options that were specified in the previous CLI example. Please see the\n\nadvanced section\n for a complete list of\nconfiguration options.\n\n\nlibstorage:\n  service: virtualbox\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes\n\n\n\n\nOnce the configuration file is in place, \nrexray service start\n can be used to\nstart the service. Sometimes it is also useful to add \n-l debug\n to enable\nmore verbose logging. Additionally, it's also occasionally beneficial to\nstart the service in the foreground with the \n-f\n flag.\n\n\n$ rexray start\n\nStarting REX-Ray...SUCCESS!\n\n  The REX-Ray daemon is now running at PID 15724. To\n  shutdown the daemon execute the following command:\n\n    sudo /usr/bin/rexray stop\n\n\n\n\nAt this point requests can now be made to the default Docker Volume Plugin\nand Volume Driver advertised by the UNIX socket \nrexray\n at\n\n/run/docker/plugins/rexray.sock\n. More details on configuring the Docker\nVolume Plug-in are available on the \nSchedulers\n page.\n\n\nlibStorage Server and Client\n\n\nIn addition to \nEmbedded Server Mode\n, REX-Ray can also\nexpose the libStorage API statically. This enables REX-Ray to server and a\nlibStorage server and perform only a storage abstraction role.\n\n\nIf the desire is to establish a centralized REX-Ray server that is called\non from remote REX-Ray instances then the following example will be useful.\nThe first configuration is for running REX-Ray purely as a libStorage server.\nThe second defines how one would would use one or more REX-Ray instances in a\nlibStorage client role.\n\n\nThe following examples require multiple systems in order to fulfill these\ndifferent roles. The \nHello REX-Ray\n section on\nthe front page has an end-to-end illustration of this use case that leverages\nVagrant to provide and configure the necessary systems.\n\n\nlibStorage Server\n\n\nThe example below illustrates the necessary settings for configuring REX-Ray\nas a libStorage server:\n\n\nrexray:\n  modules:\n    default-docker:\n      disabled: true\nlibstorage:\n  host: tcp://127.0.0.1:7979\n  embedded: true\n  client:\n    type: controller\n  server:\n    endpoints:\n      public:\n        address: tcp://:7979\n    services:\n      virtualbox:\n        driver: virtualbox\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes\n\n\n\n\nIn the above sample, the default Docker module is disabled. This means that\nwhile the REX-Ray service would be running, it would not be available to\nDocker on that host.\n\n\nThe \nlibstorage\n section defines the settings that configure the libStorage\nserver:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nlibstorage.host\n\n\nInstructs local clients which libStorage endpoint to access\n\n\n\n\n\n\nlibstorage.embedded\n\n\nIndicates the libStorage server should be started even though the \nlibstorage.host\n property is defined\n\n\n\n\n\n\nlibstorage.client.type\n\n\nWhen set to \ncontroller\n this property indicates local clients perform no integration activities\n\n\n\n\n\n\nlibstorage.server.endpoints\n\n\nThe available libStorage server HTTP endpoints\n\n\n\n\n\n\nlibstorage.server.services\n\n\nThe configured libStorage services\n\n\n\n\n\n\n\n\nStart the REX-Ray service with \nrexray service start\n.\n\n\nlibStorage Client\n\n\nOn a separate OS instance running REX-Ray, the follow command can be used to\nlist the instance's available VirtualBox storage volumes:\n\n\n$ rexray volume -h tcp://REXRAY_SERVER:7979 -s virtualbox\n\n\n\n\nAn alternative to the above CLI flags is to add them as persistent settings\nto the \n/etc/rexray/config.yml\n configuration file on this instance:\n\n\nlibstorage:\n  host:    tcp://REXRAY_SERVER:7979\n  service: virtualbox\n\n\n\n\nNow the above command can be simplified further:\n\n\n$ rexray volume\n\n\n\n\nOnce more, the REX-Ray service can be started with \nrexray service start\n and\nthe REX-Ray Docker Volume Plug-in endpoint will utilize the remote libStorage\nserver as its method for communicating with VirtualBox.\n\n\nAgain, a complete end-to-end Vagrant environment for the above example is\navailable at \nHello REX-Ray\n.\n\n\nExample sans Modules\n\n\nLets review the major sections of the configuration file:\n\n\nrexray:\n  logLevel: warn\nlibstorage:\n  service: virtualbox\n  integration:\n    volume:\n      operations:\n        create:\n          default:\n            size: 1\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes\n\n\n\n\nSettings occur in three primary areas:\n\n\n\n\nrexray\n\n\nlibstorage\n\n\nvirtualbox\n\n\n\n\nThe \nrexray\n section contains all properties specific to REX-Ray. The\nYAML property path \nrexray.logLevel\n defines the log level for REX-Ray and its\nchild components. All of the \nrexray\n properties are\n\ndocumented\n below.\n\n\nNext, the \nlibstorage\n section defines the service with which REX-Ray will\ncommunicate via the property \nlibstorage.service\n. This property also enables\nthe \nAuto Service Mode\n discussed above since this\nconfiguration example does not define a host or services section. For all\ninformation related to libStorage and its properties, please refer to the\n\nlibStorage documentation\n.\n\n\nFinally, the \nvirtualbox\n section configures the VirtualBox driver selected\nor loaded by REX-Ray, as indicated via the \nlibstorage.service\n property. The\nlibStorage Storage Drivers page has information about the configuration details\nof \neach driver\n,\nincluding \nVirtualBox\n.\n\n\nLogging\n\n\nThe \n-l|--logLevel\n option or \nrexray.logLevel\n configuration key can be set\nto any of the following values to increase or decrease the verbosity of the\ninformation logged to the console or the REX-Ray log file (defaults to\n\n/var/log/rexray/rexray.log\n).\n\n\n\n\npanic\n\n\nfatal\n\n\nerror\n\n\nwarn\n\n\ninfo\n\n\ndebug\n\n\n\n\nTroubleshooting\n\n\nThe command \nrexray env\n can be used to print out the runtime interpretation\nof the environment, including configured properties, in order to help diagnose\nconfiguration issues.\n\n\n$ rexray env | grep DEFAULT | sort -r\nREXRAY_MODULES_DEFAULT-DOCKER_TYPE=docker\nREXRAY_MODULES_DEFAULT-DOCKER_SPEC=/etc/docker/plugins/rexray.spec\nREXRAY_MODULES_DEFAULT-DOCKER_LIBSTORAGE_SERVICE=vfs\nREXRAY_MODULES_DEFAULT-DOCKER_HOST=unix:///run/docker/plugins/rexray.sock\nREXRAY_MODULES_DEFAULT-DOCKER_DISABLED=false\nREXRAY_MODULES_DEFAULT-DOCKER_DESC=The default docker module.\nREXRAY_MODULES_DEFAULT-ADMIN_TYPE=admin\nREXRAY_MODULES_DEFAULT-ADMIN_HOST=unix:///var/run/rexray/server.sock\nREXRAY_MODULES_DEFAULT-ADMIN_DISABLED=false\nREXRAY_MODULES_DEFAULT-ADMIN_DESC=The default admin module.\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_TYPE=\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_SIZE=16\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_IOPS=\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_FSTYPE=ext4\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_AVAILABILITYZONE=\n\n\n\n\nAdvanced Configuration\n\n\nThe following sections detail every last aspect of how REX-Ray works and can\nbe configured.\n\n\nExample with Modules\n\n\nModules enable a single REX-Ray instance to present multiple personalities or\nvolume endpoints, serving hosts that require access to multiple storage\nplatforms.\n\n\nDefining Modules\n\n\nThe following example demonstrates a basic configuration that presents two\nmodules using the VirtualBox driver: \ndefault-docker\n and \nvb2-module\n.\n\n\nrexray:\n  logLevel: warn\n  modules:\n    default-docker:\n      type: docker\n      desc: The default docker module.\n      host: unix:///run/docker/plugins/vb1.sock\n      libstorage:\n        service: virtualbox\n        integration:\n          volume:\n            operations:\n              create:\n                default:\n                  size: 1\n      virtualbox:\n        volumePath: $HOME/VirtualBox/Volumes\n    vb2-module:\n      type: docker\n      desc: The second docker module.\n      host: unix:///run/docker/plugins/vb2.sock\n      libstorage:\n        service: virtualbox\n        integration:\n          volume:\n            operations:\n              create:\n                default:\n                  size: 1\n      virtualbox:\n        volumePath: $HOME/VirtualBox/Volumes\nlibstorage:\n  service: virtualbox\n\n\n\n\nWhereas the previous example did not use modules and the example above does,\nthey both begin by defining the root section \nrexray\n. Unlike the previous\nexample, however, the majority of the \nlibstorage\n section and all of the\n\nvirtualbox\n section are no longer at the root. Instead the section\n\nrexray.modules\n is defined. The \nmodules\n key in the \nrexray\n section is where\nall modules are configured. Each key that is a child of \nmodules\n represents the\nname of a module.\n\n\n\n\nnote\n\n\nPlease note that while most of the \nlibstorage\n section has been relocated\nas a child of each module, the \nlibstorage.service\n property is still\ndefined at the root to activate \nAuto Service Mode\n as\na quick-start method of property configuring the embedded libStorage server.\n\n\n\n\nThe above example defines two modules:\n\n\n\n\n\n\ndefault-module\n\n\nThis is a special module, and it's always defined, even if not explicitly\nlisted. In the previous example without modules, the \nlibstorage\n and\n\nvirtualbox\n sections at the root actually informed the configuration of\nthe implicit \ndefault-docker\n module. In this example the explicit\ndeclaration of the \ndefault-docker\n module enables several of its\nproperties to be overridden and given desired values. The Advanced\nConfiguration section has more information on\n\nDefault Modules\n.\n\n\n\n\n\n\nvb2-module\n\n\nThis is a new, custom module configured almost identically to the\n\ndefault-module\n with the exception of a unique host address as defined\nby the module's \nhost\n key.\n\n\n\n\n\n\nNotice that both modules share many of the same properties and values. In fact,\nwhen defining both modules, the top-level \nlibstorage\n and \nvirtualbox\n sections\nwere simply copied into each module as sub-sections. This is perfectly valid\nas any configuration path that begins from the root of the REX-Ray\nconfiguration file can be duplicated beginning as a child of a module\ndefinition. This allows global settings to be overridden just for a specific\nmodules.\n\n\nAs noted, each module shares identical values with the exception of the module's\nname and host. The host is the address used by Docker to communicate with\nREX-Ray. The base name of the socket file specified in the address can be\nused with \ndocker --volume-driver=\n. With the current example the value of the\n\n--volume-driver\n parameter would be either \nvb1\n of \nvb2\n.\n\n\nModules and Inherited Properties\n\n\nThere is also another way to write the previous example while reducing the\nnumber of repeated, identical properties shared by two modules.\n\n\nrexray:\n  logLevel: warn\n  modules:\n    default-docker:\n      host: unix:///run/docker/plugins/vb1.sock\n      libstorage:\n        integration:\n          volume:\n            operations:\n              create:\n                default:\n                  size: 1\n    vb2:\n      type: docker\nlibstorage:\n  service: virtualbox\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes\n\n\n\n\nThe above example may look strikingly different than the previous one, but it's\nactually the same with just a few tweaks.\n\n\nWhile there are still two modules defined, the second one has been renamed from\n\nvb2-module\n to \nvb2\n. The change is a more succinct way to represent the same\nintent, and it also provides a nice side-effect. If the \nhost\n key is omitted\nfrom a Docker module, the value for the \nhost\n key is automatically generated\nusing the module's name. Therefore since there is no \nhost\n key for the \nvb2\n\nmodule, the value will be \nunix:///run/docker/plugins/vb2.sock\n.\n\n\nAdditionally, \nvirtualbox\n sections from each module definition have been\nremoved and now only a single, global \nvirtualbox\n section is present at the\nroot. When accessing properties, a module will first attempt to access a\nproperty defined in the context of the module, but if that fails the property\nlookup will resolve against globally defined keys as well.\n\n\nFinally, the \nlibstorage\n section has been completely removed from the \nvb2\n\nmodule whereas it still remains in the \ndefault-docker\n section. Volume\ncreation requests without an explicit size value sent to the \ndefault-docker\n\nmodule will result in 1GB volumes whereas the same request sent to the \nvb2\n\nmodule will result in 16GB volumes (since 16GB is the default value for the\n\nlibstorage.integration.volume.operations.create.default.size\n property).\n\n\nDefining Service Endpoints\n\n\nMultiple libStorage services can be defined in order to leverage several\ndifferent combinations of storage provider drivers and their respective\nconfigurations. The following section illustrates how to define two separate\nservices, one using the ScaleIO driver and one using VirtualBox:\n\n\nrexray:\n  modules:\n    default-docker:\n      host:     unix:///run/docker/plugins/virtualbox.sock\n      spec:     /etc/docker/plugins/virtualbox.spec\n      libstorage:\n        service: virtualbox\n    scaleio-docker:\n      type:     docker\n      host:     unix:///run/docker/plugins/scaleio.sock\n      spec:     /etc/docker/plugins/scaleio.spec\n      libstorage:\n        service: scaleio\nlibstorage:\n  server:\n    services:\n      scaleio:\n        driver: scaleio\n      virtualbox:\n        driver: virtualbox\nscaleio:\n  endpoint:             https://SCALEIO_GATEWAY/api\n  insecure:             true\n  userName:             SCALEIO_USER\n  password:             SCALEIO_PASS\n  systemName:           SCALEIO_SYSTEM_NAME\n  protectionDomainName: SCALEIO_DOMAIN_NAME\n  storagePoolName:      SCALEIO_STORAG_NAME\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes\n\n\n\n\nOnce the services have been defined, it is then up to the modules to specify\nwhich service to use. Notice how the \ndefault-docker\n module specifies\nthe \nvirtualbox\n service as its \nlibstorage.service\n. Any requests to the\nDocker Volume Plug-in endpoint \n/run/docker/plugins/virtualbox.sock\n will\nutilize the libStorage service \nvirtualbox\n on the backend.\n\n\nDefining a libStorage Server\n\n\nThe following example is very similar to the previous one, but in this instance\nthere is a centralized REX-Ray server which services requests from many\nREX-Ray clients.\n\n\nrexray:\n  modules:\n    default-docker:\n      disabled: true\nlibstorage:\n  host:     tcp://127.0.0.1:7979\n  embedded: true\n  client:\n    type: controller\n  server:\n    endpoints:\n      public:\n        address: tcp://:7979\n    services:\n      scaleio:\n        driver: scaleio\n      virtualbox:\n        driver: virtualbox\nscaleio:\n  endpoint:             https://SCALEIO_GATEWAY/api\n  insecure:             true\n  userName:             SCALEIO_USER\n  password:             SCALEIO_PASS\n  systemName:           SCALEIO_SYSTEM_NAME\n  protectionDomainName: SCALEIO_DOMAIN_NAME\n  storagePoolName:      SCALEIO_STORAG_NAME\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes\n\n\n\n\nOne of the larger differences between the above example and the previous one is\nthe removal of the module definitions. Docker does not communicate with the\ncentral REX-Ray server directly; instead Docker interacts with the REX-Ray\nservices running on the clients via their Docker Volume Endpoints. The client\nREX-Ray instances then send all storage-related requests to the central REX-Ray\nserver.\n\n\nAdditionally, the above sample configuration introduces a few new properties:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nlibstorage.host\n\n\nInstructs local clients which libStorage endpoint to access\n\n\n\n\n\n\nlibstorage.embedded\n\n\nIndicates the libStorage server should be started even though the \nlibstorage.host\n property is defined\n\n\n\n\n\n\nlibstorage.client.type\n\n\nWhen set to \ncontroller\n this property indicates local clients perform no integration activities\n\n\n\n\n\n\nlibstorage.server.endpoints\n\n\nThe available libStorage server HTTP endpoints\n\n\n\n\n\n\n\n\nDefining a libStorage Client\n\n\nThe client configuration is still rather simple. As mentioned in the previous\nsection, the \nrexray.modules\n configuration occurs here. This enables the Docker\nengines running on remote instances to communicate with local REX-Ray exposed\nDocker Volume endpoints that then handle the storage-related requests via the\ncentralized REX-Ray server.\n\n\nrexray:\n  modules:\n    default-docker:\n      host:     unix:///run/docker/plugins/virtualbox.sock\n      spec:     /etc/docker/plugins/virtualbox.spec\n      libstorage:\n        service: virtualbox\n    scaleio-docker:\n      type:     docker\n      host:     unix:///run/docker/plugins/scaleio.sock\n      spec:     /etc/docker/plugins/scaleio.spec\n      libstorage:\n        service: scaleio\nlibstorage:\n  host: tcp://REXRAY_SERVER:7979\n\n\n\n\nlibStorage Configuration\n\n\nREX-Ray embeds both the libStorage client as well as the libStorage server. For\ninformation on configuring the following, please refer to the\n\nlibStorage documentation\n:\n\n\n\n\nVolume options\n\n   such as preemption, disabling operations, etc.\n\n\nFine-tuning \nlogging\n\n\nConfiguring\n\n   OS, integration, and storage drivers\n\n\n\n\nData Directories\n\n\nThe first time REX-Ray is executed it will create several directories if\nthey do not already exist:\n\n\n\n\n/etc/rexray\n\n\n/var/log/rexray\n\n\n/var/run/rexray\n\n\n/var/lib/rexray\n\n\n\n\nThe above directories will contain configuration files, logs, PID files, and\nmounted volumes. However, the location of these directories can also be\ninfluenced with the environment variable \nREXRAY_HOME\n.\n\n\nREXRAY_HOME\n can be used to define a custom home directory for REX-Ray.\nThis directory is irrespective of the actual REX-Ray binary. Instead, the\ndirectory specified in \nREXRAY_HOME\n is the root directory where the REX-Ray\nbinary expects all of the program's data directories to be located.\n\n\nFor example, the following command sets a custom value for \nREXRAY_HOME\n and\nthen gets a volume list:\n\n\nenv REXRAY_HOME=/tmp/rexray rexray volume\n\n\n\n\nThe above command would produce a list of volumes and create the following\ndirectories in the process:\n\n\n\n\n/tmp/rexray/etc/rexray\n\n\n/tmp/rexray/var/log/rexray\n\n\n/tmp/rexray/var/run/rexray\n\n\n/tmp/rexray/var/lib/rexray\n\n\n\n\nThe entire configuration section will refer to the global configuration file as\na file located inside of \n/etc/rexray\n, but it should be noted that if\n\nREXRAY_HOME\n is set the location of the global configuration file can be\nchanged.\n\n\nConfiguration Methods\n\n\nThere are three ways to configure REX-Ray:\n\n\n\n\nCommand line options\n\n\nEnvironment variables\n\n\nConfiguration files\n\n\n\n\nThe order of the items above is also the order of precedence when considering\noptions set in multiple locations that may override one another. Values set\nvia CLI flags have the highest order of precedence, followed by values set by\nenvironment variables, followed, finally, by values set in configuration files.\n\n\nConfiguration Files\n\n\nThere are two REX-Ray configuration files - global and user:\n\n\n\n\n/etc/rexray/config.yml\n\n\n$HOME/.rexray/config.yml\n\n\n\n\nPlease note that while the user configuration file is located inside the user's\nhome directory, this is the directory of the user that starts REX-Ray. And\nif REX-Ray is being started as a service, then \nsudo\n is likely being used,\nwhich means that \n$HOME/.rexray/config.yml\n won't point to \nyour\n home\ndirectory, but rather \n/root/.rexray/config.yml\n.\n\n\nThe next section has an example configuration with the default configuration.\n\n\nConfiguration Properties\n\n\nThe section \nConfiguration Methods\n mentions there are\nthree ways to configure REX-Ray: config files, environment variables, and the\ncommand line. However, this section will illuminate the relationship between the\nnames of the configuration file properties, environment variables, and CLI\nflags.\n\n\nHere is a sample REX-Ray configuration:\n\n\nrexray:\n  logLevel: warn\nlibstorage:\n  service: virtualbox\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes\n\n\n\n\nThe properties \nrexray.logLevel\n, \nlibstorage.service\n, and\n\nvirtualbox.volumePath\n are strings. These values can also be set via\nenvironment variables or command line interface (CLI) flags, but to do so\nrequires knowing the names of the environment variables or CLI flags to use.\nLuckily those are very easy to figure out just by knowing the property names.\n\n\nAll properties that might appear in the REX-Ray configuration file\nfall under some type of heading. For example, take the configuration above:\n\n\nThe rule for environment variables is as follows:\n\n\n\n\nEach nested level becomes a part of the environment variable name followed\n    by an underscore \n_\n except for the terminating part.\n\n\nThe entire environment variable name is uppercase.\n\n\n\n\nNested properties follow these rules for CLI flags:\n\n\n\n\nThe root level's first character is lower-cased with the rest of the root\n    level's text left unaltered.\n\n\nThe remaining levels' first characters are all upper-cased with the the\n    remaining text of that level left unaltered.\n\n\nAll levels are then concatenated together.\n\n\nSee the verbose help for exact global flags using \nrexray --help -v\n\n    as they may be chopped to minimize verbosity.\n\n\n\n\nThe following table illustrates the transformations:\n\n\n\n\n\n\n\n\nProperty Name\n\n\nEnvironment Variable\n\n\nCLI Flag\n\n\n\n\n\n\n\n\n\n\nrexray.logLevel\n\n\nREXRAY_LOGLEVEL\n\n\n--logLevel\n\n\n\n\n\n\nlibstorage.service\n\n\nLIBSTORAGE_SERVICE\n\n\n--libstorageService\n\n\n\n\n\n\nvirtualbox.volumePath\n\n\nVIRTUALBOX_VOLUMEPATH\n\n\n--virtualboxVolumePath\n\n\n\n\n\n\n\n\nLogging Configuration\n\n\nThe REX-Ray log level determines the level of verbosity emitted by the\ninternal logger. The default level is \nwarn\n, but there are three other levels\nas well:\n\n\n\n\n\n\n\n\nLog Level\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nerror\n\n\nLog only errors\n\n\n\n\n\n\nwarn\n\n\nLog errors and anything out of place\n\n\n\n\n\n\ninfo\n\n\nLog errors, warnings, and workflow messages\n\n\n\n\n\n\ndebug\n\n\nLog everything\n\n\n\n\n\n\n\n\nFor example, the following two commands may look slightly different, but they\nare functionally the same, both printing a list of volumes using the \ndebug\n\nlog level:\n\n\nUse the \ndebug\n log level - Example 1\n\n\nrexray volume -l debug\n\n\n\n\nUse the \ndebug\n log level - Example 2\n\n\nenv REXRAY_LOGLEVEL=debug rexray volume", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/config/#configuring-rex-ray", 
            "text": "Tweak this, turn that, peek behind the curtain...", 
            "title": "Configuring REX-Ray"
        }, 
        {
            "location": "/user-guide/config/#overview", 
            "text": "This page reviews how to configure REX-Ray to suit any environment, beginning\nwith the the most common use cases, exploring recommended guidelines, and\nfinally, delving into the details of more advanced settings.", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/config/#basic-configuration", 
            "text": "This section outlines the two most common configuration scenarios encountered\nby REX-Ray's users:   REX-Ray as a stand-alone CLI tool  REX-Ray as a service    note  Please remember to replace the placeholders in the following examples\nwith values valid for the systems on which the examples are executed.  The example below specifies the  volumePath  property as $HOME/VirtualBox/Volumes . While the text  $HOME  will be replaced with\nthe actual value for that environment variable at runtime, the path may\nstill be invalid. The  volumePath  property should reflect a path on the\nsystem on which the VirtualBox server is running, and that is not always\nthe same system on which the  libStorage  server is running.  So please, make sure to update the  volumePath  property for the VirtualBox\ndriver to a path valid on the system on which the VirtualBox server is\nrunning.  The same goes for VirtualBox property  endpoint  as the VirtualBox\nweb service is not always available at  10.0.2.2:18083 .", 
            "title": "Basic Configuration"
        }, 
        {
            "location": "/user-guide/config/#stand-alone-cli-mode", 
            "text": "It is possible to use REX-Ray directly from the command line without any\nconfiguration files. The following example uses REX-Ray to list the storage\nvolumes available to a Linux VM hosted by VirtualBox:   note  The examples below assume that the VirtualBox web server is running on the\nhost OS with authentication disabled and accessible to the guest OS. For\nmore information please refer to the VirtualBox storage driver documentation .   $ rexray volume --service virtualbox\n- attachments:\n  - instanceID:\n      id: e71578b0-1bfb-4fa5-bcd5-4ae982fd4a9b\n      driver: virtualbox\n    status: /Users/akutz/VirtualBox/libStorage/libStorage.vmdk\n    volumeID: 1b819454-a280-4cff-aff5-141f4e8fd154\n  name: libStorage.vmdk\n  size: 64\n  status: /Users/akutz/VirtualBox/libStorage/libStorage.vmdk\n  id: 1b819454-a280-4cff-aff5-141f4e8fd154\n  type:    In addition to listing volumes, the REX-Ray CLI can be used to create and\nremove them as well as manage volume snapshots. For an end-to-end example of\nvolume creation, see  Hello REX-Ray .", 
            "title": "Stand-alone CLI Mode"
        }, 
        {
            "location": "/user-guide/config/#embedded-server-mode", 
            "text": "When operating as a stand-alone CLI, REX-Ray actually loads an embedded\nlibStorage server for the duration of the CLI process and is accessible by\nonly the the process that hosts it. This is known as  Embedded Server Mode .  While commonly used when executing one-off commands with REX-Ray as a\nstand-alone CLI tool, Embedded Server Mode can be utilized when configuring\nREX-Ray to advertise a static libStorage server as well. The following\nqualifications must be met for Embedded Server Mode to be activated:    The property  libstorage.host  must not be defined via configuration file,\n   environment variable, or CLI flag    If the  libstorage.host  property  is  defined then the property\n    libstorage.embedded  can be set to  true  to explicitly activate\n   Embedded Server Mode.    If the  libstorage.host  property is set and  libtorage.embedded  is\n   set to true, Embedded Server Mode will still only activate if the address\n   specified by  libstorage.host  (whether a UNIX socket or TCP port) is\n   not currently in use.", 
            "title": "Embedded Server Mode"
        }, 
        {
            "location": "/user-guide/config/#auto-service-mode", 
            "text": "The Stand-alone CLI Mode  example  also uses the --service  flag. This flag's argument sets the  libstorage.service  property,\nwhich has a special meaning inside of REX-Ray -- it serves to enabled Auto Service Mode .  Services represent unique libStorage endpoints that are available to libStorage\nclients. Each service is associated with a storage driver. Thus\nAuto Service Mode minimizes configuration for simple environments.  The value of the  libstorage.service  property is used to create a default\nservice configured with a storage driver. This special mode is only activated\nif all of the following conditions are met:   The  libstorage.service  property is set via:  The CLI flags  -s|--service  or  --libstorageService  The environment variable  LIBSTORAGE_SERVICE  The configuration file property  libstorage.service    The  libstorage.host  property is  not  set. This property can be set via:  The CLI flags  -h|--host  or  --libstorageHost  The environment variable  LIBSTORAGE_HOST  The configuration file property  libstorage.host    The configuration property  libstorage.server.services  must  not  be set.\n    This property is only configurable via a configuration file.   Because the above example met the auto service mode conditions, REX-Ray\ncreated a service named  virtualbox  configured to use the  virtualbox  driver.\nThis service runs on the libStorage server embedded inside of REX-Ray and is\naccessible only by the executing CLI process for the duration of said process.\nWhen used in this manner, the service name must also be a valid driver name.", 
            "title": "Auto Service Mode"
        }, 
        {
            "location": "/user-guide/config/#service-mode", 
            "text": "REX-Ray can also run as a persistent service that advertises both Docker Volume Plug-in \nand  libStorage  endpoints.", 
            "title": "Service Mode"
        }, 
        {
            "location": "/user-guide/config/#docker-volume-plug-in", 
            "text": "This section refers to the only operational mode that REX-Ray supported in\nversions 0.3.3 and prior. A UNIX socket is created by REX-Ray that serves as a\nDocker Volume Plugin compliant API endpoint. Docker is able to leverage this\nendpoint to deliver on-demand, persistent storage to containers.  The following is a simple example of a configuration file that should be\nlocated at  /etc/rexray/config.yml . This file can be used to configure the\nsame options that were specified in the previous CLI example. Please see the advanced section  for a complete list of\nconfiguration options.  libstorage:\n  service: virtualbox\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes  Once the configuration file is in place,  rexray service start  can be used to\nstart the service. Sometimes it is also useful to add  -l debug  to enable\nmore verbose logging. Additionally, it's also occasionally beneficial to\nstart the service in the foreground with the  -f  flag.  $ rexray start\n\nStarting REX-Ray...SUCCESS!\n\n  The REX-Ray daemon is now running at PID 15724. To\n  shutdown the daemon execute the following command:\n\n    sudo /usr/bin/rexray stop  At this point requests can now be made to the default Docker Volume Plugin\nand Volume Driver advertised by the UNIX socket  rexray  at /run/docker/plugins/rexray.sock . More details on configuring the Docker\nVolume Plug-in are available on the  Schedulers  page.", 
            "title": "Docker Volume Plug-in"
        }, 
        {
            "location": "/user-guide/config/#libstorage-server-and-client", 
            "text": "In addition to  Embedded Server Mode , REX-Ray can also\nexpose the libStorage API statically. This enables REX-Ray to server and a\nlibStorage server and perform only a storage abstraction role.  If the desire is to establish a centralized REX-Ray server that is called\non from remote REX-Ray instances then the following example will be useful.\nThe first configuration is for running REX-Ray purely as a libStorage server.\nThe second defines how one would would use one or more REX-Ray instances in a\nlibStorage client role.  The following examples require multiple systems in order to fulfill these\ndifferent roles. The  Hello REX-Ray  section on\nthe front page has an end-to-end illustration of this use case that leverages\nVagrant to provide and configure the necessary systems.", 
            "title": "libStorage Server and Client"
        }, 
        {
            "location": "/user-guide/config/#libstorage-server", 
            "text": "The example below illustrates the necessary settings for configuring REX-Ray\nas a libStorage server:  rexray:\n  modules:\n    default-docker:\n      disabled: true\nlibstorage:\n  host: tcp://127.0.0.1:7979\n  embedded: true\n  client:\n    type: controller\n  server:\n    endpoints:\n      public:\n        address: tcp://:7979\n    services:\n      virtualbox:\n        driver: virtualbox\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes  In the above sample, the default Docker module is disabled. This means that\nwhile the REX-Ray service would be running, it would not be available to\nDocker on that host.  The  libstorage  section defines the settings that configure the libStorage\nserver:     Property  Description      libstorage.host  Instructs local clients which libStorage endpoint to access    libstorage.embedded  Indicates the libStorage server should be started even though the  libstorage.host  property is defined    libstorage.client.type  When set to  controller  this property indicates local clients perform no integration activities    libstorage.server.endpoints  The available libStorage server HTTP endpoints    libstorage.server.services  The configured libStorage services     Start the REX-Ray service with  rexray service start .", 
            "title": "libStorage Server"
        }, 
        {
            "location": "/user-guide/config/#libstorage-client", 
            "text": "On a separate OS instance running REX-Ray, the follow command can be used to\nlist the instance's available VirtualBox storage volumes:  $ rexray volume -h tcp://REXRAY_SERVER:7979 -s virtualbox  An alternative to the above CLI flags is to add them as persistent settings\nto the  /etc/rexray/config.yml  configuration file on this instance:  libstorage:\n  host:    tcp://REXRAY_SERVER:7979\n  service: virtualbox  Now the above command can be simplified further:  $ rexray volume  Once more, the REX-Ray service can be started with  rexray service start  and\nthe REX-Ray Docker Volume Plug-in endpoint will utilize the remote libStorage\nserver as its method for communicating with VirtualBox.  Again, a complete end-to-end Vagrant environment for the above example is\navailable at  Hello REX-Ray .", 
            "title": "libStorage Client"
        }, 
        {
            "location": "/user-guide/config/#example-sans-modules", 
            "text": "Lets review the major sections of the configuration file:  rexray:\n  logLevel: warn\nlibstorage:\n  service: virtualbox\n  integration:\n    volume:\n      operations:\n        create:\n          default:\n            size: 1\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes  Settings occur in three primary areas:   rexray  libstorage  virtualbox   The  rexray  section contains all properties specific to REX-Ray. The\nYAML property path  rexray.logLevel  defines the log level for REX-Ray and its\nchild components. All of the  rexray  properties are documented  below.  Next, the  libstorage  section defines the service with which REX-Ray will\ncommunicate via the property  libstorage.service . This property also enables\nthe  Auto Service Mode  discussed above since this\nconfiguration example does not define a host or services section. For all\ninformation related to libStorage and its properties, please refer to the libStorage documentation .  Finally, the  virtualbox  section configures the VirtualBox driver selected\nor loaded by REX-Ray, as indicated via the  libstorage.service  property. The\nlibStorage Storage Drivers page has information about the configuration details\nof  each driver ,\nincluding  VirtualBox .", 
            "title": "Example sans Modules"
        }, 
        {
            "location": "/user-guide/config/#logging", 
            "text": "The  -l|--logLevel  option or  rexray.logLevel  configuration key can be set\nto any of the following values to increase or decrease the verbosity of the\ninformation logged to the console or the REX-Ray log file (defaults to /var/log/rexray/rexray.log ).   panic  fatal  error  warn  info  debug", 
            "title": "Logging"
        }, 
        {
            "location": "/user-guide/config/#troubleshooting", 
            "text": "The command  rexray env  can be used to print out the runtime interpretation\nof the environment, including configured properties, in order to help diagnose\nconfiguration issues.  $ rexray env | grep DEFAULT | sort -r\nREXRAY_MODULES_DEFAULT-DOCKER_TYPE=docker\nREXRAY_MODULES_DEFAULT-DOCKER_SPEC=/etc/docker/plugins/rexray.spec\nREXRAY_MODULES_DEFAULT-DOCKER_LIBSTORAGE_SERVICE=vfs\nREXRAY_MODULES_DEFAULT-DOCKER_HOST=unix:///run/docker/plugins/rexray.sock\nREXRAY_MODULES_DEFAULT-DOCKER_DISABLED=false\nREXRAY_MODULES_DEFAULT-DOCKER_DESC=The default docker module.\nREXRAY_MODULES_DEFAULT-ADMIN_TYPE=admin\nREXRAY_MODULES_DEFAULT-ADMIN_HOST=unix:///var/run/rexray/server.sock\nREXRAY_MODULES_DEFAULT-ADMIN_DISABLED=false\nREXRAY_MODULES_DEFAULT-ADMIN_DESC=The default admin module.\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_TYPE=\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_SIZE=16\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_IOPS=\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_FSTYPE=ext4\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_AVAILABILITYZONE=", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/user-guide/config/#advanced-configuration", 
            "text": "The following sections detail every last aspect of how REX-Ray works and can\nbe configured.", 
            "title": "Advanced Configuration"
        }, 
        {
            "location": "/user-guide/config/#example-with-modules", 
            "text": "Modules enable a single REX-Ray instance to present multiple personalities or\nvolume endpoints, serving hosts that require access to multiple storage\nplatforms.", 
            "title": "Example with Modules"
        }, 
        {
            "location": "/user-guide/config/#defining-modules", 
            "text": "The following example demonstrates a basic configuration that presents two\nmodules using the VirtualBox driver:  default-docker  and  vb2-module .  rexray:\n  logLevel: warn\n  modules:\n    default-docker:\n      type: docker\n      desc: The default docker module.\n      host: unix:///run/docker/plugins/vb1.sock\n      libstorage:\n        service: virtualbox\n        integration:\n          volume:\n            operations:\n              create:\n                default:\n                  size: 1\n      virtualbox:\n        volumePath: $HOME/VirtualBox/Volumes\n    vb2-module:\n      type: docker\n      desc: The second docker module.\n      host: unix:///run/docker/plugins/vb2.sock\n      libstorage:\n        service: virtualbox\n        integration:\n          volume:\n            operations:\n              create:\n                default:\n                  size: 1\n      virtualbox:\n        volumePath: $HOME/VirtualBox/Volumes\nlibstorage:\n  service: virtualbox  Whereas the previous example did not use modules and the example above does,\nthey both begin by defining the root section  rexray . Unlike the previous\nexample, however, the majority of the  libstorage  section and all of the virtualbox  section are no longer at the root. Instead the section rexray.modules  is defined. The  modules  key in the  rexray  section is where\nall modules are configured. Each key that is a child of  modules  represents the\nname of a module.   note  Please note that while most of the  libstorage  section has been relocated\nas a child of each module, the  libstorage.service  property is still\ndefined at the root to activate  Auto Service Mode  as\na quick-start method of property configuring the embedded libStorage server.   The above example defines two modules:    default-module  This is a special module, and it's always defined, even if not explicitly\nlisted. In the previous example without modules, the  libstorage  and virtualbox  sections at the root actually informed the configuration of\nthe implicit  default-docker  module. In this example the explicit\ndeclaration of the  default-docker  module enables several of its\nproperties to be overridden and given desired values. The Advanced\nConfiguration section has more information on Default Modules .    vb2-module  This is a new, custom module configured almost identically to the default-module  with the exception of a unique host address as defined\nby the module's  host  key.    Notice that both modules share many of the same properties and values. In fact,\nwhen defining both modules, the top-level  libstorage  and  virtualbox  sections\nwere simply copied into each module as sub-sections. This is perfectly valid\nas any configuration path that begins from the root of the REX-Ray\nconfiguration file can be duplicated beginning as a child of a module\ndefinition. This allows global settings to be overridden just for a specific\nmodules.  As noted, each module shares identical values with the exception of the module's\nname and host. The host is the address used by Docker to communicate with\nREX-Ray. The base name of the socket file specified in the address can be\nused with  docker --volume-driver= . With the current example the value of the --volume-driver  parameter would be either  vb1  of  vb2 .", 
            "title": "Defining Modules"
        }, 
        {
            "location": "/user-guide/config/#modules-and-inherited-properties", 
            "text": "There is also another way to write the previous example while reducing the\nnumber of repeated, identical properties shared by two modules.  rexray:\n  logLevel: warn\n  modules:\n    default-docker:\n      host: unix:///run/docker/plugins/vb1.sock\n      libstorage:\n        integration:\n          volume:\n            operations:\n              create:\n                default:\n                  size: 1\n    vb2:\n      type: docker\nlibstorage:\n  service: virtualbox\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes  The above example may look strikingly different than the previous one, but it's\nactually the same with just a few tweaks.  While there are still two modules defined, the second one has been renamed from vb2-module  to  vb2 . The change is a more succinct way to represent the same\nintent, and it also provides a nice side-effect. If the  host  key is omitted\nfrom a Docker module, the value for the  host  key is automatically generated\nusing the module's name. Therefore since there is no  host  key for the  vb2 \nmodule, the value will be  unix:///run/docker/plugins/vb2.sock .  Additionally,  virtualbox  sections from each module definition have been\nremoved and now only a single, global  virtualbox  section is present at the\nroot. When accessing properties, a module will first attempt to access a\nproperty defined in the context of the module, but if that fails the property\nlookup will resolve against globally defined keys as well.  Finally, the  libstorage  section has been completely removed from the  vb2 \nmodule whereas it still remains in the  default-docker  section. Volume\ncreation requests without an explicit size value sent to the  default-docker \nmodule will result in 1GB volumes whereas the same request sent to the  vb2 \nmodule will result in 16GB volumes (since 16GB is the default value for the libstorage.integration.volume.operations.create.default.size  property).", 
            "title": "Modules and Inherited Properties"
        }, 
        {
            "location": "/user-guide/config/#defining-service-endpoints", 
            "text": "Multiple libStorage services can be defined in order to leverage several\ndifferent combinations of storage provider drivers and their respective\nconfigurations. The following section illustrates how to define two separate\nservices, one using the ScaleIO driver and one using VirtualBox:  rexray:\n  modules:\n    default-docker:\n      host:     unix:///run/docker/plugins/virtualbox.sock\n      spec:     /etc/docker/plugins/virtualbox.spec\n      libstorage:\n        service: virtualbox\n    scaleio-docker:\n      type:     docker\n      host:     unix:///run/docker/plugins/scaleio.sock\n      spec:     /etc/docker/plugins/scaleio.spec\n      libstorage:\n        service: scaleio\nlibstorage:\n  server:\n    services:\n      scaleio:\n        driver: scaleio\n      virtualbox:\n        driver: virtualbox\nscaleio:\n  endpoint:             https://SCALEIO_GATEWAY/api\n  insecure:             true\n  userName:             SCALEIO_USER\n  password:             SCALEIO_PASS\n  systemName:           SCALEIO_SYSTEM_NAME\n  protectionDomainName: SCALEIO_DOMAIN_NAME\n  storagePoolName:      SCALEIO_STORAG_NAME\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes  Once the services have been defined, it is then up to the modules to specify\nwhich service to use. Notice how the  default-docker  module specifies\nthe  virtualbox  service as its  libstorage.service . Any requests to the\nDocker Volume Plug-in endpoint  /run/docker/plugins/virtualbox.sock  will\nutilize the libStorage service  virtualbox  on the backend.", 
            "title": "Defining Service Endpoints"
        }, 
        {
            "location": "/user-guide/config/#defining-a-libstorage-server", 
            "text": "The following example is very similar to the previous one, but in this instance\nthere is a centralized REX-Ray server which services requests from many\nREX-Ray clients.  rexray:\n  modules:\n    default-docker:\n      disabled: true\nlibstorage:\n  host:     tcp://127.0.0.1:7979\n  embedded: true\n  client:\n    type: controller\n  server:\n    endpoints:\n      public:\n        address: tcp://:7979\n    services:\n      scaleio:\n        driver: scaleio\n      virtualbox:\n        driver: virtualbox\nscaleio:\n  endpoint:             https://SCALEIO_GATEWAY/api\n  insecure:             true\n  userName:             SCALEIO_USER\n  password:             SCALEIO_PASS\n  systemName:           SCALEIO_SYSTEM_NAME\n  protectionDomainName: SCALEIO_DOMAIN_NAME\n  storagePoolName:      SCALEIO_STORAG_NAME\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes  One of the larger differences between the above example and the previous one is\nthe removal of the module definitions. Docker does not communicate with the\ncentral REX-Ray server directly; instead Docker interacts with the REX-Ray\nservices running on the clients via their Docker Volume Endpoints. The client\nREX-Ray instances then send all storage-related requests to the central REX-Ray\nserver.  Additionally, the above sample configuration introduces a few new properties:     Property  Description      libstorage.host  Instructs local clients which libStorage endpoint to access    libstorage.embedded  Indicates the libStorage server should be started even though the  libstorage.host  property is defined    libstorage.client.type  When set to  controller  this property indicates local clients perform no integration activities    libstorage.server.endpoints  The available libStorage server HTTP endpoints", 
            "title": "Defining a libStorage Server"
        }, 
        {
            "location": "/user-guide/config/#defining-a-libstorage-client", 
            "text": "The client configuration is still rather simple. As mentioned in the previous\nsection, the  rexray.modules  configuration occurs here. This enables the Docker\nengines running on remote instances to communicate with local REX-Ray exposed\nDocker Volume endpoints that then handle the storage-related requests via the\ncentralized REX-Ray server.  rexray:\n  modules:\n    default-docker:\n      host:     unix:///run/docker/plugins/virtualbox.sock\n      spec:     /etc/docker/plugins/virtualbox.spec\n      libstorage:\n        service: virtualbox\n    scaleio-docker:\n      type:     docker\n      host:     unix:///run/docker/plugins/scaleio.sock\n      spec:     /etc/docker/plugins/scaleio.spec\n      libstorage:\n        service: scaleio\nlibstorage:\n  host: tcp://REXRAY_SERVER:7979", 
            "title": "Defining a libStorage Client"
        }, 
        {
            "location": "/user-guide/config/#libstorage-configuration", 
            "text": "REX-Ray embeds both the libStorage client as well as the libStorage server. For\ninformation on configuring the following, please refer to the libStorage documentation :   Volume options \n   such as preemption, disabling operations, etc.  Fine-tuning  logging  Configuring \n   OS, integration, and storage drivers", 
            "title": "libStorage Configuration"
        }, 
        {
            "location": "/user-guide/config/#data-directories", 
            "text": "The first time REX-Ray is executed it will create several directories if\nthey do not already exist:   /etc/rexray  /var/log/rexray  /var/run/rexray  /var/lib/rexray   The above directories will contain configuration files, logs, PID files, and\nmounted volumes. However, the location of these directories can also be\ninfluenced with the environment variable  REXRAY_HOME .  REXRAY_HOME  can be used to define a custom home directory for REX-Ray.\nThis directory is irrespective of the actual REX-Ray binary. Instead, the\ndirectory specified in  REXRAY_HOME  is the root directory where the REX-Ray\nbinary expects all of the program's data directories to be located.  For example, the following command sets a custom value for  REXRAY_HOME  and\nthen gets a volume list:  env REXRAY_HOME=/tmp/rexray rexray volume  The above command would produce a list of volumes and create the following\ndirectories in the process:   /tmp/rexray/etc/rexray  /tmp/rexray/var/log/rexray  /tmp/rexray/var/run/rexray  /tmp/rexray/var/lib/rexray   The entire configuration section will refer to the global configuration file as\na file located inside of  /etc/rexray , but it should be noted that if REXRAY_HOME  is set the location of the global configuration file can be\nchanged.", 
            "title": "Data Directories"
        }, 
        {
            "location": "/user-guide/config/#configuration-methods", 
            "text": "There are three ways to configure REX-Ray:   Command line options  Environment variables  Configuration files   The order of the items above is also the order of precedence when considering\noptions set in multiple locations that may override one another. Values set\nvia CLI flags have the highest order of precedence, followed by values set by\nenvironment variables, followed, finally, by values set in configuration files.", 
            "title": "Configuration Methods"
        }, 
        {
            "location": "/user-guide/config/#configuration-files", 
            "text": "There are two REX-Ray configuration files - global and user:   /etc/rexray/config.yml  $HOME/.rexray/config.yml   Please note that while the user configuration file is located inside the user's\nhome directory, this is the directory of the user that starts REX-Ray. And\nif REX-Ray is being started as a service, then  sudo  is likely being used,\nwhich means that  $HOME/.rexray/config.yml  won't point to  your  home\ndirectory, but rather  /root/.rexray/config.yml .  The next section has an example configuration with the default configuration.", 
            "title": "Configuration Files"
        }, 
        {
            "location": "/user-guide/config/#configuration-properties", 
            "text": "The section  Configuration Methods  mentions there are\nthree ways to configure REX-Ray: config files, environment variables, and the\ncommand line. However, this section will illuminate the relationship between the\nnames of the configuration file properties, environment variables, and CLI\nflags.  Here is a sample REX-Ray configuration:  rexray:\n  logLevel: warn\nlibstorage:\n  service: virtualbox\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes  The properties  rexray.logLevel ,  libstorage.service , and virtualbox.volumePath  are strings. These values can also be set via\nenvironment variables or command line interface (CLI) flags, but to do so\nrequires knowing the names of the environment variables or CLI flags to use.\nLuckily those are very easy to figure out just by knowing the property names.  All properties that might appear in the REX-Ray configuration file\nfall under some type of heading. For example, take the configuration above:  The rule for environment variables is as follows:   Each nested level becomes a part of the environment variable name followed\n    by an underscore  _  except for the terminating part.  The entire environment variable name is uppercase.   Nested properties follow these rules for CLI flags:   The root level's first character is lower-cased with the rest of the root\n    level's text left unaltered.  The remaining levels' first characters are all upper-cased with the the\n    remaining text of that level left unaltered.  All levels are then concatenated together.  See the verbose help for exact global flags using  rexray --help -v \n    as they may be chopped to minimize verbosity.   The following table illustrates the transformations:     Property Name  Environment Variable  CLI Flag      rexray.logLevel  REXRAY_LOGLEVEL  --logLevel    libstorage.service  LIBSTORAGE_SERVICE  --libstorageService    virtualbox.volumePath  VIRTUALBOX_VOLUMEPATH  --virtualboxVolumePath", 
            "title": "Configuration Properties"
        }, 
        {
            "location": "/user-guide/config/#logging-configuration", 
            "text": "The REX-Ray log level determines the level of verbosity emitted by the\ninternal logger. The default level is  warn , but there are three other levels\nas well:     Log Level  Description      error  Log only errors    warn  Log errors and anything out of place    info  Log errors, warnings, and workflow messages    debug  Log everything     For example, the following two commands may look slightly different, but they\nare functionally the same, both printing a list of volumes using the  debug \nlog level:  Use the  debug  log level - Example 1  rexray volume -l debug  Use the  debug  log level - Example 2  env REXRAY_LOGLEVEL=debug rexray volume", 
            "title": "Logging Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/", 
            "text": "Storage Providers\n\n\nConnecting storage and platforms...\n\n\n\n\nOverview\n\n\nThe list of storage providers supported by REX-Ray now mirrors the validated\nstorage platform table from the libStorage project.\n\n\n\n\nnote\n\n\nThe initial REX-Ray 0.4.x release omits support for several,\npreviously verified storage platforms. These providers will be\nreintroduced incrementally, beginning with 0.4.1. If an absent driver\nprevents the use of REX-Ray, please continue to use 0.3.3 until such time\nthe storage platform is introduced in REX-Ray 0.4.x. Instructions on how\nto install REX-Ray 0.3.3 may be found\n\nhere\n.\n\n\n\n\nSupported Providers\n\n\nThe following storage providers and platforms are supported by REX-Ray.\n\n\n\n\n\n\n\n\nProvider\n\n\nStorage Platform(s)\n\n\n\n\n\n\n\n\n\n\nEMC\n\n\nScaleIO\n, \nIsilon\n\n\n\n\n\n\nOracle VirtualBox\n\n\nVirtual Media\n\n\n\n\n\n\n\n\nComing Soon\n\n\nSupport for the following storage providers will be reintroduced in upcoming\nreleases:\n\n\n\n\n\n\n\n\nProvider\n\n\nStorage Platform(s)\n\n\n\n\n\n\n\n\n\n\nAmazon EC2\n\n\nEBS\n\n\n\n\n\n\nGoogle Compute Engine (GCE)\n\n\nDisk\n\n\n\n\n\n\nOpen Stack\n\n\nCinder\n\n\n\n\n\n\nRackspace\n\n\nCinder\n\n\n\n\n\n\nEMC\n\n\nXtremIO, VMAX", 
            "title": "Storage Providers"
        }, 
        {
            "location": "/user-guide/storage-providers/#storage-providers", 
            "text": "Connecting storage and platforms...", 
            "title": "Storage Providers"
        }, 
        {
            "location": "/user-guide/storage-providers/#overview", 
            "text": "The list of storage providers supported by REX-Ray now mirrors the validated\nstorage platform table from the libStorage project.   note  The initial REX-Ray 0.4.x release omits support for several,\npreviously verified storage platforms. These providers will be\nreintroduced incrementally, beginning with 0.4.1. If an absent driver\nprevents the use of REX-Ray, please continue to use 0.3.3 until such time\nthe storage platform is introduced in REX-Ray 0.4.x. Instructions on how\nto install REX-Ray 0.3.3 may be found here .", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/storage-providers/#supported-providers", 
            "text": "The following storage providers and platforms are supported by REX-Ray.     Provider  Storage Platform(s)      EMC  ScaleIO ,  Isilon    Oracle VirtualBox  Virtual Media", 
            "title": "Supported Providers"
        }, 
        {
            "location": "/user-guide/storage-providers/#coming-soon", 
            "text": "Support for the following storage providers will be reintroduced in upcoming\nreleases:     Provider  Storage Platform(s)      Amazon EC2  EBS    Google Compute Engine (GCE)  Disk    Open Stack  Cinder    Rackspace  Cinder    EMC  XtremIO, VMAX", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/user-guide/schedulers/", 
            "text": "Schedulers\n\n\nScheduling storage one resource at a time...\n\n\n\n\nOverview\n\n\nThis page reviews the scheduling systems supported by REX-Ray.\n\n\nDocker\n\n\nThe majority of the documentation for the Docker integration driver has been\n\nrelocated\n\nto the libStorage project.\n\n\nExternal Access\n\n\nBy default, REX-Ray's embedded Docker Volume Plug-in endpoint handles\nrequests from the local Docker service via a UNIX socket. Doing so\nrestricts the endpoint to the localhost, increasing network security by removing\na possible attack vector. If an externally accessible Docker Volume Plug-in\nendpoint is required, it's still possible to create one by overriding the\naddress for the \ndefault-docker\n module in REX-Ray's configuration file:\n\n\nrexray:\n  modules:\n    default-docker:\n      host: tcp://:7981\n\n\n\n\nThe above example illustrates how to override the \ndefault-docker\n module's\nendpoint address. The value \ntcp://:7981\n instructs the Docker Volume Plug-in\nto listen on port 7981 for all configured interfaces.\n\n\nUsing a TCP endpoint has a side-effect however -- the local Docker instance\nwill not know about the Volume Plug-in endpoint as there is no longer a UNIX\nsocket file in the directory the Docker service continually scans.\n\n\nOn the local system, and in fact on all systems where the Docker service needs\nto know about this externally accessible Volume Plug-in endpoint, a spec file\nmust be created at \n/etc/docker/plugins/rexray.spec\n. Inside this file simply\ninclude a single line with the network address of the endpoint. For example:\n\n\ntcp://192.168.56.20:7981\n\n\n\n\nWith a spec file located at \n/etc/docker/plugins/rexray.spec\n that contains\nthe above contents, Docker instances will query the Volume Plug-in endpoint at\n\ntcp://192.168.56.20:7981\n when volume requests are received.\n\n\nVolume Management\n\n\nThe \nvolume\n sub-command for Docker 1.12+ should look similar to the following:\n\n\n$ docker volume\n\nUsage:  docker volume [OPTIONS] [COMMAND]\n\nManage Docker volumes\n\nCommands:\n  create                   Create a volume\n  inspect                  Return low-level information on a volume\n  ls                       List volumes\n  rm                       Remove a volume\n\n\n\n\nList Volumes\n\n\nThe list command reviews a list of available volumes that have been discovered\nvia Docker Volume Plug-in endpoints such as REX-Ray. Each volume name is\nexpected to be unique. Thus volume names must also be unique across all\nendpoints, and in turn, across all storage platforms exposed by REX-Ray.\n\n\nWith the exception of the \nlocal\n driver, the list of returned volumes is\ngenerated by the backend storage platform to which the configured driver\ncommunicates:\n\n\n$ docker volume ls\nDRIVER              VOLUME NAME\nlocal               local1\nscaleio             Volume-001\nvirtualbox          vbox1\n\n\n\n\nInspect Volume\n\n\nThe inspect command can be used to retrieve details about a volume related to\nboth Docker and the underlying storage platform. The fields listed under\n\nStatus\n are all generated by REX-Ray, including \nSize in GB\n, \nVolume Type\n,\nand \nAvailability Zone\n.\n\n\nThe \nScope\n parameter ensures that when the specified volume driver is\ninspected by multiple Docker hosts, the volumes tagged as \nglobal\n are all\ninterpreted as the same volume. This reduces unnecessary round-trips in\nsituations where an application such as Docker Swarm is connected to hosts\nconfigured with REX-Ray.\n\n\n$ docker volume inspect vbox1\n[\n    {\n        \nName\n: \nvbox1\n,\n        \nDriver\n: \nvirtualbox\n,\n        \nMountpoint\n: \n,\n        \nStatus\n: {\n            \navailabilityZone\n: \n,\n            \nfields\n: null,\n            \niops\n: 0,\n            \nname\n: \nvbox1\n,\n            \nserver\n: \nvirtualbox\n,\n            \nservice\n: \nvirtualbox\n,\n            \nsize\n: 8,\n            \ntype\n: \n\n        },\n        \nLabels\n: {},\n        \nScope\n: \nglobal\n\n    }\n]\n\n\n\n\nCreate Volume\n\n\nDocker's \nvolume create\n command enables the creation of new volumes on the\nunderlying storage platform. Newly created volumes are available immediately\nto be attached and mounted. The \nvolume create\n command also supports the CLI\nflag \n-o|--opt\n in order to support providing custom data to the volume creation\nworkflow:\n\n\n$ docker volume create --driver=virtualbox --name=vbox2 --opt=size=2\nvbox2\n\n\n\n\nAdditional, valid options for the \n-o|--opt\n parameter include:\n\n\n\n\n\n\n\n\noption\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nsize\n\n\nSize in GB\n\n\n\n\n\n\nIOPS\n\n\nIOPS\n\n\n\n\n\n\nvolumeType\n\n\nType of Volume or Storage Pool\n\n\n\n\n\n\nvolumeName\n\n\nCreate from an existing volume name\n\n\n\n\n\n\nvolumeID\n\n\nCreate from an existing volume ID\n\n\n\n\n\n\nsnapshotName\n\n\nCreate from an existing snapshot name\n\n\n\n\n\n\nsnapshotID\n\n\nCreate from an existing snapshot ID\n\n\n\n\n\n\n\n\nRemove Volume\n\n\nA volume may be removed once it is no longer in use by a container, running or\notherwise. The process of removing a container actually causes the volume to\nbe removed if that is the last container to leverage said volume:\n\n\n$ docker volume rm vbox2\n\n\n\n\nContainers with Volumes\n\n\nPlease review the \nApplications\n section for information on\nconfiguring popular applications with persistent storage via Docker and REX-Ray.\n\n\nMesos\n\n\nIn Mesos the frameworks are responsible for receiving requests from\nconsumers and then proceeding to schedule and manage tasks. While some\nframeworks, like Marathon, are open to run any workload for sustained periods\nof time, others are use case specific, such as Cassandra. Frameworks may\nalso receive requests from other platforms in addition to schedulers instead of\nconsumers such as Cloud Foundry, Kubernetes, and Swarm.\n\n\nOnce a resource offer is accepted from Mesos, tasks are launched to support the\nassociated workloads. These tasks are eventually distributed to Mesos agents in\norder to spin up containers.\n\n\nREX-Ray enables on-demand storage allocation for agents receiving tasks via\ntwo deployment configurations:\n\n\n\n\n\n\nDocker Containerizer with Marathon\n\n\n\n\n\n\nMesos Containerizer with Marathon\n\n\n\n\n\n\nDocker Containerizer with Marathon\n\n\nWhen the framework leverages the Docker containerizer, Docker and REX-Ray\nshould both already be configured and working. The following example shows\nhow to use Marathon in order to bring an application online with external\nvolumes:\n\n\n{\n    \nid\n: \nnginx\n,\n    \ncontainer\n: {\n        \ndocker\n: {\n            \nimage\n: \nmillion12/nginx\n,\n            \nnetwork\n: \nBRIDGE\n,\n            \nportMappings\n: [{\n                \ncontainerPort\n: 80,\n                \nhostPort\n: 0,\n                \nprotocol\n: \ntcp\n\n            }],\n            \nparameters\n: [{\n                \nkey\n: \nvolume-driver\n,\n                \nvalue\n: \nrexray\n\n            }, {\n                \nkey\n: \nvolume\n,\n                \nvalue\n: \nnginx-data:/data/www\n\n            }]\n        }\n    },\n    \ncpus\n: 0.2,\n    \nmem\n: 32.0,\n    \ninstances\n: 1\n}\n\n\n\n\nMesos Containerizer with Marathon\n\n\nMesos 0.23+ includes modules that enable extensibility for different\nportions the architecture. The \ndvdcli\n and\n\nmesos-module-dvdi\n projects are\nrequired for to enable external volume support with the native containerizer.\n\n\nThe next example is similar to the one above, except in this instance the\nnative containerizer is preferred and volume requests are handled by the\n\nenv\n section.\n\n\n{\n  \nid\n: \nhello-play\n,\n  \ncmd\n: \nwhile [ true ] ; do touch /var/lib/rexray/volumes/test12345/hello ; sleep 5 ; done\n,\n  \nmem\n: 32,\n  \ncpus\n: 0.1,\n  \ninstances\n: 1,\n  \nenv\n: {\n    \nDVDI_VOLUME_NAME\n: \ntest12345\n,\n    \nDVDI_VOLUME_DRIVER\n: \nrexray\n,\n    \nDVDI_VOLUME_OPTS\n: \nsize=5,iops=150,volumetype=io1,newfstype=xfs,overwritefs=true\n\n  }\n}\n\n\n\n\nThis example also illustrates several important settings for the native method.\nWhile the VirtualBox driver is being used, any validated storage platform\nshould work. Additionally, there are two options recommended for this type of\nconfiguration:\n\n\n\n\n\n\n\n\nProperty\n\n\nRecommendation\n\n\n\n\n\n\n\n\n\n\nlibstorage.integration.volume.operations.mount.preempt\n\n\nSetting this flag to true ensures any host can preempt control of a volume from other hosts\n\n\n\n\n\n\nlibstorage.integration.volume.operations.unmount.ignoreUsedCount\n\n\nEnabling this flag declares that \nmesos-module-dvdi\n is the authoritative source for deciding when to unmount volumes\n\n\n\n\n\n\n\n\nPlease refer to the libStorage documentation for more information on\n\nVolume Configuration\n\noptions.\n\n\n\n\nnote\n\n\nThe \nlibstorage.integration.volume.operations.remove.disable\n property can\nprevent the scheduler from removing volumes. Setting this flag to \ntrue\n is\nrecommended when using Mesos with Docker 1.9.1 or earlier.\n\n\n\n\nlibstorage:\n  service: virtualbox\n  integration:\n    volume:\n      operations:\n        mount:\n          preempt: true\n        unmount:\n          ignoreusedcount: true\n        remove:\n          disable: true\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes", 
            "title": "Schedulers"
        }, 
        {
            "location": "/user-guide/schedulers/#schedulers", 
            "text": "Scheduling storage one resource at a time...", 
            "title": "Schedulers"
        }, 
        {
            "location": "/user-guide/schedulers/#overview", 
            "text": "This page reviews the scheduling systems supported by REX-Ray.", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/schedulers/#docker", 
            "text": "The majority of the documentation for the Docker integration driver has been relocated \nto the libStorage project.", 
            "title": "Docker"
        }, 
        {
            "location": "/user-guide/schedulers/#external-access", 
            "text": "By default, REX-Ray's embedded Docker Volume Plug-in endpoint handles\nrequests from the local Docker service via a UNIX socket. Doing so\nrestricts the endpoint to the localhost, increasing network security by removing\na possible attack vector. If an externally accessible Docker Volume Plug-in\nendpoint is required, it's still possible to create one by overriding the\naddress for the  default-docker  module in REX-Ray's configuration file:  rexray:\n  modules:\n    default-docker:\n      host: tcp://:7981  The above example illustrates how to override the  default-docker  module's\nendpoint address. The value  tcp://:7981  instructs the Docker Volume Plug-in\nto listen on port 7981 for all configured interfaces.  Using a TCP endpoint has a side-effect however -- the local Docker instance\nwill not know about the Volume Plug-in endpoint as there is no longer a UNIX\nsocket file in the directory the Docker service continually scans.  On the local system, and in fact on all systems where the Docker service needs\nto know about this externally accessible Volume Plug-in endpoint, a spec file\nmust be created at  /etc/docker/plugins/rexray.spec . Inside this file simply\ninclude a single line with the network address of the endpoint. For example:  tcp://192.168.56.20:7981  With a spec file located at  /etc/docker/plugins/rexray.spec  that contains\nthe above contents, Docker instances will query the Volume Plug-in endpoint at tcp://192.168.56.20:7981  when volume requests are received.", 
            "title": "External Access"
        }, 
        {
            "location": "/user-guide/schedulers/#volume-management", 
            "text": "The  volume  sub-command for Docker 1.12+ should look similar to the following:  $ docker volume\n\nUsage:  docker volume [OPTIONS] [COMMAND]\n\nManage Docker volumes\n\nCommands:\n  create                   Create a volume\n  inspect                  Return low-level information on a volume\n  ls                       List volumes\n  rm                       Remove a volume", 
            "title": "Volume Management"
        }, 
        {
            "location": "/user-guide/schedulers/#list-volumes", 
            "text": "The list command reviews a list of available volumes that have been discovered\nvia Docker Volume Plug-in endpoints such as REX-Ray. Each volume name is\nexpected to be unique. Thus volume names must also be unique across all\nendpoints, and in turn, across all storage platforms exposed by REX-Ray.  With the exception of the  local  driver, the list of returned volumes is\ngenerated by the backend storage platform to which the configured driver\ncommunicates:  $ docker volume ls\nDRIVER              VOLUME NAME\nlocal               local1\nscaleio             Volume-001\nvirtualbox          vbox1", 
            "title": "List Volumes"
        }, 
        {
            "location": "/user-guide/schedulers/#inspect-volume", 
            "text": "The inspect command can be used to retrieve details about a volume related to\nboth Docker and the underlying storage platform. The fields listed under Status  are all generated by REX-Ray, including  Size in GB ,  Volume Type ,\nand  Availability Zone .  The  Scope  parameter ensures that when the specified volume driver is\ninspected by multiple Docker hosts, the volumes tagged as  global  are all\ninterpreted as the same volume. This reduces unnecessary round-trips in\nsituations where an application such as Docker Swarm is connected to hosts\nconfigured with REX-Ray.  $ docker volume inspect vbox1\n[\n    {\n         Name :  vbox1 ,\n         Driver :  virtualbox ,\n         Mountpoint :  ,\n         Status : {\n             availabilityZone :  ,\n             fields : null,\n             iops : 0,\n             name :  vbox1 ,\n             server :  virtualbox ,\n             service :  virtualbox ,\n             size : 8,\n             type :  \n        },\n         Labels : {},\n         Scope :  global \n    }\n]", 
            "title": "Inspect Volume"
        }, 
        {
            "location": "/user-guide/schedulers/#create-volume", 
            "text": "Docker's  volume create  command enables the creation of new volumes on the\nunderlying storage platform. Newly created volumes are available immediately\nto be attached and mounted. The  volume create  command also supports the CLI\nflag  -o|--opt  in order to support providing custom data to the volume creation\nworkflow:  $ docker volume create --driver=virtualbox --name=vbox2 --opt=size=2\nvbox2  Additional, valid options for the  -o|--opt  parameter include:     option  description      size  Size in GB    IOPS  IOPS    volumeType  Type of Volume or Storage Pool    volumeName  Create from an existing volume name    volumeID  Create from an existing volume ID    snapshotName  Create from an existing snapshot name    snapshotID  Create from an existing snapshot ID", 
            "title": "Create Volume"
        }, 
        {
            "location": "/user-guide/schedulers/#remove-volume", 
            "text": "A volume may be removed once it is no longer in use by a container, running or\notherwise. The process of removing a container actually causes the volume to\nbe removed if that is the last container to leverage said volume:  $ docker volume rm vbox2", 
            "title": "Remove Volume"
        }, 
        {
            "location": "/user-guide/schedulers/#containers-with-volumes", 
            "text": "Please review the  Applications  section for information on\nconfiguring popular applications with persistent storage via Docker and REX-Ray.", 
            "title": "Containers with Volumes"
        }, 
        {
            "location": "/user-guide/schedulers/#mesos", 
            "text": "In Mesos the frameworks are responsible for receiving requests from\nconsumers and then proceeding to schedule and manage tasks. While some\nframeworks, like Marathon, are open to run any workload for sustained periods\nof time, others are use case specific, such as Cassandra. Frameworks may\nalso receive requests from other platforms in addition to schedulers instead of\nconsumers such as Cloud Foundry, Kubernetes, and Swarm.  Once a resource offer is accepted from Mesos, tasks are launched to support the\nassociated workloads. These tasks are eventually distributed to Mesos agents in\norder to spin up containers.  REX-Ray enables on-demand storage allocation for agents receiving tasks via\ntwo deployment configurations:    Docker Containerizer with Marathon    Mesos Containerizer with Marathon", 
            "title": "Mesos"
        }, 
        {
            "location": "/user-guide/schedulers/#docker-containerizer-with-marathon", 
            "text": "When the framework leverages the Docker containerizer, Docker and REX-Ray\nshould both already be configured and working. The following example shows\nhow to use Marathon in order to bring an application online with external\nvolumes:  {\n     id :  nginx ,\n     container : {\n         docker : {\n             image :  million12/nginx ,\n             network :  BRIDGE ,\n             portMappings : [{\n                 containerPort : 80,\n                 hostPort : 0,\n                 protocol :  tcp \n            }],\n             parameters : [{\n                 key :  volume-driver ,\n                 value :  rexray \n            }, {\n                 key :  volume ,\n                 value :  nginx-data:/data/www \n            }]\n        }\n    },\n     cpus : 0.2,\n     mem : 32.0,\n     instances : 1\n}", 
            "title": "Docker Containerizer with Marathon"
        }, 
        {
            "location": "/user-guide/schedulers/#mesos-containerizer-with-marathon", 
            "text": "Mesos 0.23+ includes modules that enable extensibility for different\nportions the architecture. The  dvdcli  and mesos-module-dvdi  projects are\nrequired for to enable external volume support with the native containerizer.  The next example is similar to the one above, except in this instance the\nnative containerizer is preferred and volume requests are handled by the env  section.  {\n   id :  hello-play ,\n   cmd :  while [ true ] ; do touch /var/lib/rexray/volumes/test12345/hello ; sleep 5 ; done ,\n   mem : 32,\n   cpus : 0.1,\n   instances : 1,\n   env : {\n     DVDI_VOLUME_NAME :  test12345 ,\n     DVDI_VOLUME_DRIVER :  rexray ,\n     DVDI_VOLUME_OPTS :  size=5,iops=150,volumetype=io1,newfstype=xfs,overwritefs=true \n  }\n}  This example also illustrates several important settings for the native method.\nWhile the VirtualBox driver is being used, any validated storage platform\nshould work. Additionally, there are two options recommended for this type of\nconfiguration:     Property  Recommendation      libstorage.integration.volume.operations.mount.preempt  Setting this flag to true ensures any host can preempt control of a volume from other hosts    libstorage.integration.volume.operations.unmount.ignoreUsedCount  Enabling this flag declares that  mesos-module-dvdi  is the authoritative source for deciding when to unmount volumes     Please refer to the libStorage documentation for more information on Volume Configuration \noptions.   note  The  libstorage.integration.volume.operations.remove.disable  property can\nprevent the scheduler from removing volumes. Setting this flag to  true  is\nrecommended when using Mesos with Docker 1.9.1 or earlier.   libstorage:\n  service: virtualbox\n  integration:\n    volume:\n      operations:\n        mount:\n          preempt: true\n        unmount:\n          ignoreusedcount: true\n        remove:\n          disable: true\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes", 
            "title": "Mesos Containerizer with Marathon"
        }, 
        {
            "location": "/user-guide/applications/", 
            "text": "Applications\n\n\nPersistence for applications in containers.\n\n\n\n\nGetting Started\n\n\nThis tutorial will serve as a generic guide for taking Docker images found on\n\nDocker Hub\n and utilizing persistent external storage\nvia REX-Ray. This should provide guidance for certain applications, but also\ngenerically so you can add persistence properly to other applications.\n\n\nInstructions\n\n\nThe following are a set of instructions for investigating an existing container\nimage to determine how to properly apply persistence.\n\n\nThe first step is to determine which application you are looking to deploy, then\nproceed to its \nDocker Hub\n page. In this example, we\nwill be using \nPostgreSQL on Docker Hub\n.\n\n\nMost application vendors will post their Dockerfile on the main page for that\ngiven image. Many of them will also make them available by version minimally via\na download or via Github. Continuing with our PostgreSQL example, we will use\nthe \nDockerfile for version 9.3\n\nsince it happens to be the default version provided with Ubuntu 14.04.\n\n\nProperly written Dockerfiles will include the proper information that separates\npersistent information from the container image and deployed container. This is\nvisible when the author of the \nDockerfile\n includes a \nVOLUME\n statement to\ndefine where stateful information should be held.\n\n\nOpen the \nDockerfile\n and do a search for \nVOLUME\n and take note of the\nvolumes that will be created for this image. Then we can use REX-Ray or\n\nDocker\n to create the external persistent volume for this image. In the\nPostgreSQL 9.3 example, there is a single volume in the Dockerfile:\n\n\nVOLUME /var/lib/postgresql/data\n\n\n\n\nThe single path or paths listed refer to the volumes that should be attached\nwhen running the container. Following this you can create a volume and attach\nit to a container with the \n-v\n flag.\n\n\n\n\nPostgreSQL\n  \n\n\n\n\n$ docker volume create --driver=rexray --name=postgresql --opt=size=\nsizeInGB\n\n$ docker run -d -e POSTGRES_PASSWORD=mysecretpassword --volume-driver=rexray \\\n    -v data:/var/lib/postgresql/data postgres\n\n\n\n\nPopular Applications\n\n\nExternal persistent storage can be applied to any number of applications\nincluding but not limited the following examples.\n\n\n\n\nCassandra\n\n\n\n\nPostgreSQL\n\n\n$ docker volume create --driver=rexray --name=postgresql --opt=size=\nsizeInGB\n\n$ docker run -d -e POSTGRES_PASSWORD=mysecretpassword --volume-driver=rexray \\\n    -v data:/var/lib/postgresql/data postgres\n\n\n\n\n\n\n\nMariaDB\n\n\n\n\n\n\nMongoDB\n\n\n$ docker volume create --driver=rexray --name=mongodb --opt=size=\nsizeInGB\n\n$ docker run -d --volume-driver=rexray -v mongodb:/data/db mongo\n\n\n\n\n\n\n\nMySQL\n\n\n\n\nRedis\n$ docker volume create --driver=rexray --name=redis --opt=size=\nsizeInGB\n\n$ docker run -d --volume-driver=rexray -v redis:/data redis", 
            "title": "Applications"
        }, 
        {
            "location": "/user-guide/applications/#applications", 
            "text": "Persistence for applications in containers.", 
            "title": "Applications"
        }, 
        {
            "location": "/user-guide/applications/#getting-started", 
            "text": "This tutorial will serve as a generic guide for taking Docker images found on Docker Hub  and utilizing persistent external storage\nvia REX-Ray. This should provide guidance for certain applications, but also\ngenerically so you can add persistence properly to other applications.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/user-guide/applications/#instructions", 
            "text": "The following are a set of instructions for investigating an existing container\nimage to determine how to properly apply persistence.  The first step is to determine which application you are looking to deploy, then\nproceed to its  Docker Hub  page. In this example, we\nwill be using  PostgreSQL on Docker Hub .  Most application vendors will post their Dockerfile on the main page for that\ngiven image. Many of them will also make them available by version minimally via\na download or via Github. Continuing with our PostgreSQL example, we will use\nthe  Dockerfile for version 9.3 \nsince it happens to be the default version provided with Ubuntu 14.04.  Properly written Dockerfiles will include the proper information that separates\npersistent information from the container image and deployed container. This is\nvisible when the author of the  Dockerfile  includes a  VOLUME  statement to\ndefine where stateful information should be held.  Open the  Dockerfile  and do a search for  VOLUME  and take note of the\nvolumes that will be created for this image. Then we can use REX-Ray or Docker  to create the external persistent volume for this image. In the\nPostgreSQL 9.3 example, there is a single volume in the Dockerfile:  VOLUME /var/lib/postgresql/data  The single path or paths listed refer to the volumes that should be attached\nwhen running the container. Following this you can create a volume and attach\nit to a container with the  -v  flag.   PostgreSQL      $ docker volume create --driver=rexray --name=postgresql --opt=size= sizeInGB \n$ docker run -d -e POSTGRES_PASSWORD=mysecretpassword --volume-driver=rexray \\\n    -v data:/var/lib/postgresql/data postgres", 
            "title": "Instructions"
        }, 
        {
            "location": "/user-guide/applications/#popular-applications", 
            "text": "External persistent storage can be applied to any number of applications\nincluding but not limited the following examples.   Cassandra   PostgreSQL  $ docker volume create --driver=rexray --name=postgresql --opt=size= sizeInGB \n$ docker run -d -e POSTGRES_PASSWORD=mysecretpassword --volume-driver=rexray \\\n    -v data:/var/lib/postgresql/data postgres    MariaDB    MongoDB  $ docker volume create --driver=rexray --name=mongodb --opt=size= sizeInGB \n$ docker run -d --volume-driver=rexray -v mongodb:/data/db mongo    MySQL   Redis $ docker volume create --driver=rexray --name=redis --opt=size= sizeInGB \n$ docker run -d --volume-driver=rexray -v redis:/data redis", 
            "title": "Popular Applications"
        }, 
        {
            "location": "/dev-guide/project-guidelines/", 
            "text": "Project Guidelines\n\n\nThese are important.\n\n\n\n\nPeople contributing code to this project must adhere to the following rules.\nThese standards are in place to keep code clean, consistent, and stable.\n\n\nDocumentation\n\n\nThere are two types of documentation: source and markdown.\n\n\nSource Code\n\n\nAll source code should be documented in accordance with the\n\nGo's documentation rules\n.\n\n\nMarkdown\n\n\nWhen creating or modifying the project's \nREADME.md\n file or any of the\ndocumentation in the \n.docs\n directory, please keep the following rules in\nmind:\n\n\n\n\nAll links to internal resources should be relative.\n\n\nAll links to markdown files should include the file extension.\n\n\n\n\nFor example, the below link points to the anchor \nbasic-configuration\n on the\n\nConfiguration\n page:\n\n\n\n\n/user-guide/config#basic-configuration\n\n\nHowever, when the above link is followed when viewing this page directly from\nthe Github repository instead of the generated site documentation, the link\nwill return a 404.\n\n\nWhile it's recommended that users view the generated site documentation instead\nof the source Markdown directly, we can still fix it so that the above link\nwill work regardless. To fix the link, simply make it relative and add the\nMarkdown file extension:\n\n\n\n\n../user-guide/config.md#basic-configuration\n\n\nNow the link will work regardless from where it's viewed.\n\n\nStyle \n Syntax\n\n\nAll source files should be processed by the following tools prior to being\ncommitted. Any errors or warnings produced by the tools should be corrected\nbefore the source is committed.\n\n\n\n\n\n\n\n\nTool\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngofmt\n\n\nA golang source formatting tool\n\n\n\n\n\n\ngolint\n\n\nA golang linter\n\n\n\n\n\n\ngovet\n\n\nA golang source optimization tool\n\n\n\n\n\n\ngocyclo\n\n\nA golang cyclomatic complexity detection tool. No function should have a score above 0.15\n\n\n\n\n\n\n\n\nIf \nAtom\n is your IDE of choice, install the\n\ngo-plus\n package, and it will execute all of\nthe tools above less gocyclo upon saving a file.\n\n\nIn lieu of using Atom as the IDE, the project's \nMakefile\n automatically\nexecutes the above tools as part of the build process and will fail the build\nif problems are discovered.\n\n\nAnother option is to use a client-side, pre-commit hook to ensure that the\nsources meet the required standards. For example, in the project's \n.git/hooks\n\ndirectory create a file called \npre-commit\n and mark it as executable. Then\npaste the following content inside the file:\n\n\n#!/bin/sh\nmake fmt 1\n /dev/null\n\n\n\n\nThe above script will execute prior to a Git commit operation, prior to even\nthe commit message dialog. The script will invoke the \nMakefile\n's \nfmt\n\ntarget, formatting the sources. If the command returns a non-zero exit code,\nthe commit operation will abort with the error.\n\n\nCode Coverage\n\n\nAll new work submitted to the project should have associated tests where\napplicable. If there is ever a question of whether or not a test is applicable\nthen the answer is likely yes.\n\n\nThis project uses\n\nCoveralls\n for code coverage, and\nall pull requests are processed just as a build from \nmaster\n. If a pull request\ndecreases the project's code coverage, the pull request will be declined until\nsuch time that testing is added or enhanced to compensate.\n\n\nIt's also possible to test the project locally while outputting the code\ncoverage. On the command line, from the project's root directory, execute the\nfollowing:\n\n\n$ .build/test.sh\nok      github.com/emccode/rexray/rexray/cli    0.039s  coverage: 33.6% of statements\nok      github.com/emccode/rexray/test  0.080s  coverage: 94.0% of statements in github.com/emccode/rexray, github.com/emccode/rexray/core\n...\nok      github.com/emccode/rexray/util  0.024s  coverage: 100.0% of statements\n[0]akutz@pax:rexray$\n\n\n\n\nThe file \ntest.sh\n in the \n.build\n directory is the same script executed during\nthe project's \nautomated build system\n. The only\ndifference is when executed locally the results are not submitted to Coveralls.\nStill, using the \ntest.sh\n file one can easily determine if a package's coverage\nhas decreased and if additional testing is necessary.\n\n\nCommit Messages\n\n\nCommit messages should follow the guide \n5 Useful Tips For a Better Commit\nMessage\n.\nThe two primary rules to which to adhere are:\n\n\n\n\n\n\nCommit message subjects should not exceed 50 characters in total and\n     should be followed by a blank line.\n\n\n\n\n\n\nThe commit message's body should not have a width that exceeds 72\n     characters.\n\n\n\n\n\n\nFor example, the following commit has a very useful message that is succinct\nwithout losing utility.\n\n\ncommit e80c696939a03f26cd180934ba642a729b0d2941\nAuthor: akutz \nsakutz@gmail.com\n\nDate:   Tue Oct 20 23:47:36 2015 -0500\n\n    Added --format,-f option for CLI\n\n    This patch adds the flag '--format' or '-f' for the\n    following CLI commands:\n\n        * adapter instances\n        * device [get]\n        * snapshot [get]\n        * snapshot copy\n        * snapshot create\n        * volume [get]\n        * volume attach\n        * volume create\n        * volume map\n        * volume mount\n        * volume path\n\n    The user can specify either '--format=yml|yaml|json' or\n    '-f yml|yaml|json' in order to influence how the resulting,\n    structured data is marshaled prior to being emitted to the console.\n\n\n\n\nPlease note that the output above is the full output for viewing a commit.\nHowever, because the above message adheres to the commit message rules, it's\nquite easy to show just the commit's subject:\n\n\n$ git show e80c696939a03f26cd180934ba642a729b0d2941 --format=\n%s\n -s\nAdded --format,-f option for CLI\n\n\n\n\nIt's also equally simple to print the commit's subject and body together:\n\n\n$ git show e80c696939a03f26cd180934ba642a729b0d2941 --format=\n%s%n%n%b\n -s\nAdded --format,-f option for CLI\n\nThis patch adds the flag '--format' or '-f' for the\nfollowing CLI commands:\n\n    * adapter instances\n    * device [get]\n    * snapshot [get]\n    * snapshot copy\n    * snapshot create\n    * volume [get]\n    * volume attach\n    * volume create\n    * volume map\n    * volume mount\n    * volume path\n\nThe user can specify either '--format=yml|yaml|json' or\n'-f yml|yaml|json' in order to influence how the resulting,\nstructured data is marshaled prior to being emitted to the console.\n\n\n\n\nSubmitting Changes\n\n\nAll developers are required to follow the\n\nGitHub Flow model\n when\nproposing new features or even submitting fixes.\n\n\nPlease note that although not explicitly stated in the referenced GitHub Flow\nmodel, all work should occur on a \nfork\n of this project, not from within a\nbranch of this project itself.\n\n\nPull requests submitted to this project should adhere to the following\nguidelines:\n\n\n\n\n\n\nBranches should be rebased off of the upstream master prior to being\n    opened as pull requests and again prior to merge. This is to ensure that\n    the build system accounts for any changes that may only be detected during\n    the build and test phase.\n\n\n\n\n\n\nUnless granted an exception a pull request should contain only a single\n    commit. This is because features and patches should be atomic -- wholly\n    shippable items that are either included in a release, or not. Please\n    squash commits on a branch before opening a pull request. It is not a\n    deal-breaker otherwise, but please be prepared to add a comment or\n    explanation as to why you feel multiple commits are required.", 
            "title": "Project Guidelines"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#project-guidelines", 
            "text": "These are important.   People contributing code to this project must adhere to the following rules.\nThese standards are in place to keep code clean, consistent, and stable.", 
            "title": "Project Guidelines"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#documentation", 
            "text": "There are two types of documentation: source and markdown.", 
            "title": "Documentation"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#source-code", 
            "text": "All source code should be documented in accordance with the Go's documentation rules .", 
            "title": "Source Code"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#markdown", 
            "text": "When creating or modifying the project's  README.md  file or any of the\ndocumentation in the  .docs  directory, please keep the following rules in\nmind:   All links to internal resources should be relative.  All links to markdown files should include the file extension.   For example, the below link points to the anchor  basic-configuration  on the Configuration  page:   /user-guide/config#basic-configuration  However, when the above link is followed when viewing this page directly from\nthe Github repository instead of the generated site documentation, the link\nwill return a 404.  While it's recommended that users view the generated site documentation instead\nof the source Markdown directly, we can still fix it so that the above link\nwill work regardless. To fix the link, simply make it relative and add the\nMarkdown file extension:   ../user-guide/config.md#basic-configuration  Now the link will work regardless from where it's viewed.", 
            "title": "Markdown"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#style-syntax", 
            "text": "All source files should be processed by the following tools prior to being\ncommitted. Any errors or warnings produced by the tools should be corrected\nbefore the source is committed.     Tool  Description      gofmt  A golang source formatting tool    golint  A golang linter    govet  A golang source optimization tool    gocyclo  A golang cyclomatic complexity detection tool. No function should have a score above 0.15     If  Atom  is your IDE of choice, install the go-plus  package, and it will execute all of\nthe tools above less gocyclo upon saving a file.  In lieu of using Atom as the IDE, the project's  Makefile  automatically\nexecutes the above tools as part of the build process and will fail the build\nif problems are discovered.  Another option is to use a client-side, pre-commit hook to ensure that the\nsources meet the required standards. For example, in the project's  .git/hooks \ndirectory create a file called  pre-commit  and mark it as executable. Then\npaste the following content inside the file:  #!/bin/sh\nmake fmt 1  /dev/null  The above script will execute prior to a Git commit operation, prior to even\nthe commit message dialog. The script will invoke the  Makefile 's  fmt \ntarget, formatting the sources. If the command returns a non-zero exit code,\nthe commit operation will abort with the error.", 
            "title": "Style &amp; Syntax"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#code-coverage", 
            "text": "All new work submitted to the project should have associated tests where\napplicable. If there is ever a question of whether or not a test is applicable\nthen the answer is likely yes.  This project uses Coveralls  for code coverage, and\nall pull requests are processed just as a build from  master . If a pull request\ndecreases the project's code coverage, the pull request will be declined until\nsuch time that testing is added or enhanced to compensate.  It's also possible to test the project locally while outputting the code\ncoverage. On the command line, from the project's root directory, execute the\nfollowing:  $ .build/test.sh\nok      github.com/emccode/rexray/rexray/cli    0.039s  coverage: 33.6% of statements\nok      github.com/emccode/rexray/test  0.080s  coverage: 94.0% of statements in github.com/emccode/rexray, github.com/emccode/rexray/core\n...\nok      github.com/emccode/rexray/util  0.024s  coverage: 100.0% of statements\n[0]akutz@pax:rexray$  The file  test.sh  in the  .build  directory is the same script executed during\nthe project's  automated build system . The only\ndifference is when executed locally the results are not submitted to Coveralls.\nStill, using the  test.sh  file one can easily determine if a package's coverage\nhas decreased and if additional testing is necessary.", 
            "title": "Code Coverage"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#commit-messages", 
            "text": "Commit messages should follow the guide  5 Useful Tips For a Better Commit\nMessage .\nThe two primary rules to which to adhere are:    Commit message subjects should not exceed 50 characters in total and\n     should be followed by a blank line.    The commit message's body should not have a width that exceeds 72\n     characters.    For example, the following commit has a very useful message that is succinct\nwithout losing utility.  commit e80c696939a03f26cd180934ba642a729b0d2941\nAuthor: akutz  sakutz@gmail.com \nDate:   Tue Oct 20 23:47:36 2015 -0500\n\n    Added --format,-f option for CLI\n\n    This patch adds the flag '--format' or '-f' for the\n    following CLI commands:\n\n        * adapter instances\n        * device [get]\n        * snapshot [get]\n        * snapshot copy\n        * snapshot create\n        * volume [get]\n        * volume attach\n        * volume create\n        * volume map\n        * volume mount\n        * volume path\n\n    The user can specify either '--format=yml|yaml|json' or\n    '-f yml|yaml|json' in order to influence how the resulting,\n    structured data is marshaled prior to being emitted to the console.  Please note that the output above is the full output for viewing a commit.\nHowever, because the above message adheres to the commit message rules, it's\nquite easy to show just the commit's subject:  $ git show e80c696939a03f26cd180934ba642a729b0d2941 --format= %s  -s\nAdded --format,-f option for CLI  It's also equally simple to print the commit's subject and body together:  $ git show e80c696939a03f26cd180934ba642a729b0d2941 --format= %s%n%n%b  -s\nAdded --format,-f option for CLI\n\nThis patch adds the flag '--format' or '-f' for the\nfollowing CLI commands:\n\n    * adapter instances\n    * device [get]\n    * snapshot [get]\n    * snapshot copy\n    * snapshot create\n    * volume [get]\n    * volume attach\n    * volume create\n    * volume map\n    * volume mount\n    * volume path\n\nThe user can specify either '--format=yml|yaml|json' or\n'-f yml|yaml|json' in order to influence how the resulting,\nstructured data is marshaled prior to being emitted to the console.", 
            "title": "Commit Messages"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#submitting-changes", 
            "text": "All developers are required to follow the GitHub Flow model  when\nproposing new features or even submitting fixes.  Please note that although not explicitly stated in the referenced GitHub Flow\nmodel, all work should occur on a  fork  of this project, not from within a\nbranch of this project itself.  Pull requests submitted to this project should adhere to the following\nguidelines:    Branches should be rebased off of the upstream master prior to being\n    opened as pull requests and again prior to merge. This is to ensure that\n    the build system accounts for any changes that may only be detected during\n    the build and test phase.    Unless granted an exception a pull request should contain only a single\n    commit. This is because features and patches should be atomic -- wholly\n    shippable items that are either included in a release, or not. Please\n    squash commits on a branch before opening a pull request. It is not a\n    deal-breaker otherwise, but please be prepared to add a comment or\n    explanation as to why you feel multiple commits are required.", 
            "title": "Submitting Changes"
        }, 
        {
            "location": "/dev-guide/build-reference/", 
            "text": "Build Reference\n\n\nHow to build REX-Ray\n\n\n\n\nBuild Requirements\n\n\nThis project has very few build requirements, but there are still one or two\nitems of which to be aware. Also, please note that this are the requirements to\n\nbuild\n REX-Ray, not run it.\n\n\n\n\n\n\n\n\nRequirement\n\n\nVersion\n\n\n\n\n\n\n\n\n\n\nOperating System\n\n\nLinux, OS X\n\n\n\n\n\n\nGo\n\n\n=1.6\n\n\n\n\n\n\nGNU Make\n\n\n=3.80\n\n\n\n\n\n\n\n\nOS X ships with a very old version of GNU Make, and a package manager like\n\nHomebrew\n can be used to install the required version.\n\n\nCross-Compilation\n\n\nThis project's \nMakefile\n\nis configured by default to build for a Linux x86_64 target, but\ncross-compilation \nis\n supported. Therefore the build environment should be\nconfigured to support cross-compilation. Please take a minute to read\nthis \nblog post\n\nregarding cross-compilation with Go \n=1.5.\n\n\nBuild Binary\n\n\nBuilding from source is pretty simple as all steps, including fetching\ndependencies (as well as fetching the tool that fetches dependencies), are\nconfigured as part of the included \nMakefile\n.\n\n\nBuild Dependencies\n\n\nThe \nmake deps\n command should be executed prior to any other targets in order\nto ensure the necessary dependencies are available.\n\n\n$ make deps\ntarget: deps\n  ...installing glide...SUCCESS!\n  ...glide up...[WARN] Only resolving dependencies for the current OS/Arch\n[INFO] Downloading dependencies. Please wait...\n[INFO] Fetching updates for github.com/Sirupsen/logrus.\n[INFO] Fetching updates for golang.org/x/net.\n[INFO] Fetching updates for github.com/spf13/cobra.\n[INFO] Fetching updates for github.com/akutz/gofig.\n[INFO] Fetching updates for github.com/emccode/libstorage.\n[INFO] Fetching updates for gopkg.in/yaml.v1.\n[INFO] Fetching updates for gopkg.in/yaml.v2.\n[INFO] Fetching updates for google.golang.org/api.\n[INFO] Fetching updates for github.com/go-yaml/yaml.\n[INFO] Fetching updates for github.com/akutz/gotil.\n[INFO] Fetching updates for github.com/spf13/pflag.\n[INFO] Fetching updates for github.com/akutz/golf.\n[INFO] Fetching updates for github.com/akutz/goof.\n[INFO] Setting version for github.com/akutz/golf to v0.1.1.\n[INFO] Setting version for gopkg.in/yaml.v1 to b4a9f8c4b84c6c4256d669c649837f1441e4b050.\n[INFO] Setting version for github.com/go-yaml/yaml to b4a9f8c4b84c6c4256d669c649837f1441e4b050.\n[INFO] Setting version for github.com/spf13/cobra to 363816bb13ce1710460c2345017fd35593cbf5ed.\n[INFO] Setting version for google.golang.org/api to fd081149e482b10c55262756934088ffe3197ea3.\n[INFO] Setting version for github.com/akutz/goof to v0.1.0.\n[INFO] Setting version for github.com/akutz/gofig to v0.1.4.\n[INFO] Setting version for github.com/spf13/pflag to b084184666e02084b8ccb9b704bf0d79c466eb1d.\n[INFO] Setting version for github.com/Sirupsen/logrus to feature/logrus-aware-types.\n[INFO] Setting version for gopkg.in/yaml.v2 to b4a9f8c4b84c6c4256d669c649837f1441e4b050.\n[INFO] Setting version for github.com/akutz/gotil to v0.1.0.\n[INFO] Setting version for github.com/emccode/libstorage to v0.1.3.\n[INFO] Resolving imports\n[INFO] Fetching github.com/spf13/viper into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Setting version for github.com/spf13/viper to support/rexray.\n[INFO] Fetching github.com/kardianos/osext into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Setting version for github.com/kardianos/osext to master.\n[INFO] Fetching github.com/gorilla/context into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/gorilla/mux into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/gorilla/handlers into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/cpuguy83/go-md2man/md2man into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/inconshreveable/mousetrap into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/BurntSushi/toml into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/kr/pretty into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/magiconair/properties into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/mitchellh/mapstructure into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/spf13/cast into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/spf13/jwalterweatherman into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching gopkg.in/fsnotify.v1 into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/emccode/goisilon into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Setting version for github.com/emccode/goisilon to f9b53f0aaadb12a26b134830142fc537f492cb13.\n[INFO] Fetching github.com/emccode/goscaleio into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Setting version for github.com/emccode/goscaleio to 53ea76f52205380ab52b9c1f4ad89321c286bb95.\n[INFO] Fetching github.com/appropriate/go-virtualboxclient/vboxwebsrv into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Setting version for github.com/appropriate/go-virtualboxclient to e0978ab2ed407095400a69d5933958dd260058cd.\n[INFO] Fetching github.com/russross/blackfriday into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/kr/text into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching golang.org/x/sys/unix into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/cesanta/ucl into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/cesanta/validate-json/schema into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/shurcooL/sanitized_anchor_name into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/asaskevich/govalidator into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/jteeuwen/go-bindata into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Setting version for github.com/jteeuwen/go-bindata to feature/md5checksum.\n[INFO] Downloading dependencies. Please wait...\n[INFO] Setting references for remaining imports\n[INFO] Versions did not change. Skipping glide.lock update.\n[INFO] Project relies on 38 dependencies.\nSUCCESS!\n  ...go get...SUCCESS!\ncd vendor/github.com/emccode/libstorage \n make api/api_generated.go \n cd -\necho generating api/api_generated.go\ngenerating api/api_generated.go\n/Users/akutz/Projects/go/src/github.com/emccode/rexray\ncd vendor/github.com/emccode/libstorage \n make api/server/executors/executors_generated.go \n cd -\nGOOS=darwin GOARCH=amd64 go install ./api/types\nGOOS=darwin GOARCH=amd64 go install ./api/context\nGOOS=darwin GOARCH=amd64 go install ./api/utils\nGOOS=darwin GOARCH=amd64 go install ./api/registry\nGOOS=darwin GOARCH=amd64 go install ./api/utils/config\nGOOS=darwin GOARCH=amd64 go install ./imports/config\nGOOS=darwin GOARCH=amd64 go install ./drivers/storage/isilon\nGOOS=darwin GOARCH=amd64 go install ./drivers/storage/isilon/executor\nGOOS=darwin GOARCH=amd64 go install ./drivers/storage/scaleio\nGOOS=darwin GOARCH=amd64 go install ./drivers/storage/scaleio/executor\nGOOS=darwin GOARCH=amd64 go install ./drivers/storage/vbox\nGOOS=darwin GOARCH=amd64 go install ./drivers/storage/vbox/executor\nGOOS=darwin GOARCH=amd64 go install ./drivers/storage/vfs\nGOOS=darwin GOARCH=amd64 go install ./drivers/storage/vfs/executor\nGOOS=darwin GOARCH=amd64 go install ./imports/executors\nGOOS=darwin GOARCH=amd64 go install ./cli/lsx\nGOOS=darwin GOARCH=amd64 go install ./cli/lsx/lsx-darwin\n/Users/akutz/Projects/go/bin/go-bindata -md5checksum -pkg executors -prefix api/server/executors/bin -o api/server/executors/executors_generated.go api/server/executors/bin/...\n/Users/akutz/Projects/go/src/github.com/emccode/rexray\n\n\n\n\nBasic Build\n\n\nThe most basic build can be achieved by simply typing \nmake\n from the project's\nroot directory. For what it's worth, executing \nmake\n sans arguments is the\nsame as executing \nmake install\n for this project's \nMakefile\n.\n\n\n$ make\nSemVer: 0.4.0-rc4+10+dirty\nRpmVer: 0.4.0+rc4+10+dirty\nBranch: release/0.4.0-rc4\nCommit: d2f0283c7fed29ad0a142a6a51624828893f5db5\nFormed: Wed, 15 Jun 2016 16:53:44 CDT\n\ntarget: fmt\n  ...formatting rexray...SUCCESS!\ntarget: install\n  ...installing rexray Darwin-x86_64...SUCCESS!\n\nThe REX-Ray binary is 40MB and located at:\n\n  /Users/akutz/Projects/go/bin/rexray\n\n\n\n\nBinary Size\n\n\nPlease note that the large size of the binary is due to the Isilon\nstorage adapter's dependency on the\n\nVMware VMOMI library for Go\n. The types\ndefinition source in that package compiles to a 45MB, uncompressed archive.\nEfforts are being made to figure out an alternative to this dependency in order\nto reduce the binary size, even if it means creating VMware SOAP messages from\nscratch.\n\n\nBuild All\n\n\nIn order to build all versions of the binary type the following:\n\n\n$ make build-all\nSemVer: 0.4.0-rc4+10\nRpmVer: 0.4.0+rc4+10\nBranch: release/0.4.0-rc4\nCommit: d2f0283c7fed29ad0a142a6a51624828893f5db5\nFormed: Wed, 15 Jun 2016 16:42:33 CDT\n\ntarget: fmt\n  ...formatting rexray...SUCCESS!\ntarget: build\n  ...building rexray Linux-x86_64...SUCCESS!\n\nThe REX-Ray binary is 40MB and located at:\n\n  .build/bin/Linux-x86_64/rexray\n\n\n\n\nBuild Packages\n\n\nThe \nMakefile\n also includes targets that assist in the creation of\ndistributable packages using the produced artifact.\n\n\nBuild Tarballs\n\n\nThe \nMakefile\n's \nbuild\n and \nbuild-all\n targets will not only build the binary\nin place, but it will also compress the binary as a tarball so it's ready for\ndeployment. For example, after the \nmake build-all\n above, this is the contents\nof the directory \n.build/deploy\n:\n\n\n$ ls .build/deploy\nlatest/\nLinux-x86_64/\n\n\n\n\nLooking inside the directory \n.build/deploy/Linux-x86_64\n reveals:\n\n\n$ ls .build/deploy/Linux-x86_64/\nrexray-Linux-x86_64-0.4.0-rc4+10.tar.gz\n\n\n\n\nBuild RPMs\n\n\nThe \nMakefile\n's \nrpm-all\n target will package the binary as\narchitecture-specific RPMs when executed on a system that supports the RPM\ndevelopment environment:\n\n\n$ make rpm-all\ntarget: rpm\n  ...building rpm x86_64...SUCCESS!\n\nThe REX-Ray RPM is 11MB and located at:\n\n  .build/deploy/Linux-x86_64/rexray-0.4.0+rc4+10-1.x86_64.rpm\n\n\n\n\nBuild DEBs\n\n\nThe \nMakefile\n's \ndeb-all\n target will package the binary as\narchitecture-specific DEBs when executed on a system that supports the Alien\nRPM-to-DEB conversion tools:\n\n\n$ make deb-all\ntarget: deb\n  ...building deb x86_64...SUCCESS!\n\nThe REX-Ray DEB is 9MB and located at:\n\n  .build/deploy/Linux-x86_64/rexray_0.4.0+rc4+10-1_amd64.deb\n\n\n\n\nVersion File\n\n\nThere is a file at the root of the project named \nVERSION\n. The file contains\na single line with the \ntarget\n version of the project in the file. The version\nfollows the format:\n\n\n(?\nmajor\n\\d+)\\.(?\nminor\n\\d+)\\.(?\npatch\n\\d+)(-rc\\d+)?\n\n\nFor example, during active development of version \n0.4.0\n the file would\ncontain the version \n0.4.0\n. When it's time to create \n0.4.0\n's first\nrelease candidate the version in the file will be changed to \n0.4.0-rc1\n. And\nwhen it's time to release \n0.4.0\n the version is changed back to \n0.4.0\n.\n\n\nPlease note that we've discussed making the actively developed version the\ntargeted version with a \n-dev\n suffix, but trying this resulted in confusion\nfor the RPM and DEB package managers when using \nunstable\n releases.\n\n\nSo what's the point of the file if it's basically duplicating the utility of a\ntag? Well, the \nVERSION\n file in fact has two purposes:\n\n\n\n\n\n\nFirst and foremost updating the \nVERSION\n file with the same value as that\n     of the tag used to create a release provides a single, contextual reason to\n     push a commit and tag. Otherwise some random commit off of \nmaster\n would\n     be tagged as a release candidate or release. Always using the commit that\n     is related to updating the \nVERSION\n file is much cleaner.\n\n\n\n\n\n\nThe contents of the \nVERSION\n file are also used during the build process\n     as a means of overriding the output of a \ngit describe\n. This enables the\n     semantic version injected into the produced binary to be created using\n     the \ntargeted\n version of the next release and not just the value of the\n     last, tagged commit.", 
            "title": "Build Reference"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-reference", 
            "text": "How to build REX-Ray", 
            "title": "Build Reference"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-requirements", 
            "text": "This project has very few build requirements, but there are still one or two\nitems of which to be aware. Also, please note that this are the requirements to build  REX-Ray, not run it.     Requirement  Version      Operating System  Linux, OS X    Go  =1.6    GNU Make  =3.80     OS X ships with a very old version of GNU Make, and a package manager like Homebrew  can be used to install the required version.", 
            "title": "Build Requirements"
        }, 
        {
            "location": "/dev-guide/build-reference/#cross-compilation", 
            "text": "This project's  Makefile \nis configured by default to build for a Linux x86_64 target, but\ncross-compilation  is  supported. Therefore the build environment should be\nconfigured to support cross-compilation. Please take a minute to read\nthis  blog post \nregarding cross-compilation with Go  =1.5.", 
            "title": "Cross-Compilation"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-binary", 
            "text": "Building from source is pretty simple as all steps, including fetching\ndependencies (as well as fetching the tool that fetches dependencies), are\nconfigured as part of the included  Makefile .", 
            "title": "Build Binary"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-dependencies", 
            "text": "The  make deps  command should be executed prior to any other targets in order\nto ensure the necessary dependencies are available.  $ make deps\ntarget: deps\n  ...installing glide...SUCCESS!\n  ...glide up...[WARN] Only resolving dependencies for the current OS/Arch\n[INFO] Downloading dependencies. Please wait...\n[INFO] Fetching updates for github.com/Sirupsen/logrus.\n[INFO] Fetching updates for golang.org/x/net.\n[INFO] Fetching updates for github.com/spf13/cobra.\n[INFO] Fetching updates for github.com/akutz/gofig.\n[INFO] Fetching updates for github.com/emccode/libstorage.\n[INFO] Fetching updates for gopkg.in/yaml.v1.\n[INFO] Fetching updates for gopkg.in/yaml.v2.\n[INFO] Fetching updates for google.golang.org/api.\n[INFO] Fetching updates for github.com/go-yaml/yaml.\n[INFO] Fetching updates for github.com/akutz/gotil.\n[INFO] Fetching updates for github.com/spf13/pflag.\n[INFO] Fetching updates for github.com/akutz/golf.\n[INFO] Fetching updates for github.com/akutz/goof.\n[INFO] Setting version for github.com/akutz/golf to v0.1.1.\n[INFO] Setting version for gopkg.in/yaml.v1 to b4a9f8c4b84c6c4256d669c649837f1441e4b050.\n[INFO] Setting version for github.com/go-yaml/yaml to b4a9f8c4b84c6c4256d669c649837f1441e4b050.\n[INFO] Setting version for github.com/spf13/cobra to 363816bb13ce1710460c2345017fd35593cbf5ed.\n[INFO] Setting version for google.golang.org/api to fd081149e482b10c55262756934088ffe3197ea3.\n[INFO] Setting version for github.com/akutz/goof to v0.1.0.\n[INFO] Setting version for github.com/akutz/gofig to v0.1.4.\n[INFO] Setting version for github.com/spf13/pflag to b084184666e02084b8ccb9b704bf0d79c466eb1d.\n[INFO] Setting version for github.com/Sirupsen/logrus to feature/logrus-aware-types.\n[INFO] Setting version for gopkg.in/yaml.v2 to b4a9f8c4b84c6c4256d669c649837f1441e4b050.\n[INFO] Setting version for github.com/akutz/gotil to v0.1.0.\n[INFO] Setting version for github.com/emccode/libstorage to v0.1.3.\n[INFO] Resolving imports\n[INFO] Fetching github.com/spf13/viper into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Setting version for github.com/spf13/viper to support/rexray.\n[INFO] Fetching github.com/kardianos/osext into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Setting version for github.com/kardianos/osext to master.\n[INFO] Fetching github.com/gorilla/context into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/gorilla/mux into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/gorilla/handlers into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/cpuguy83/go-md2man/md2man into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/inconshreveable/mousetrap into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/BurntSushi/toml into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/kr/pretty into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/magiconair/properties into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/mitchellh/mapstructure into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/spf13/cast into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/spf13/jwalterweatherman into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching gopkg.in/fsnotify.v1 into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/emccode/goisilon into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Setting version for github.com/emccode/goisilon to f9b53f0aaadb12a26b134830142fc537f492cb13.\n[INFO] Fetching github.com/emccode/goscaleio into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Setting version for github.com/emccode/goscaleio to 53ea76f52205380ab52b9c1f4ad89321c286bb95.\n[INFO] Fetching github.com/appropriate/go-virtualboxclient/vboxwebsrv into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Setting version for github.com/appropriate/go-virtualboxclient to e0978ab2ed407095400a69d5933958dd260058cd.\n[INFO] Fetching github.com/russross/blackfriday into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/kr/text into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching golang.org/x/sys/unix into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/cesanta/ucl into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/cesanta/validate-json/schema into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/shurcooL/sanitized_anchor_name into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/asaskevich/govalidator into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Fetching github.com/jteeuwen/go-bindata into /Users/akutz/Projects/go/src/github.com/emccode/rexray/vendor\n[INFO] Setting version for github.com/jteeuwen/go-bindata to feature/md5checksum.\n[INFO] Downloading dependencies. Please wait...\n[INFO] Setting references for remaining imports\n[INFO] Versions did not change. Skipping glide.lock update.\n[INFO] Project relies on 38 dependencies.\nSUCCESS!\n  ...go get...SUCCESS!\ncd vendor/github.com/emccode/libstorage   make api/api_generated.go   cd -\necho generating api/api_generated.go\ngenerating api/api_generated.go\n/Users/akutz/Projects/go/src/github.com/emccode/rexray\ncd vendor/github.com/emccode/libstorage   make api/server/executors/executors_generated.go   cd -\nGOOS=darwin GOARCH=amd64 go install ./api/types\nGOOS=darwin GOARCH=amd64 go install ./api/context\nGOOS=darwin GOARCH=amd64 go install ./api/utils\nGOOS=darwin GOARCH=amd64 go install ./api/registry\nGOOS=darwin GOARCH=amd64 go install ./api/utils/config\nGOOS=darwin GOARCH=amd64 go install ./imports/config\nGOOS=darwin GOARCH=amd64 go install ./drivers/storage/isilon\nGOOS=darwin GOARCH=amd64 go install ./drivers/storage/isilon/executor\nGOOS=darwin GOARCH=amd64 go install ./drivers/storage/scaleio\nGOOS=darwin GOARCH=amd64 go install ./drivers/storage/scaleio/executor\nGOOS=darwin GOARCH=amd64 go install ./drivers/storage/vbox\nGOOS=darwin GOARCH=amd64 go install ./drivers/storage/vbox/executor\nGOOS=darwin GOARCH=amd64 go install ./drivers/storage/vfs\nGOOS=darwin GOARCH=amd64 go install ./drivers/storage/vfs/executor\nGOOS=darwin GOARCH=amd64 go install ./imports/executors\nGOOS=darwin GOARCH=amd64 go install ./cli/lsx\nGOOS=darwin GOARCH=amd64 go install ./cli/lsx/lsx-darwin\n/Users/akutz/Projects/go/bin/go-bindata -md5checksum -pkg executors -prefix api/server/executors/bin -o api/server/executors/executors_generated.go api/server/executors/bin/...\n/Users/akutz/Projects/go/src/github.com/emccode/rexray", 
            "title": "Build Dependencies"
        }, 
        {
            "location": "/dev-guide/build-reference/#basic-build", 
            "text": "The most basic build can be achieved by simply typing  make  from the project's\nroot directory. For what it's worth, executing  make  sans arguments is the\nsame as executing  make install  for this project's  Makefile .  $ make\nSemVer: 0.4.0-rc4+10+dirty\nRpmVer: 0.4.0+rc4+10+dirty\nBranch: release/0.4.0-rc4\nCommit: d2f0283c7fed29ad0a142a6a51624828893f5db5\nFormed: Wed, 15 Jun 2016 16:53:44 CDT\n\ntarget: fmt\n  ...formatting rexray...SUCCESS!\ntarget: install\n  ...installing rexray Darwin-x86_64...SUCCESS!\n\nThe REX-Ray binary is 40MB and located at:\n\n  /Users/akutz/Projects/go/bin/rexray", 
            "title": "Basic Build"
        }, 
        {
            "location": "/dev-guide/build-reference/#binary-size", 
            "text": "Please note that the large size of the binary is due to the Isilon\nstorage adapter's dependency on the VMware VMOMI library for Go . The types\ndefinition source in that package compiles to a 45MB, uncompressed archive.\nEfforts are being made to figure out an alternative to this dependency in order\nto reduce the binary size, even if it means creating VMware SOAP messages from\nscratch.", 
            "title": "Binary Size"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-all", 
            "text": "In order to build all versions of the binary type the following:  $ make build-all\nSemVer: 0.4.0-rc4+10\nRpmVer: 0.4.0+rc4+10\nBranch: release/0.4.0-rc4\nCommit: d2f0283c7fed29ad0a142a6a51624828893f5db5\nFormed: Wed, 15 Jun 2016 16:42:33 CDT\n\ntarget: fmt\n  ...formatting rexray...SUCCESS!\ntarget: build\n  ...building rexray Linux-x86_64...SUCCESS!\n\nThe REX-Ray binary is 40MB and located at:\n\n  .build/bin/Linux-x86_64/rexray", 
            "title": "Build All"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-packages", 
            "text": "The  Makefile  also includes targets that assist in the creation of\ndistributable packages using the produced artifact.", 
            "title": "Build Packages"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-tarballs", 
            "text": "The  Makefile 's  build  and  build-all  targets will not only build the binary\nin place, but it will also compress the binary as a tarball so it's ready for\ndeployment. For example, after the  make build-all  above, this is the contents\nof the directory  .build/deploy :  $ ls .build/deploy\nlatest/\nLinux-x86_64/  Looking inside the directory  .build/deploy/Linux-x86_64  reveals:  $ ls .build/deploy/Linux-x86_64/\nrexray-Linux-x86_64-0.4.0-rc4+10.tar.gz", 
            "title": "Build Tarballs"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-rpms", 
            "text": "The  Makefile 's  rpm-all  target will package the binary as\narchitecture-specific RPMs when executed on a system that supports the RPM\ndevelopment environment:  $ make rpm-all\ntarget: rpm\n  ...building rpm x86_64...SUCCESS!\n\nThe REX-Ray RPM is 11MB and located at:\n\n  .build/deploy/Linux-x86_64/rexray-0.4.0+rc4+10-1.x86_64.rpm", 
            "title": "Build RPMs"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-debs", 
            "text": "The  Makefile 's  deb-all  target will package the binary as\narchitecture-specific DEBs when executed on a system that supports the Alien\nRPM-to-DEB conversion tools:  $ make deb-all\ntarget: deb\n  ...building deb x86_64...SUCCESS!\n\nThe REX-Ray DEB is 9MB and located at:\n\n  .build/deploy/Linux-x86_64/rexray_0.4.0+rc4+10-1_amd64.deb", 
            "title": "Build DEBs"
        }, 
        {
            "location": "/dev-guide/build-reference/#version-file", 
            "text": "There is a file at the root of the project named  VERSION . The file contains\na single line with the  target  version of the project in the file. The version\nfollows the format:  (? major \\d+)\\.(? minor \\d+)\\.(? patch \\d+)(-rc\\d+)?  For example, during active development of version  0.4.0  the file would\ncontain the version  0.4.0 . When it's time to create  0.4.0 's first\nrelease candidate the version in the file will be changed to  0.4.0-rc1 . And\nwhen it's time to release  0.4.0  the version is changed back to  0.4.0 .  Please note that we've discussed making the actively developed version the\ntargeted version with a  -dev  suffix, but trying this resulted in confusion\nfor the RPM and DEB package managers when using  unstable  releases.  So what's the point of the file if it's basically duplicating the utility of a\ntag? Well, the  VERSION  file in fact has two purposes:    First and foremost updating the  VERSION  file with the same value as that\n     of the tag used to create a release provides a single, contextual reason to\n     push a commit and tag. Otherwise some random commit off of  master  would\n     be tagged as a release candidate or release. Always using the commit that\n     is related to updating the  VERSION  file is much cleaner.    The contents of the  VERSION  file are also used during the build process\n     as a means of overriding the output of a  git describe . This enables the\n     semantic version injected into the produced binary to be created using\n     the  targeted  version of the next release and not just the value of the\n     last, tagged commit.", 
            "title": "Version File"
        }, 
        {
            "location": "/dev-guide/release-process/", 
            "text": "Release Process\n\n\nHow to release REX-Ray\n\n\n\n\nProject Stages\n\n\nThis project has three parallels stages of release:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nunstable\n\n\nThe tip or HEAD of the \nmaster\n branch is referred to as \nunstable\n\n\n\n\n\n\nstaged\n\n\nA commit tagged with the suffix \n-rc\\d+\n such as \nv0.3.1-rc2\n is a \nstaged\n release. These are release candidates.\n\n\n\n\n\n\nstable\n\n\nA commit tagged with a version sans \n-rc\\d+\n suffix such as \nv0.3.1\n is a \nstable\n release.\n\n\n\n\n\n\n\n\nThere are no steps necessary to create an \nunstable\n release as that happens\nautomatically whenever an untagged commit is pushed to \nmaster\n. However, the\nfollowing workflow should be used when tagging a \nstaged\n release candidate\nor \nstable\n release.\n\n\n\n\nReview outstanding issues \n pull requests\n\n\nPrepare release notes\n\n\nUpdate the version file\n\n\nCommit \n pull request\n\n\nTag the release\n\n\nUpdate the version file (again)\n\n\n\n\nReview Issues \n Pull Requests\n\n\nThe first step to a release is to review the outstanding\n\nissues\n and\n\npull requests\n that are tagged for\nthe release in question.\n\n\nIf there are outstanding issues requiring changes or pending pull requests to\nbe merged, handle those prior to tagging any commit as a release candidate or\nrelease.\n\n\nIt is \nhighly\n recommended that pull requests be merged synchronously after\nrebasing each subsequent one off of the new tip of \nmaster\n. Remember, while\nGitHub will update a pull request as in conflict if a change to \nmaster\n\nresults in a merge conflict with the pull request, GitHub will \nnot\n force a\nnew build to spawn unless the pull request is actually updated.\n\n\nAt the very minimum a pull request's build should be re-executed prior to the\npull request being merged if \nmaster\n has changed since the pull request was\nopened.\n\n\nPrepare Release Notes\n\n\nUpdate the release notes at \n.docs/about/release-notes.md\n. This file is\nproject's authoritative changelog and should reflect new features, fixes, and\nany significant changes.\n\n\nThe most recent, \nstable\n version of the release notes are always available\nonline at\n\nREX-Ray's documentation site\n.\n\n\nUpdate Version File\n\n\nThe \nVERSION\n file exists at the root of the project and should be updated to\nreflect the value of the intended release.\n\n\nFor example, if creating the first release candidate for version 0.3.1, the\ncontents of the \nVERSION\n file should be a single line \n0.3.1-rc1\n followed by\na newline character:\n\n\n$ cat VERSION\n0.3.1-rc1\n\n\n\n\nIf releasing version 0.3.1 proper then the contents of the \nVERSION\n file\nshould be \n0.3.1\n followed by a newline character:\n\n\n$ cat VERSION\n0.3.1\n\n\n\n\nCommit \n Pull Request\n\n\nOnce all outstanding issues and pull requests are handled, the release notes\nand version are updated, it's time to create a commit.\n\n\nPlease make sure that the changes to the release notes and version files are\na part of the same commit. This makes identifying the aspects of a release,\nstaged or otherwise, far easier for future developers.\n\n\nA release's commit message can either be a reflection of the release notes or\nsomething simple. Either way the commit message should have the following\nsubject format and first line in its body:\n\n\nRelease (Candidate) v0.3.1-rc1\n\nThis patch bumps the version to v0.3.1-rc1.\n\n\n\n\nIf the commit message is longer it should simply reflect the same information\nfrom the release notes.\n\n\nOnce committed push the change to a fork and open a pull request. Even though\nthis commit marks a staged or official release, the pull request system is still\nused to assure that the build completes successfully and there are no unforeseen\nerrors.\n\n\nTag the Release\n\n\nOnce the pull request marking the \nstaged\n or \nstable\n release has been merged\ninto \nupstream\n's \nmaster\n it's time to tag the release.\n\n\nTag Format\n\n\nThe release tag should follow a prescribed format depending upon the release\ntype:\n\n\n\n\n\n\n\n\nRelease Type\n\n\nTag Format\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nstaged\n\n\nvMAJOR.MINOR.PATCH-rc[0-9]\n\n\nv0.3.1-rc1\n\n\n\n\n\n\nstable\n\n\nvMAJOR.MINOR-PATCH\n\n\nv0.3.1\n\n\n\n\n\n\n\n\nTag Methods\n\n\nThere are two ways to tag a release:\n\n\n\n\nGitHub Releases\n\n\nCommand Line\n\n\n\n\nCommand Line\n\n\nIf tagging a release via the command line be sure to fetch the latest changes\nfrom \nupstream\n's \nmaster\n and either merge them into your local copy of\n\nmaster\n or reset the local copy to reflect \nupstream\n prior to creating\nany tags.\n\n\nThe following combination of commands can be used to create a tag for\n0.3.1 Release Candidate 1:\n\n\ngit fetch upstream \n \\\n  git checkout master \n \\\n  git reset --hard upstream/master \n \\\n  git tag -a -m v0.3.1-rc1 v0.3.1-rc1\n\n\n\n\nThe above example combines a few operations:\n\n\n\n\nThe first command fetches the \nupstream\n changes\n\n\nThe local \nmaster\n branch is checked out\n\n\nThe local \nmaster\n branch is hard reset to \nupstream/master\n\n\nAn annotated tag is created on \nmaster\n for \nv0.3.1-rc1\n, or 0.3.1 Release\n     Candidate 1, with a tag message of \nv0.3.1-rc1\n.\n\n\n\n\nPlease note that the third step will erase any changes that exist only in the\nlocal \nmaster\n branch that do not also exist in the remote, upstream copy.\nHowever, if the two branches are not equal this method should not be used to\ncreate a tag anyway.\n\n\nThe above steps do not actually push the tag upstream. This is to allow for one\nfinal review of all the changes before doing so since the appearance of a new,\nannotated tag in the repository will cause the project's build system to\nautomatically kick off a build that will result in the release of a \nstaged\n or\n\nstable\n release. For \nstable\n releases the project's documentation will also be\nupdated.\n\n\nOnce positive everything looks good simply execute the following command to\npush the tag to the \nupstream\n repository:\n\n\ngit push upstream v0.3.1-rc1\n\n\n\n\nUpdate Version File (Again)\n\n\nAfter a release is tagged there is one final step involving the \nVERSION\n file.\nThe contents of the file should be updated to reflect the next, targeted release\nso that the produced artifacts reflect the targeted version value and not a\nvalue based on the last, tagged commit.\n\n\nFollowing the above examples where version \nv0.3.1-rc1\n was just staged, the\n\nVERSION\n file should be updated to indicate that 0.3.1 Release Candidate 2\n(\n0.3.1-rc2\n) is the next, targeted release:\n\n\n$ cat VERSION\n0.3.1-rc2\n\n\n\n\nCommit the change to the \nVERSION\n file with a commit message similar to the\nfollowing:\n\n\nBumped active dev version to v0.3.1-rc2\n\nThis patch bumps the active dev version to v0.3.1-rc2.\n\n\n\n\nOnce the \nVERSION\n file change is committed, push the change and open a pull\nrequest to merge into the project.", 
            "title": "Release Process"
        }, 
        {
            "location": "/dev-guide/release-process/#release-process", 
            "text": "How to release REX-Ray", 
            "title": "Release Process"
        }, 
        {
            "location": "/dev-guide/release-process/#project-stages", 
            "text": "This project has three parallels stages of release:     Name  Description      unstable  The tip or HEAD of the  master  branch is referred to as  unstable    staged  A commit tagged with the suffix  -rc\\d+  such as  v0.3.1-rc2  is a  staged  release. These are release candidates.    stable  A commit tagged with a version sans  -rc\\d+  suffix such as  v0.3.1  is a  stable  release.     There are no steps necessary to create an  unstable  release as that happens\nautomatically whenever an untagged commit is pushed to  master . However, the\nfollowing workflow should be used when tagging a  staged  release candidate\nor  stable  release.   Review outstanding issues   pull requests  Prepare release notes  Update the version file  Commit   pull request  Tag the release  Update the version file (again)", 
            "title": "Project Stages"
        }, 
        {
            "location": "/dev-guide/release-process/#review-issues-pull-requests", 
            "text": "The first step to a release is to review the outstanding issues  and pull requests  that are tagged for\nthe release in question.  If there are outstanding issues requiring changes or pending pull requests to\nbe merged, handle those prior to tagging any commit as a release candidate or\nrelease.  It is  highly  recommended that pull requests be merged synchronously after\nrebasing each subsequent one off of the new tip of  master . Remember, while\nGitHub will update a pull request as in conflict if a change to  master \nresults in a merge conflict with the pull request, GitHub will  not  force a\nnew build to spawn unless the pull request is actually updated.  At the very minimum a pull request's build should be re-executed prior to the\npull request being merged if  master  has changed since the pull request was\nopened.", 
            "title": "Review Issues &amp; Pull Requests"
        }, 
        {
            "location": "/dev-guide/release-process/#prepare-release-notes", 
            "text": "Update the release notes at  .docs/about/release-notes.md . This file is\nproject's authoritative changelog and should reflect new features, fixes, and\nany significant changes.  The most recent,  stable  version of the release notes are always available\nonline at REX-Ray's documentation site .", 
            "title": "Prepare Release Notes"
        }, 
        {
            "location": "/dev-guide/release-process/#update-version-file", 
            "text": "The  VERSION  file exists at the root of the project and should be updated to\nreflect the value of the intended release.  For example, if creating the first release candidate for version 0.3.1, the\ncontents of the  VERSION  file should be a single line  0.3.1-rc1  followed by\na newline character:  $ cat VERSION\n0.3.1-rc1  If releasing version 0.3.1 proper then the contents of the  VERSION  file\nshould be  0.3.1  followed by a newline character:  $ cat VERSION\n0.3.1", 
            "title": "Update Version File"
        }, 
        {
            "location": "/dev-guide/release-process/#commit-pull-request", 
            "text": "Once all outstanding issues and pull requests are handled, the release notes\nand version are updated, it's time to create a commit.  Please make sure that the changes to the release notes and version files are\na part of the same commit. This makes identifying the aspects of a release,\nstaged or otherwise, far easier for future developers.  A release's commit message can either be a reflection of the release notes or\nsomething simple. Either way the commit message should have the following\nsubject format and first line in its body:  Release (Candidate) v0.3.1-rc1\n\nThis patch bumps the version to v0.3.1-rc1.  If the commit message is longer it should simply reflect the same information\nfrom the release notes.  Once committed push the change to a fork and open a pull request. Even though\nthis commit marks a staged or official release, the pull request system is still\nused to assure that the build completes successfully and there are no unforeseen\nerrors.", 
            "title": "Commit &amp; Pull Request"
        }, 
        {
            "location": "/dev-guide/release-process/#tag-the-release", 
            "text": "Once the pull request marking the  staged  or  stable  release has been merged\ninto  upstream 's  master  it's time to tag the release.", 
            "title": "Tag the Release"
        }, 
        {
            "location": "/dev-guide/release-process/#tag-format", 
            "text": "The release tag should follow a prescribed format depending upon the release\ntype:     Release Type  Tag Format  Example      staged  vMAJOR.MINOR.PATCH-rc[0-9]  v0.3.1-rc1    stable  vMAJOR.MINOR-PATCH  v0.3.1", 
            "title": "Tag Format"
        }, 
        {
            "location": "/dev-guide/release-process/#tag-methods", 
            "text": "There are two ways to tag a release:   GitHub Releases  Command Line", 
            "title": "Tag Methods"
        }, 
        {
            "location": "/dev-guide/release-process/#command-line", 
            "text": "If tagging a release via the command line be sure to fetch the latest changes\nfrom  upstream 's  master  and either merge them into your local copy of master  or reset the local copy to reflect  upstream  prior to creating\nany tags.  The following combination of commands can be used to create a tag for\n0.3.1 Release Candidate 1:  git fetch upstream   \\\n  git checkout master   \\\n  git reset --hard upstream/master   \\\n  git tag -a -m v0.3.1-rc1 v0.3.1-rc1  The above example combines a few operations:   The first command fetches the  upstream  changes  The local  master  branch is checked out  The local  master  branch is hard reset to  upstream/master  An annotated tag is created on  master  for  v0.3.1-rc1 , or 0.3.1 Release\n     Candidate 1, with a tag message of  v0.3.1-rc1 .   Please note that the third step will erase any changes that exist only in the\nlocal  master  branch that do not also exist in the remote, upstream copy.\nHowever, if the two branches are not equal this method should not be used to\ncreate a tag anyway.  The above steps do not actually push the tag upstream. This is to allow for one\nfinal review of all the changes before doing so since the appearance of a new,\nannotated tag in the repository will cause the project's build system to\nautomatically kick off a build that will result in the release of a  staged  or stable  release. For  stable  releases the project's documentation will also be\nupdated.  Once positive everything looks good simply execute the following command to\npush the tag to the  upstream  repository:  git push upstream v0.3.1-rc1", 
            "title": "Command Line"
        }, 
        {
            "location": "/dev-guide/release-process/#update-version-file-again", 
            "text": "After a release is tagged there is one final step involving the  VERSION  file.\nThe contents of the file should be updated to reflect the next, targeted release\nso that the produced artifacts reflect the targeted version value and not a\nvalue based on the last, tagged commit.  Following the above examples where version  v0.3.1-rc1  was just staged, the VERSION  file should be updated to indicate that 0.3.1 Release Candidate 2\n( 0.3.1-rc2 ) is the next, targeted release:  $ cat VERSION\n0.3.1-rc2  Commit the change to the  VERSION  file with a commit message similar to the\nfollowing:  Bumped active dev version to v0.3.1-rc2\n\nThis patch bumps the active dev version to v0.3.1-rc2.  Once the  VERSION  file change is committed, push the change and open a pull\nrequest to merge into the project.", 
            "title": "Update Version File (Again)"
        }, 
        {
            "location": "/about/contributing/", 
            "text": "Contributing to REX-Ray\n\n\nAn introduction to contributing to the REX-Ray project\n\n\n\n\nThe REX-Ray project welcomes, and depends, on contributions from developers and\nusers in the open source community. Contributions can be made in a number of\nways, a few examples are:\n\n\n\n\nCode patches via pull requests\n\n\nDocumentation improvements\n\n\nBug reports and patch reviews\n\n\nOS, Storage, and Volume Drivers\n\n\nA distributed server/client model with profile support\n\n\n\n\nReporting an Issue\n\n\nPlease include as much detail as you can. This includes:\n\n\n\n\nThe OS type and version\n\n\nThe REX-Ray version\n\n\nThe storage system in question\n\n\nA set of logs with debug-logging enabled that show the problem\n\n\n\n\nTesting the Development Version\n\n\nIf you want to just install and try out the latest development version of\nREX-Ray you can do so with the following command. This can be useful if you\nwant to provide feedback for a new feature or want to confirm if a bug you\nhave encountered is fixed in the git master. It is \nstrongly\n recommended\nthat you do this within a [virtualenv].\n\n\ncurl -sSL https://dl.bintray.com/emccode/rexray/install | sh -s -- unstable\n\n\n\n\nInstalling for Development\n\n\nFirst you'll need to fork and clone the repository. Once you have a local\ncopy, run the following command.\n\n\nmake deps \n make\n\n\n\n\nThis will install REX-Ray into your \nGOPATH\n and you'll be able to make changes\nlocally, test them, and commit ideas and fixes back to your fork of the\nrepository.\n\n\nRunning the tests\n\n\nTo run the tests, run the following commands:\n\n\nmake test\n\n\n\n\nThe \nmake install\n isn't strictly necessary, but it ensures that the tests are\nexecuted with the latest bits.\n\n\nSubmitting Pull Requests\n\n\nOnce you are happy with your changes or you are ready for some feedback, push\nit to your fork and send a pull request. For a change to be accepted it will\nmost likely need to have tests and documentation if it is a new feature.", 
            "title": "Contributing"
        }, 
        {
            "location": "/about/contributing/#contributing-to-rex-ray", 
            "text": "An introduction to contributing to the REX-Ray project   The REX-Ray project welcomes, and depends, on contributions from developers and\nusers in the open source community. Contributions can be made in a number of\nways, a few examples are:   Code patches via pull requests  Documentation improvements  Bug reports and patch reviews  OS, Storage, and Volume Drivers  A distributed server/client model with profile support", 
            "title": "Contributing to REX-Ray"
        }, 
        {
            "location": "/about/contributing/#reporting-an-issue", 
            "text": "Please include as much detail as you can. This includes:   The OS type and version  The REX-Ray version  The storage system in question  A set of logs with debug-logging enabled that show the problem", 
            "title": "Reporting an Issue"
        }, 
        {
            "location": "/about/contributing/#testing-the-development-version", 
            "text": "If you want to just install and try out the latest development version of\nREX-Ray you can do so with the following command. This can be useful if you\nwant to provide feedback for a new feature or want to confirm if a bug you\nhave encountered is fixed in the git master. It is  strongly  recommended\nthat you do this within a [virtualenv].  curl -sSL https://dl.bintray.com/emccode/rexray/install | sh -s -- unstable", 
            "title": "Testing the Development Version"
        }, 
        {
            "location": "/about/contributing/#installing-for-development", 
            "text": "First you'll need to fork and clone the repository. Once you have a local\ncopy, run the following command.  make deps   make  This will install REX-Ray into your  GOPATH  and you'll be able to make changes\nlocally, test them, and commit ideas and fixes back to your fork of the\nrepository.", 
            "title": "Installing for Development"
        }, 
        {
            "location": "/about/contributing/#running-the-tests", 
            "text": "To run the tests, run the following commands:  make test  The  make install  isn't strictly necessary, but it ensures that the tests are\nexecuted with the latest bits.", 
            "title": "Running the tests"
        }, 
        {
            "location": "/about/contributing/#submitting-pull-requests", 
            "text": "Once you are happy with your changes or you are ready for some feedback, push\nit to your fork and send a pull request. For a change to be accepted it will\nmost likely need to have tests and documentation if it is a new feature.", 
            "title": "Submitting Pull Requests"
        }, 
        {
            "location": "/about/license/", 
            "text": "Licensing\n\n\nThe legal stuff\n\n\n\n\nREX-Ray License\n\n\nLicensed under the Apache License, Version 2.0 (the \u201cLicense\u201d); you may not use\nthis file except in compliance with the License. You may obtain a copy of the\nLicense at \nhttp://www.apache.org/licenses/LICENSE-2.0\n\n\nUnless required by applicable law or agreed to in writing, software distributed\nunder the License is distributed on an \u201cAS IS\u201d BASIS, WITHOUT WARRANTIES OR\nCONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.", 
            "title": "License"
        }, 
        {
            "location": "/about/license/#licensing", 
            "text": "The legal stuff", 
            "title": "Licensing"
        }, 
        {
            "location": "/about/license/#rex-ray-license", 
            "text": "Licensed under the Apache License, Version 2.0 (the \u201cLicense\u201d); you may not use\nthis file except in compliance with the License. You may obtain a copy of the\nLicense at  http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed\nunder the License is distributed on an \u201cAS IS\u201d BASIS, WITHOUT WARRANTIES OR\nCONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.", 
            "title": "REX-Ray License"
        }, 
        {
            "location": "/about/release-notes/", 
            "text": "Release Notes\n\n\n\n\nUpgrading\n\n\nTo upgrade REX-Ray to the latest version, use \ncurl install\n:\n\n\ncurl -sSL https://dl.bintray.com/emccode/rexray/install | sh\n\n\n\nUse \nrexray version\n to determine the currently installed version of REX-Ray:\n\n\n$ rexray version\nREX-Ray\n-------\nBinary: /Users/akutz/Projects/go/bin/rexray\nSemVer: 0.4.0\nOsArch: Linux-x86_64\nBranch: v0.4.0\nCommit: c83f0237e60792cfe89c4255d7149b5670965539\nFormed: Mon, 20 Jun 2016 20:56:48 CDT\n\nlibStorage\n----------\nSemVer: 0.1.3\nOsArch: Linux-x86_64\nBranch: v0.1.3\nCommit: 182a626937677a081b89651598ee2eac839308e7\nFormed: Wed, 15 Jun 2016 16:27:36 CDT\n\n\n\nVersion 0.4.2 (2016/07/12)\n\n\nThis minor update represents a \nmajor\n performance boost for REX-Ray.\nOperations that use to take up to minutes now take seconds or less. The memory\nfootprint has been reduced from the magnitude of phenomenal cosmic powers to\nthe size of an itty bitty living space!\n\n\nEnhancements\n\n\n\n\nlibStorage 0.1.5 (\n#TBA\n)\n\n\nImproved volume path caching (\n#500\n)\n\n\n\n\nVersion 0.4.1 (2016/07/08)\n\n\nAlthough a minor release, 0.4.1 provides some meaningful and useful enhancements\nand fixes, further strengthening the foundation of the REX-Ray platform.\n\n\nEnhancements\n\n\n\n\nImproved build process (\n#474\n, \n#492\n)\n\n\nlibStorage\n 0.1.4 (\n#493\n)\n\n\nRemoved Docker spec file (\n#486\n)\n\n\nImproved REX-Ray 0.3.3 Config Backwards Compatibility (\n#481\n)\n\n\nImproved install script (\n#439\n, \n#495\n)\n\n\n\n\nBug Fixes\n\n\n\n\nFixed input validation bug when creating volume sans name (\n#478\n)\n\n\n\n\nVersion 0.4.0 (2016/06/20)\n\n\nREX-Ray 0.4.0 introduces centralized configuration and control along with\na new client/server architecture -- features made possible by\n\nlibStorage\n. Users are no longer\nrequired to configure storage drivers or store privileged information on all\nsystems running the REX-Ray client. The new client delegates storage-platform\nrelated operations to a remote, libStorage-compatible server such as REX-Ray\nor \nPoly\n.\n\n\nPlease note that the initial release of REX-Ray 0.4 includes support for only\nthe following storage platforms:\n\n\n\n\nScaleIO\n\n\nVirtualBox\n\n\n\n\nSupport for the full compliment of drivers present in earlier versions of\nREX-Ray will be reintroduced over the course of several, incremental updates,\nbeginning with 0.4.1.\n\n\nNew Features\n\n\n\n\nDistributed architecture (\n#399\n, \n#401\n, \n#411\n, \n#417\n, \n#418\n, \n#419\n, \n#420\n, \n#423\n)\n\n\nVolume locking mechanism (\n#171\n)\n\n\nVolume creation with initial data (\n#169\n)\n\n\n\n\nEnhancements\n\n\n\n\nImproved storage driver logging (\n#396\n)\n\n\nDocker mount path (\n#403\n)\n\n\n\n\nBug Fixes\n\n\n\n\nFixed issue with install script (\n#409\n)\n\n\nFixed volume ls filter (\n#400\n)\n\n\nFixed panic during access attempt of offline REX-Ray daemon (\n#148\n)\n\n\n\n\nThank You\n\n\nYes, the author is so lazy as to blatantly\n\ncopy\n\nthis section. So sue me :)\n\n\n\n\n\n\n\n\nName\n\n\nBlame\n\n\n\n\n\n\n\n\n\n\nClint Kitson\n\n\nHis vision come to fruition. That's \nhis\n vision, thus please assign \nall\n bugs to Clint :)\n\n\n\n\n\n\nVladimir Vivien\n\n\nA nascent player, Vlad had to hit the ground running and has been a key contributor\n\n\n\n\n\n\nKenny Coleman\n\n\nWhile some come close, none are comparable to Kenny's handlebar\n\n\n\n\n\n\nJonas Rosland\n\n\nAlways good for a sanity check and keeping things on the straight and narrow\n\n\n\n\n\n\nSteph Carlson\n\n\nSteph keeps the convention train chugging along...\n\n\n\n\n\n\nAmanda Katona\n\n\nAnd Amanda is the one keeping the locomotive from going off the rails\n\n\n\n\n\n\nDrew Smith\n\n\nDrew is always ready to lend a hand, no matter the problem\n\n\n\n\n\n\nChris Duchesne\n\n\nHis short time with the team is in complete opposition to the value he has added to this project\n\n\n\n\n\n\nDavid vonThenen\n\n\nDavid has been a go-to guy for debugging the most difficult of issues\n\n\n\n\n\n\nSteve Wong\n\n\nSteve stays on top of the things and keeps use cases in sync with industry needs\n\n\n\n\n\n\nTravis Rhoden\n\n\nAnother keen mind, Travis is also a great font of technical know-how\n\n\n\n\n\n\nPeter Blum\n\n\nAbsent Peter, the EMC World demo would not have been ready\n\n\n\n\n\n\nMegan Hyland\n\n\nAnd absent Megan, Peter's work would only have taken things halfway there\n\n\n\n\n\n\nEugene Chupriyanov\n\n\nFor helping with the EC2 planning\n\n\n\n\n\n\nMatt Farina\n\n\nWithout Glide, it all comes crashing down\n\n\n\n\n\n\nJosh Bernstein\n\n\nThe shadowy figure behind the curtain...\n\n\n\n\n\n\n\n\nVersion 0.3.3 (2016/04/21)\n\n\nNew Features\n\n\n\n\nScaleIO v2 support (\n#355\n)\n\n\nEC2 Tags added to Volumes \n Snapshots (\n#314\n)\n\n\n\n\nEnhancements\n\n\n\n\nUse of official Amazon EC2 SDK (\n#359\n)\n\n\nAdded a disable feature for create/remove volume (\n#366\n)\n\n\nAdded ScaleIO troubleshooting information (\n#367\n)\n\n\n\n\nBug Fixes\n\n\n\n\nFixes URLs for documentation when viewed via Github (\n#337\n)\n\n\nFixes logging bug on Ubuntu 14.04 (\n#377\n)\n\n\nFixes module start timeout error (\n#376\n)\n\n\nFixes ScaleIO authentication loop bug (\n#375\n)\n\n\n\n\nThank You\n\n\n\n\nPhilipp Franke\n\n\nEugene Chupriyanov\n\n\nPeter Blum\n\n\nMegan Hyland\n\n\n\n\nVersion 0.3.2 (2016-03-04)\n\n\nNew Features\n\n\n\n\nSupport for Docker 1.10 and Volume Plugin Interface 1.2 (\n#273\n)\n\n\nStale PID File Prevents Service Start (\n#258\n)\n\n\nModule/Personality Support (\n#275\n)\n\n\nIsilon Preemption (\n#231\n)\n\n\nIsilon Snapshots (\n#260\n)\n\n\nboot2Docker Support (\n#263\n)\n\n\nScaleIO Dynamic Storage Pool Support (\n#267\n)\n\n\n\n\nEnhancements\n\n\n\n\nImproved installation documentation (\n#331\n)\n\n\nScaleIO volume name limitation (\n#304\n)\n\n\nDocker cache volumes for path operations (\n#306\n)\n\n\nConfig file validation (\n#312\n)\n\n\nBetter logging (\n#296\n)\n\n\nDocumentation Updates (\n#285\n)\n\n\n\n\nBug Fixes\n\n\n\n\nFixes issue with daemon process getting cleaned as part of SystemD Cgroup (\n#327\n)\n\n\nFixes regression in 0.3.2 RC3/RC4 resulting in no log file (\n#319\n)\n\n\nFixes no volumes returned on empty list (\n#322\n)\n\n\nFixes \"Unsupported FS\" when mounting/unmounting with EC2 (\n#321\n)\n\n\nScaleIO re-authentication issue (\n#303\n)\n\n\nDocker XtremIO create volume issue (\n#307\n)\n\n\nService status is reported correctly (\n#310\n)\n\n\n\n\nUpdates\n\n\n\n\nGo 1.6 (\n#308\n)\n\n\n\n\nThank You\n\n\n\n\nDan Forrest\n\n\nKapil Jain\n\n\nAlex Kamalov\n\n\n\n\nVersion 0.3.1 (2015-12-30)\n\n\nNew Features\n\n\n\n\nSupport for VirtualBox (\n#209\n)\n\n\nAdded Developer's Guide (\n#226\n)\n\n\n\n\nEnhancements\n\n\n\n\nMount/Unmount Accounting (\n#212\n)\n\n\nSupport for Sub-Path Volume Mounts / Permissions (\n#215\n)\n\n\n\n\nMilestone Issues\n\n\nThis release also includes many other small enhancements and bug fixes. For a\ncomplete list click \nhere\n.\n\n\nDownloads\n\n\nClick \nhere\n for the 0.3.1\nbinaries.\n\n\nVersion 0.3.0 (2015-12-08)\n\n\nNew Features\n\n\n\n\nPre-Emption support (\n#190\n)\n\n\nSupport for VMAX (\n#197\n)\n\n\nSupport for Isilon (\n#198\n)\n\n\nSupport for Google Compute Engine (GCE) (\n#194\n)\n\n\n\n\nEnhancements\n\n\n\n\nAdded driver example configurations (\n#201\n)\n\n\nNew configuration file format (\n#188\n)\n\n\n\n\nTweaks\n\n\n\n\nChopped flags \n--rexrayLogLevel\n becomes \nlogLevel\n (\n#196\n)\n\n\n\n\nPre-Emption Support\n\n\nPre-Emption is an important feature when using persistent volumes and container\nschedulers.  Without pre-emption, the default behavior of the storage drivers is\nto deny the attaching operation if the volume is already mounted elsewhere.\n\nIf it is desired that a host should be able to pre-empt from other hosts, then\nthis feature can be used to enable any host to pre-empt from another.\n\n\nMilestone Issues\n\n\nThis release also includes many other small enhancements and bug fixes. For a\ncomplete list click \nhere\n.\n\n\nDownloads\n\n\nClick \nhere\n for the 0.3.0\nbinaries.\n\n\nVersion 0.2.1 (2015-10-27)\n\n\nREX-Ray release 0.2.1 includes OpenStack support, vastly improved documentation,\nand continued foundation changes for future features.\n\n\nNew Features\n\n\n\n\nSupport for OpenStack (\n#111\n)\n\n\nCreate volume from volume using existing settings (\n#129\n)\n\n\n\n\nEnhancements\n\n\n\n\nA+ \nGoReport Card\n\n\nA+ \nCode Coverage\n\n\nGoDoc Support\n\n\nAbility to load REX-Ray as an independent storage platform (\n#127\n)\n\n\nNew documentation at http://rexray.readthedocs.org (\n#145\n)\n\n\nMore foundation updates\n\n\n\n\nTweaks\n\n\n\n\nCommand aliases for \nget\n and \ndelete\n - \nls\n and \nrm\n (\n#107\n)\n\n\n\n\nVersion 0.2.0 (2015-09-30)\n\n\nInstallation, SysV, SystemD Support\n\n\nREX-Ray now includes built-in support for installing itself as a service on\nLinux distributions that support either SystemV or SystemD initialization\nsystems. This feature has been tested successfully on both CentOS 7 Minimal\n(SystemD) and Ubuntu 14.04 Server (SystemV) distributions.\n\n\nTo install REX-Ray on a supported Linux distribution, all that is required\nnow is to download the binary and execute:\n\n\nsudo ./rexray service install\n\n\n\nWhat does that do? In short the above command will determine if the Linux\ndistribution uses systemctl, update-rc.d, or chkconfig to manage system\nservices. After that the following steps occur:\n\n\n\n\nThe path /opt/rexray is created and chowned to root:root with permissions\n set to 0755.\n\n\nThe binary is copied to /opt/rexray/rexray and chowned to root:root with\n permissions set to 4755. This is important, because this means that any\n non-privileged user can execute the rexray binary as root without requiring\n sudo privileges. For more information on this feature, please read about the\n \nLinux kernel's super-user ID (SUID) bit\n.\n\n\n\n\nBecause the REX-Ray binary can now be executed with root privileges by\n non-root users, the binary can be used by non-root users to easily attach\n and mount external storage.\n\n\n\n\nThe directory /etc/rexray is created and chowned to root:root.\n\n\n\n\nThe next steps depends on the type of Linux distribution. However, it's\nimportant to know that the new version of the REX-Ray binary now supports\nmanaging its own PID (at \n/var/run/rexray.pid\n) when run as a service as well\nas supports the standard SysV control commands such as \nstart\n, \nstop\n,\n\nstatus\n, and \nrestart\n.\n\n\nFor SysV Linux distributions that use \nchkconfig\n or \nupdate-rc.d\n, a symlink\nof the REX-Ray binary is created in \n/etc/init.d\n and then either\n\nchkconfig rexray on\n or \nupdate-rc.d rexray defaults\n is executed.\n\n\nModern Linux distributions have moved to SystemD for controlling services.\nIf the \nsystemctl\n command is detected when installing REX-Ray then a unit\nfile is written to \n/etc/systemd/system/rexray.servic\ne with the following\ncontents:\n\n\n[Unit]\nDescription=rexray\nBefore=docker.service\n\n[Service]\nEnvironmentFile=/etc/rexray/rexray.env\nExecStart=/usr/local/bin/rexray start -f\nExecReload=/bin/kill -HUP $MAINPID\nKillMode=process\n\n[Install]\nWantedBy=docker.service\n\n\n\nThe REX-Ray service is not started immediately upon installation. The install\ncommand completes by informing the users that they should visit the\n\nREX-Ray website\n for information on how to\nconfigure REX-Ray's storage drivers. The text to the users also explains how\nto start the REX-Ray service once it's configured using the service command\nparticular to the Linux distribution.\n\n\nSingle Service\n\n\nThis release also removes the need for REX-Ray to be configured as multiple\nservice instances in order to provide multiple end-points to such consumers\nsuch as \nDocker\n. REX-Ray's backend now supports an internal, modular design\nwhich enables it to host multiple module instances of any module, such as the\nDockerVolumeDriverModule. In fact, one of the default, included modules is...\n\n\nAdmin Module \n HTTP JSON API\n\n\nThe AdminModule enables an HTTP JSON API for managing REX-Ray's module system\nas well as provides a UI to view the currently running modules. Simply start\nthe REX-Ray server and then visit the URL http://localhost:7979 in your favorite\nbrowser to see what's loaded. Or you can access either of the currently\nsupported REST URLs:\n\n\nhttp://localhost:7979/r/module/types\n\n\n\nand\n\n\nhttp://localhost:7979/r/module/instances\n\n\n\nActually, those aren't the \nonly\n two URLs, but the others are for internal\nusers as of this point. However, the source \nis\n open, so... :)\n\n\nIf you want to know what modules are available by using the CLI, after starting\nthe REX-Ray service simply type:\n\n\n[0]akutz@poppy:rexray$ rexray service module types\n[\n  {\n    \"id\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"addresses\": [\n      \"unix:///run/docker/plugins/rexray.sock\",\n      \"tcp://:7980\"\n    ]\n  },\n  {\n    \"id\": 1,\n    \"name\": \"AdminModule\",\n    \"addresses\": [\n      \"tcp://:7979\"\n    ]\n  }\n]\n[0]akutz@poppy:rexray$\n\n\n\nTo get a list of the \nrunning\n modules you would type:\n\n\n[0]akutz@poppy:rexray$ rexray service module instance get\n[\n  {\n    \"id\": 1,\n    \"typeId\": 1,\n    \"name\": \"AdminModule\",\n    \"address\": \"tcp://:7979\",\n    \"description\": \"The REX-Ray admin module\",\n    \"started\": true\n  },\n  {\n    \"id\": 2,\n    \"typeId\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"address\": \"unix:///run/docker/plugins/rexray.sock\",\n    \"description\": \"The REX-Ray Docker VolumeDriver module\",\n    \"started\": true\n  },\n  {\n    \"id\": 3,\n    \"typeId\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"address\": \"tcp://:7980\",\n    \"description\": \"The REX-Ray Docker VolumeDriver module\",\n    \"started\": true\n  }\n]\n[0]akutz@poppy:rexray$\n\n\n\nHmmm, you know, the REX-Ray CLI looks a little different in the above examples,\ndoesn't it? About that...\n\n\nCommand Line Interface\n\n\nThe CLI has also been enhanced to present a more simplified view up front to\nusers. The commands are now categorized into logical groups:\n\n\n[0]akutz@pax:~$ rexray\nREX-Ray:\n  A guest-based storage introspection tool that enables local\n  visibility and management from cloud and storage platforms.\n\nUsage:\n  rexray [flags]\n  rexray [command]\n\nAvailable Commands:\n  volume      The volume manager\n  snapshot    The snapshot manager\n  device      The device manager\n  adapter     The adapter manager\n  service     The service controller\n  version     Print the version\n  help        Help about any command\n\nGlobal Flags:\n  -c, --config=\"/Users/akutz/.rexray/config.yaml\": The REX-Ray configuration file\n  -?, --help[=false]: Help for rexray\n  -h, --host=\"tcp://:7979\": The REX-Ray service address\n  -l, --logLevel=\"info\": The log level (panic, fatal, error, warn, info, debug)\n  -v, --verbose[=false]: Print verbose help information\n\nUse \"rexray [command] --help\" for more information about a command.\n\n\n\nTravis-CI Support\n\n\nREX-Ray now supports Travis-CI builds either from the primary REX-Ray repository\nor via a fork. All builds should be executed through the Makefile, which is a\nTravis-CI default. For the Travis-CI settings please be sure to set the\nenvironment variable \nGO15VENDOREXPERIMENT\n to \n1\n.", 
            "title": "Release Notes"
        }, 
        {
            "location": "/about/release-notes/#release-notes", 
            "text": "", 
            "title": "Release Notes"
        }, 
        {
            "location": "/about/release-notes/#upgrading", 
            "text": "To upgrade REX-Ray to the latest version, use  curl install :  curl -sSL https://dl.bintray.com/emccode/rexray/install | sh  Use  rexray version  to determine the currently installed version of REX-Ray:  $ rexray version\nREX-Ray\n-------\nBinary: /Users/akutz/Projects/go/bin/rexray\nSemVer: 0.4.0\nOsArch: Linux-x86_64\nBranch: v0.4.0\nCommit: c83f0237e60792cfe89c4255d7149b5670965539\nFormed: Mon, 20 Jun 2016 20:56:48 CDT\n\nlibStorage\n----------\nSemVer: 0.1.3\nOsArch: Linux-x86_64\nBranch: v0.1.3\nCommit: 182a626937677a081b89651598ee2eac839308e7\nFormed: Wed, 15 Jun 2016 16:27:36 CDT", 
            "title": "Upgrading"
        }, 
        {
            "location": "/about/release-notes/#version-042-20160712", 
            "text": "This minor update represents a  major  performance boost for REX-Ray.\nOperations that use to take up to minutes now take seconds or less. The memory\nfootprint has been reduced from the magnitude of phenomenal cosmic powers to\nthe size of an itty bitty living space!", 
            "title": "Version 0.4.2 (2016/07/12)"
        }, 
        {
            "location": "/about/release-notes/#enhancements", 
            "text": "libStorage 0.1.5 ( #TBA )  Improved volume path caching ( #500 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#version-041-20160708", 
            "text": "Although a minor release, 0.4.1 provides some meaningful and useful enhancements\nand fixes, further strengthening the foundation of the REX-Ray platform.", 
            "title": "Version 0.4.1 (2016/07/08)"
        }, 
        {
            "location": "/about/release-notes/#enhancements_1", 
            "text": "Improved build process ( #474 ,  #492 )  libStorage  0.1.4 ( #493 )  Removed Docker spec file ( #486 )  Improved REX-Ray 0.3.3 Config Backwards Compatibility ( #481 )  Improved install script ( #439 ,  #495 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes", 
            "text": "Fixed input validation bug when creating volume sans name ( #478 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#version-040-20160620", 
            "text": "REX-Ray 0.4.0 introduces centralized configuration and control along with\na new client/server architecture -- features made possible by libStorage . Users are no longer\nrequired to configure storage drivers or store privileged information on all\nsystems running the REX-Ray client. The new client delegates storage-platform\nrelated operations to a remote, libStorage-compatible server such as REX-Ray\nor  Poly .  Please note that the initial release of REX-Ray 0.4 includes support for only\nthe following storage platforms:   ScaleIO  VirtualBox   Support for the full compliment of drivers present in earlier versions of\nREX-Ray will be reintroduced over the course of several, incremental updates,\nbeginning with 0.4.1.", 
            "title": "Version 0.4.0 (2016/06/20)"
        }, 
        {
            "location": "/about/release-notes/#new-features", 
            "text": "Distributed architecture ( #399 ,  #401 ,  #411 ,  #417 ,  #418 ,  #419 ,  #420 ,  #423 )  Volume locking mechanism ( #171 )  Volume creation with initial data ( #169 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements_2", 
            "text": "Improved storage driver logging ( #396 )  Docker mount path ( #403 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_1", 
            "text": "Fixed issue with install script ( #409 )  Fixed volume ls filter ( #400 )  Fixed panic during access attempt of offline REX-Ray daemon ( #148 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#thank-you", 
            "text": "Yes, the author is so lazy as to blatantly copy \nthis section. So sue me :)     Name  Blame      Clint Kitson  His vision come to fruition. That's  his  vision, thus please assign  all  bugs to Clint :)    Vladimir Vivien  A nascent player, Vlad had to hit the ground running and has been a key contributor    Kenny Coleman  While some come close, none are comparable to Kenny's handlebar    Jonas Rosland  Always good for a sanity check and keeping things on the straight and narrow    Steph Carlson  Steph keeps the convention train chugging along...    Amanda Katona  And Amanda is the one keeping the locomotive from going off the rails    Drew Smith  Drew is always ready to lend a hand, no matter the problem    Chris Duchesne  His short time with the team is in complete opposition to the value he has added to this project    David vonThenen  David has been a go-to guy for debugging the most difficult of issues    Steve Wong  Steve stays on top of the things and keeps use cases in sync with industry needs    Travis Rhoden  Another keen mind, Travis is also a great font of technical know-how    Peter Blum  Absent Peter, the EMC World demo would not have been ready    Megan Hyland  And absent Megan, Peter's work would only have taken things halfway there    Eugene Chupriyanov  For helping with the EC2 planning    Matt Farina  Without Glide, it all comes crashing down    Josh Bernstein  The shadowy figure behind the curtain...", 
            "title": "Thank You"
        }, 
        {
            "location": "/about/release-notes/#version-033-20160421", 
            "text": "", 
            "title": "Version 0.3.3 (2016/04/21)"
        }, 
        {
            "location": "/about/release-notes/#new-features_1", 
            "text": "ScaleIO v2 support ( #355 )  EC2 Tags added to Volumes   Snapshots ( #314 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements_3", 
            "text": "Use of official Amazon EC2 SDK ( #359 )  Added a disable feature for create/remove volume ( #366 )  Added ScaleIO troubleshooting information ( #367 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_2", 
            "text": "Fixes URLs for documentation when viewed via Github ( #337 )  Fixes logging bug on Ubuntu 14.04 ( #377 )  Fixes module start timeout error ( #376 )  Fixes ScaleIO authentication loop bug ( #375 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#thank-you_1", 
            "text": "Philipp Franke  Eugene Chupriyanov  Peter Blum  Megan Hyland", 
            "title": "Thank You"
        }, 
        {
            "location": "/about/release-notes/#version-032-2016-03-04", 
            "text": "", 
            "title": "Version 0.3.2 (2016-03-04)"
        }, 
        {
            "location": "/about/release-notes/#new-features_2", 
            "text": "Support for Docker 1.10 and Volume Plugin Interface 1.2 ( #273 )  Stale PID File Prevents Service Start ( #258 )  Module/Personality Support ( #275 )  Isilon Preemption ( #231 )  Isilon Snapshots ( #260 )  boot2Docker Support ( #263 )  ScaleIO Dynamic Storage Pool Support ( #267 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements_4", 
            "text": "Improved installation documentation ( #331 )  ScaleIO volume name limitation ( #304 )  Docker cache volumes for path operations ( #306 )  Config file validation ( #312 )  Better logging ( #296 )  Documentation Updates ( #285 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_3", 
            "text": "Fixes issue with daemon process getting cleaned as part of SystemD Cgroup ( #327 )  Fixes regression in 0.3.2 RC3/RC4 resulting in no log file ( #319 )  Fixes no volumes returned on empty list ( #322 )  Fixes \"Unsupported FS\" when mounting/unmounting with EC2 ( #321 )  ScaleIO re-authentication issue ( #303 )  Docker XtremIO create volume issue ( #307 )  Service status is reported correctly ( #310 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#updates", 
            "text": "Go 1.6 ( #308 )", 
            "title": "Updates"
        }, 
        {
            "location": "/about/release-notes/#thank-you_2", 
            "text": "Dan Forrest  Kapil Jain  Alex Kamalov", 
            "title": "Thank You"
        }, 
        {
            "location": "/about/release-notes/#version-031-2015-12-30", 
            "text": "", 
            "title": "Version 0.3.1 (2015-12-30)"
        }, 
        {
            "location": "/about/release-notes/#new-features_3", 
            "text": "Support for VirtualBox ( #209 )  Added Developer's Guide ( #226 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements_5", 
            "text": "Mount/Unmount Accounting ( #212 )  Support for Sub-Path Volume Mounts / Permissions ( #215 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#milestone-issues", 
            "text": "This release also includes many other small enhancements and bug fixes. For a\ncomplete list click  here .", 
            "title": "Milestone Issues"
        }, 
        {
            "location": "/about/release-notes/#downloads", 
            "text": "Click  here  for the 0.3.1\nbinaries.", 
            "title": "Downloads"
        }, 
        {
            "location": "/about/release-notes/#version-030-2015-12-08", 
            "text": "", 
            "title": "Version 0.3.0 (2015-12-08)"
        }, 
        {
            "location": "/about/release-notes/#new-features_4", 
            "text": "Pre-Emption support ( #190 )  Support for VMAX ( #197 )  Support for Isilon ( #198 )  Support for Google Compute Engine (GCE) ( #194 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements_6", 
            "text": "Added driver example configurations ( #201 )  New configuration file format ( #188 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#tweaks", 
            "text": "Chopped flags  --rexrayLogLevel  becomes  logLevel  ( #196 )", 
            "title": "Tweaks"
        }, 
        {
            "location": "/about/release-notes/#pre-emption-support", 
            "text": "Pre-Emption is an important feature when using persistent volumes and container\nschedulers.  Without pre-emption, the default behavior of the storage drivers is\nto deny the attaching operation if the volume is already mounted elsewhere. \nIf it is desired that a host should be able to pre-empt from other hosts, then\nthis feature can be used to enable any host to pre-empt from another.", 
            "title": "Pre-Emption Support"
        }, 
        {
            "location": "/about/release-notes/#milestone-issues_1", 
            "text": "This release also includes many other small enhancements and bug fixes. For a\ncomplete list click  here .", 
            "title": "Milestone Issues"
        }, 
        {
            "location": "/about/release-notes/#downloads_1", 
            "text": "Click  here  for the 0.3.0\nbinaries.", 
            "title": "Downloads"
        }, 
        {
            "location": "/about/release-notes/#version-021-2015-10-27", 
            "text": "REX-Ray release 0.2.1 includes OpenStack support, vastly improved documentation,\nand continued foundation changes for future features.", 
            "title": "Version 0.2.1 (2015-10-27)"
        }, 
        {
            "location": "/about/release-notes/#new-features_5", 
            "text": "Support for OpenStack ( #111 )  Create volume from volume using existing settings ( #129 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements_7", 
            "text": "A+  GoReport Card  A+  Code Coverage  GoDoc Support  Ability to load REX-Ray as an independent storage platform ( #127 )  New documentation at http://rexray.readthedocs.org ( #145 )  More foundation updates", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#tweaks_1", 
            "text": "Command aliases for  get  and  delete  -  ls  and  rm  ( #107 )", 
            "title": "Tweaks"
        }, 
        {
            "location": "/about/release-notes/#version-020-2015-09-30", 
            "text": "", 
            "title": "Version 0.2.0 (2015-09-30)"
        }, 
        {
            "location": "/about/release-notes/#installation-sysv-systemd-support", 
            "text": "REX-Ray now includes built-in support for installing itself as a service on\nLinux distributions that support either SystemV or SystemD initialization\nsystems. This feature has been tested successfully on both CentOS 7 Minimal\n(SystemD) and Ubuntu 14.04 Server (SystemV) distributions.  To install REX-Ray on a supported Linux distribution, all that is required\nnow is to download the binary and execute:  sudo ./rexray service install  What does that do? In short the above command will determine if the Linux\ndistribution uses systemctl, update-rc.d, or chkconfig to manage system\nservices. After that the following steps occur:   The path /opt/rexray is created and chowned to root:root with permissions\n set to 0755.  The binary is copied to /opt/rexray/rexray and chowned to root:root with\n permissions set to 4755. This is important, because this means that any\n non-privileged user can execute the rexray binary as root without requiring\n sudo privileges. For more information on this feature, please read about the\n  Linux kernel's super-user ID (SUID) bit .   Because the REX-Ray binary can now be executed with root privileges by\n non-root users, the binary can be used by non-root users to easily attach\n and mount external storage.   The directory /etc/rexray is created and chowned to root:root.   The next steps depends on the type of Linux distribution. However, it's\nimportant to know that the new version of the REX-Ray binary now supports\nmanaging its own PID (at  /var/run/rexray.pid ) when run as a service as well\nas supports the standard SysV control commands such as  start ,  stop , status , and  restart .  For SysV Linux distributions that use  chkconfig  or  update-rc.d , a symlink\nof the REX-Ray binary is created in  /etc/init.d  and then either chkconfig rexray on  or  update-rc.d rexray defaults  is executed.  Modern Linux distributions have moved to SystemD for controlling services.\nIf the  systemctl  command is detected when installing REX-Ray then a unit\nfile is written to  /etc/systemd/system/rexray.servic e with the following\ncontents:  [Unit]\nDescription=rexray\nBefore=docker.service\n\n[Service]\nEnvironmentFile=/etc/rexray/rexray.env\nExecStart=/usr/local/bin/rexray start -f\nExecReload=/bin/kill -HUP $MAINPID\nKillMode=process\n\n[Install]\nWantedBy=docker.service  The REX-Ray service is not started immediately upon installation. The install\ncommand completes by informing the users that they should visit the REX-Ray website  for information on how to\nconfigure REX-Ray's storage drivers. The text to the users also explains how\nto start the REX-Ray service once it's configured using the service command\nparticular to the Linux distribution.", 
            "title": "Installation, SysV, SystemD Support"
        }, 
        {
            "location": "/about/release-notes/#single-service", 
            "text": "This release also removes the need for REX-Ray to be configured as multiple\nservice instances in order to provide multiple end-points to such consumers\nsuch as  Docker . REX-Ray's backend now supports an internal, modular design\nwhich enables it to host multiple module instances of any module, such as the\nDockerVolumeDriverModule. In fact, one of the default, included modules is...", 
            "title": "Single Service"
        }, 
        {
            "location": "/about/release-notes/#admin-module-http-json-api", 
            "text": "The AdminModule enables an HTTP JSON API for managing REX-Ray's module system\nas well as provides a UI to view the currently running modules. Simply start\nthe REX-Ray server and then visit the URL http://localhost:7979 in your favorite\nbrowser to see what's loaded. Or you can access either of the currently\nsupported REST URLs:  http://localhost:7979/r/module/types  and  http://localhost:7979/r/module/instances  Actually, those aren't the  only  two URLs, but the others are for internal\nusers as of this point. However, the source  is  open, so... :)  If you want to know what modules are available by using the CLI, after starting\nthe REX-Ray service simply type:  [0]akutz@poppy:rexray$ rexray service module types\n[\n  {\n    \"id\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"addresses\": [\n      \"unix:///run/docker/plugins/rexray.sock\",\n      \"tcp://:7980\"\n    ]\n  },\n  {\n    \"id\": 1,\n    \"name\": \"AdminModule\",\n    \"addresses\": [\n      \"tcp://:7979\"\n    ]\n  }\n]\n[0]akutz@poppy:rexray$  To get a list of the  running  modules you would type:  [0]akutz@poppy:rexray$ rexray service module instance get\n[\n  {\n    \"id\": 1,\n    \"typeId\": 1,\n    \"name\": \"AdminModule\",\n    \"address\": \"tcp://:7979\",\n    \"description\": \"The REX-Ray admin module\",\n    \"started\": true\n  },\n  {\n    \"id\": 2,\n    \"typeId\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"address\": \"unix:///run/docker/plugins/rexray.sock\",\n    \"description\": \"The REX-Ray Docker VolumeDriver module\",\n    \"started\": true\n  },\n  {\n    \"id\": 3,\n    \"typeId\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"address\": \"tcp://:7980\",\n    \"description\": \"The REX-Ray Docker VolumeDriver module\",\n    \"started\": true\n  }\n]\n[0]akutz@poppy:rexray$  Hmmm, you know, the REX-Ray CLI looks a little different in the above examples,\ndoesn't it? About that...", 
            "title": "Admin Module &amp; HTTP JSON API"
        }, 
        {
            "location": "/about/release-notes/#command-line-interface", 
            "text": "The CLI has also been enhanced to present a more simplified view up front to\nusers. The commands are now categorized into logical groups:  [0]akutz@pax:~$ rexray\nREX-Ray:\n  A guest-based storage introspection tool that enables local\n  visibility and management from cloud and storage platforms.\n\nUsage:\n  rexray [flags]\n  rexray [command]\n\nAvailable Commands:\n  volume      The volume manager\n  snapshot    The snapshot manager\n  device      The device manager\n  adapter     The adapter manager\n  service     The service controller\n  version     Print the version\n  help        Help about any command\n\nGlobal Flags:\n  -c, --config=\"/Users/akutz/.rexray/config.yaml\": The REX-Ray configuration file\n  -?, --help[=false]: Help for rexray\n  -h, --host=\"tcp://:7979\": The REX-Ray service address\n  -l, --logLevel=\"info\": The log level (panic, fatal, error, warn, info, debug)\n  -v, --verbose[=false]: Print verbose help information\n\nUse \"rexray [command] --help\" for more information about a command.", 
            "title": "Command Line Interface"
        }, 
        {
            "location": "/about/release-notes/#travis-ci-support", 
            "text": "REX-Ray now supports Travis-CI builds either from the primary REX-Ray repository\nor via a fork. All builds should be executed through the Makefile, which is a\nTravis-CI default. For the Travis-CI settings please be sure to set the\nenvironment variable  GO15VENDOREXPERIMENT  to  1 .", 
            "title": "Travis-CI Support"
        }
    ]
}